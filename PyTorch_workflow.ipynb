{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNennpSMPkANF1kaTEwn/HM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sebastian-Constantin-Iacob/learning_pytorch/blob/main/PyTorch_workflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "nufH9zD2c-G_",
        "outputId": "ff8e9e57-b678-4031-b922-06257a9dca53"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.0.1+cu118'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn # nn contains all of PyTorch's building blocks for neural networks\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Check PyTorch version\n",
        "torch.__version__"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# What dose this notebook cover\n",
        "what_were_covering = {1: \"data (prepare and load)\",\n",
        "                      2: \"build model\",\n",
        "                      3: \"fitting the model to data (training)\",\n",
        "                      4: \"making predictions and evaluating a model (inference)\",\n",
        "                      5: \"saving and loading a model\",\n",
        "                      6: \"putting it all toghether\"}\n",
        "what_were_covering"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUcqr591dFDc",
        "outputId": "f4b58ffd-cbbb-4312-910e-64a1b73ade17"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1: 'data (prepare and load)',\n",
              " 2: 'build model',\n",
              " 3: 'fitting the model to data (training)',\n",
              " 4: 'making predictions and evaluating a model (inference)',\n",
              " 5: 'saving and loading a model',\n",
              " 6: 'putting it all toghether'}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Data ( preparing and loading )\n",
        "Data can be almost anything in ML.\n",
        "\n",
        "* Excel spreadsheets\n",
        "* Images\n",
        "* Videos\n",
        "* Audio\n",
        "* Text\n",
        "* DNA\n",
        "* etc\n",
        "\n",
        "ML is a game of two parts:\n",
        "1. Get data in a numerical representation.\n",
        "2. Build a model to learn patters in that numerical representation.\n",
        "\n",
        "To shwocase , we shall create some data using linear regression formula.\n",
        "\n",
        "We'll use a linear regression formula to make a straight line with known parameters."
      ],
      "metadata": {
        "id": "Q8s11NI3fcUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create known parameters\n",
        "weight = 0.7\n",
        "bias = 0.3\n",
        "\n",
        "# Create data\n",
        "start = 0\n",
        "end = 1\n",
        "step = 0.02\n",
        "X = torch.arange(start, end, step).unsqueeze(dim=1)\n",
        "y = weight * X + bias\n",
        "X[:10], y[:10]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yVdQBBWf4S4",
        "outputId": "46eddea4-7bcc-4cd6-84a6-a85cd99d000f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0.0000],\n",
              "         [0.0200],\n",
              "         [0.0400],\n",
              "         [0.0600],\n",
              "         [0.0800],\n",
              "         [0.1000],\n",
              "         [0.1200],\n",
              "         [0.1400],\n",
              "         [0.1600],\n",
              "         [0.1800]]),\n",
              " tensor([[0.3000],\n",
              "         [0.3140],\n",
              "         [0.3280],\n",
              "         [0.3420],\n",
              "         [0.3560],\n",
              "         [0.3700],\n",
              "         [0.3840],\n",
              "         [0.3980],\n",
              "         [0.4120],\n",
              "         [0.4260]]))"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(X), len(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unRouO4MhgoI",
        "outputId": "a4421bad-1b68-45ed-b859-5224ede88e66"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50, 50)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spliting data into training and test sets\n",
        "\n",
        "Creating a training and test set with the data."
      ],
      "metadata": {
        "id": "yOSD81no7ZWv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a train/set split\n",
        "train_split = int(0.8 * len(X))\n",
        "X_train, y_train = X[:train_split], y[:train_split]\n",
        "X_test, y_test = X[train_split:], y[train_split:]\n",
        "\n",
        "len(X_train), len(y_train), len(X_test), len(y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JB9OJe07ehL",
        "outputId": "63a1b09a-33eb-4fc2-a88e-480ab4ed8068"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40, 40, 10, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualising data for better understanding"
      ],
      "metadata": {
        "id": "EIrlziuvg-cK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function do view data\n",
        "\n",
        "def plot_predictions(train_data=X_train,\n",
        "                     train_labels=y_train,\n",
        "                     test_data=X_test,\n",
        "                     test_labels=y_test,\n",
        "                     predictions=None):\n",
        "  \"\"\"\n",
        "  Plots training data, test data and compares predictions\n",
        "  \"\"\"\n",
        "  plt.figure(figsize=(10, 7))\n",
        "\n",
        "  # Plot training data in blue\n",
        "  plt.scatter(train_data, train_labels, c=\"b\", s=4, label=\"Training data\")\n",
        "\n",
        "  # Plot test data in green\n",
        "  plt.scatter(test_data, test_labels, c=\"g\", s=4, label=\"Testing data\")\n",
        "  # Are there predictions?\n",
        "  if predictions is not None:\n",
        "    # Plot the predictions if they exist\n",
        "    plt.scatter(test_data, predictions, c=\"r\", s=4, label=\"Predictions\")\n",
        "\n",
        "  # Show the legend\n",
        "  plt.legend(prop={\"size\": 14})"
      ],
      "metadata": {
        "id": "cRBwJDwbhCnA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_predictions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "CXLMUnSGkOEn",
        "outputId": "e0bea0e0-fc0e-45fe-a6de-dbbe9c5ac249"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x700 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Building model\n",
        "\n",
        "What are model dose:\n",
        "* Start with random values ( weight and bias )\n",
        "* Look at training data and adjust the random values to better represent (or get closer to) the ideal values ( the weight and bias values we use to create the data )\n",
        "\n",
        "It dose this through 2 main algorythms:\n",
        "1. Gradient decent.\n",
        "2. Backpropagation."
      ],
      "metadata": {
        "id": "4D5v5GSskvWu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a linear regression model class\n",
        "class LinearRegressionModel(nn.Module): # almost everything in PyTorch inherits from nn.Module\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.weights = nn.Parameter(torch.randn(1,\n",
        "                                            requires_grad=True,\n",
        "                                            dtype=torch.float))\n",
        "    self.bias = nn.Parameter(torch.randn(1,\n",
        "                                         requires_grad=True,\n",
        "                                         dtype=torch.float))\n",
        "    \n",
        "  # Forward method to define the computation in the model\n",
        "  def forward(self, X: torch.Tensor) -> torch.Tensor: # \"X\" is the input data\n",
        "    return self.weights * X + self.bias # Linear regression formula\n"
      ],
      "metadata": {
        "id": "ie3BKCo1lE0M"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PyTorch model building essentials\n",
        "\n",
        "* torch.nn - contains all the building blocks for computational graphs (a neural network may be considered a computational graph)\n",
        "* torch.nn.Parameter - what parameters should our model try and learn, often a PyTorch layer from torch.nn will set these for us\n",
        "* torch.nn.Module - The base class for all neural network modules, if you subclass it, you should overwrite the forward() method\n",
        "* torch.optin - This is where a optimizers in PyTorch live, they will help with gradient decent\n",
        "* def forward() - All nn.Module subclasses require you to overwrite forward(), this method defines what happens in the forward computation"
      ],
      "metadata": {
        "id": "baySyfO086RS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checking the contents of our PyTorch model"
      ],
      "metadata": {
        "id": "_vu9uPfW1GKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a random seed\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Crete an instance of the model (this is a subclass of nn.Module)\n",
        "model_0 = LinearRegressionModel()\n",
        "\n",
        "# Check out the parameters\n",
        "list(model_0.parameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYnz46qF1M7i",
        "outputId": "0c2b45ad-ad2f-4cad-ad27-3cc7375bf069"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([0.3367], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([0.1288], requires_grad=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List named parameters\n",
        "model_0.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmrH4d7o1yvR",
        "outputId": "b9381974-6a46-47b8-d2dc-3958af2f14f3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('weights', tensor([0.3367])), ('bias', tensor([0.1288]))])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weight, bias"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3S0omZY527bS",
        "outputId": "a8dfb616-241d-41c5-8b8f-7c56d36e4ff0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.7, 0.3)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Making predictions using `torch.inference_mode()`\n",
        "\n",
        "To check our model's predictive power, lets see how well it predicts y_test based on x_test.\n",
        "When we pass data throw our model, it is going to run it through the `forward()` method."
      ],
      "metadata": {
        "id": "mKk9Seoc4VOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions with model\n",
        "with torch.inference_mode():\n",
        "  y_preds = model_0(X_test)\n",
        "\n",
        "y_preds"
      ],
      "metadata": {
        "id": "D8hhaxz24jZA",
        "outputId": "ebf1ba32-14ad-4c74-9d3b-f9bfca59586b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.3982],\n",
              "        [0.4049],\n",
              "        [0.4116],\n",
              "        [0.4184],\n",
              "        [0.4251],\n",
              "        [0.4318],\n",
              "        [0.4386],\n",
              "        [0.4453],\n",
              "        [0.4520],\n",
              "        [0.4588]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let us see our model after one training loop (perfect means green dots overlap with red dots)\n",
        "plot_predictions(predictions=y_preds)"
      ],
      "metadata": {
        "id": "i63l_bKI5jAE",
        "outputId": "590fd542-af0d-4048-8d4d-54ba8449202e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x700 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzoAAAJGCAYAAACTJvC6AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABUIElEQVR4nO3dfVxUdf7//+cwXGkKrpqIyopZWW2mpenalTNFsZsfZ2xrs/qk6JZ9LcsWal2tFK2PUVsZhXbx8aPZxZa2Zc3ZbK2kwbaitdVsu1Ba8zIS1M0GowQdzu+P+TFEgDIIzMzhcb/d5jZxOOfMa/AQPHm/z/tlM03TFAAAAABYSEy4CwAAAACA1kbQAQAAAGA5BB0AAAAAlkPQAQAAAGA5BB0AAAAAlkPQAQAAAGA5BB0AAAAAlhMb7gKao6amRl9//bW6du0qm80W7nIAAAAAhIlpmjpw4ID69OmjmJimx22iIuh8/fXXSktLC3cZAAAAACLErl271K9fvyY/HxVBp2vXrpICbyYpKSnM1QAAAAAIl4qKCqWlpQUzQlOiIujUTldLSkoi6AAAAAA46i0tLEYAAAAAwHIIOgAAAAAsh6ADAAAAwHIIOgAAAAAsh6ADAAAAwHIIOgAAAAAsJyqWl26JQ4cOye/3h7sMICzi4uJkt9vDXQYAAEDYWC7oVFRUaN++faqqqgp3KUDY2Gw2JScnq3fv3kddYx4AAMCKQg4677zzjh544AGtX79eu3fv1iuvvKJx48Yd8ZiioiLl5OTos88+U1pamu666y5NmjSphSU3raKiQqWlperSpYt69uypuLg4fslDh2OapiorK7V371516tRJ3bp1C3dJAAAA7S7koFNZWakhQ4bod7/7nX7zm98cdf9t27ZpzJgxmjp1qv785z+rsLBQ119/vVJTU5WZmdmiopuyb98+denSRf369SPgoEPr1KmTqqqqtGfPHiUnJ/P9AAAAOpyQg86vf/1r/frXv272/k888YQGDBighx56SJJ06qmn6t1339XDDz/cqkHn0KFDqqqqUs+ePfmlDpCUlJSkiooK+f1+xcZabpYqAADAEbX5qmvFxcXKyMioty0zM1PFxcVNHlNVVaWKiop6j6OpXXggLi7u2AoGLKI23Bw+fDjMlQAAALS/Ng86ZWVlSklJqbctJSVFFRUV+uGHHxo9Ji8vT8nJycFHWlpas1+P0RwggO8FAADQkUVkH51Zs2bJ5/MFH7t27Qp3SQAAAACiSJtP3O/du7fKy8vrbSsvL1dSUpI6derU6DEJCQlKSEho69IAAAAAWFSbj+iMGjVKhYWF9ba99dZbGjVqVFu/NNqJzWaTw+E4pnMUFRXJZrNp7ty5rVJTW0tPT1d6enq4ywAAAEATQg463333nTZu3KiNGzdKCiwfvXHjRu3cuVNSYNrZxIkTg/tPnTpVW7du1YwZM7R582Y99thjevHFF5Wdnd067wCSAmEjlAfCz+Fw8G8BAADQRkKeuvbPf/5TTqcz+HFOTo4kKSsrS8uWLdPu3buDoUeSBgwYoFWrVik7O1uPPPKI+vXrp//7v/9r9R46HV1ubm6Dbfn5+fL5fI1+rjVt2rRJnTt3PqZzjBgxQps2bVLPnj1bqSoAAAB0ZDbTNM1wF3E0FRUVSk5Ols/nU1JSUqP7HDx4UNu2bdOAAQOUmJjYzhVGpvT0dO3YsUNR8E8cdWqnrW3fvr3F53A4HFq7dm2b/fvwPQEAAKyoOdlAitBV19B2tm/fLpvNpkmTJmnTpk267LLL1KNHD9lstuAv7a+88oquvvpqnXjiiercubOSk5N1/vnn6+WXX270nI3dozNp0iTZbDZt27ZNjz76qE455RQlJCSof//+mjdvnmpqaurt39Q9OrX3wnz33Xe69dZb1adPHyUkJOiMM87QSy+91OR7HD9+vLp3764uXbpo9OjReueddzR37lzZbDYVFRU1++vl8Xh09tlnq1OnTkpJSdGUKVO0f//+Rvf94osvNGPGDJ111lnq0aOHEhMTdfLJJ2vmzJn67rvvGnzN1q5dG/zv2sekSZOC+yxdulRut1vp6elKTExU9+7dlZmZKa/X2+z6AQAAOirapXdQW7Zs0S9/+UsNHjxYkyZN0n/+8x/Fx8dLCtxnFR8fr/POO0+pqanau3evDMPQFVdcoUcffVS33HJLs1/nD3/4g9auXav/+q//UmZmpl599VXNnTtX1dXVmj9/frPOcejQIV1yySXav3+/Lr/8cn3//fdavny5rrzySq1evVqXXHJJcN/S0lKdc8452r17t371q1/pzDPPVElJiS6++GJdeOGFIX2NnnnmGWVlZSkpKUkTJkxQt27d9NprrykjI0PV1dXBr1etlStXasmSJXI6nXI4HKqpqdEHH3yg+++/X2vXrtU777wTbGibm5urZcuWaceOHfWmFg4dOjT439OmTdOQIUOUkZGh448/XqWlpXr11VeVkZGhlStXyu12h/R+AAAAWsIoMeTd5pVzgFOuQa5wl9N8ZhTw+XymJNPn8zW5zw8//GB+/vnn5g8//NCOlUW2/v37mz/9J962bZspyZRkzpkzp9HjvvzyywbbDhw4YA4ePNhMTk42Kysr631Okjl69Oh627KyskxJ5oABA8yvv/46uH3v3r1mt27dzK5du5pVVVXB7V6v15Rk5ubmNvoe3G53vf3XrFljSjIzMzPr7X/ttdeaksz58+fX275kyZLg+/Z6vY2+7x/z+XxmUlKSedxxx5klJSXB7dXV1eYFF1xgSjL79+9f75ivvvqqXo215s2bZ0oyn3vuuXrbR48e3eDf58e2bt3aYNvXX39t9unTxzzppJOO+h74ngAAAMfKs9ljaq5M+zy7qbkyPZs94S6pWdnANE2TqWsdVO/evXXnnXc2+rkTTjihwbYuXbpo0qRJ8vl8+vDDD5v9OrNnz1Zqamrw4549e8rtduvAgQMqKSlp9nkefvjheiMoF110kfr371+vlqqqKv3lL39Rr169dNttt9U7fvLkyRo0aFCzX+/VV19VRUWFfve73+nkk08Obo+Li2tyJKpv374NRnkk6eabb5YkrVmzptmvLwUW8vip1NRUXX755fr3v/+tHTt2hHQ+AACAUHm3eWW32eU3/bLb7CraXhTukpqNoNNChiFlZweeo9GQIUMa/aVckvbs2aOcnBydeuqp6ty5c/D+kdrw8PXXXzf7dYYNG9ZgW79+/SRJ3377bbPO0a1bt0Z/6e/Xr1+9c5SUlKiqqkrDhw9v0HDWZrPpnHPOaXbdH3/8sSTp/PPPb/C5UaNGKTa24axP0zS1dOlSXXDBBerevbvsdrtsNpt69OghKbSvmyRt3bpVU6ZM0cCBA5WYmBj8dygoKGjR+QAAAELlHOAMhhy/6Zcj3RHukpqNe3RawDAkt1uy26X8fMnjkVxRNF1RklJSUhrd/s033+jss8/Wzp07de655yojI0PdunWT3W7Xxo0b5fF4VFVV1ezXaWwljNqQ4Pf7m3WO5OTkRrfHxsbWW9SgoqJCktSrV69G92/qPTfG5/M1eS673R4MLz82ffp0LVy4UGlpaXK5XEpNTQ0Grnnz5oX0dduyZYtGjBihiooKOZ1OjR07VklJSYqJiVFRUZHWrl0b0vkAAABawjXIJc9VHhVtL5Ij3RFV9+gQdFrA6w2EHL8/8FxUFH1Bp6lGlUuWLNHOnTt1zz336K677qr3ufvuu08ej6c9ymuR2lC1Z8+eRj9fXl7e7HPVhqvGzuX3+/Wf//xHffv2DW7bs2ePFi1apDPOOEPFxcX1+gqVlZVp3rx5zX5tKTBVb//+/Xr22Wd17bXX1vvc1KlTgyu2AQAAtDXXIFdUBZxaTF1rAaezLuT4/dJPVlaOal9++aUkNbqi19///vf2LickgwYNUkJCgtavX99gtMM0TRUXFzf7XEOGDJHU+HsuLi7W4cOH623bunWrTNNURkZGg+apTX3d7Ha7pMZHtpr6dzBNU++9914z3wUAAEDHRdBpAZcrMF1t+vTonLZ2JP3795ckvfvuu/W2P//883r99dfDUVKzJSQk6IorrlB5ebny8/Prfe6ZZ57R5s2bm30ut9utpKQkLV26VF988UVw+6FDhxqMdEl1X7f333+/3nS6r776SrNmzWr0Nbp37y5J2rVrV5Pn++m/w3333adPP/202e8DAACgo2LqWgu5XNYKOLUmTJig+++/X7fccou8Xq/69++vjz/+WIWFhfrNb36jlStXhrvEI8rLy9OaNWs0c+ZMrV27NthH57XXXtOvfvUrrV69WjExR8/3ycnJevTRRzVp0iSdffbZuuqqq5ScnKzXXntNnTp1qreSnFS3GtrLL7+s4cOH66KLLlJ5eblee+01XXTRRcERmh+78MIL9dJLL+nyyy/Xr3/9ayUmJmrIkCEaO3aspk6dqqeeekqXX365rrzySvXo0UMffPCBNmzYoDFjxmjVqlWt9jUDAACwIkZ0UE+/fv20du1aXXTRRVqzZo2efPJJVVdX680339TYsWPDXd5RpaWlqbi4WL/97W/1/vvvKz8/X3v27NGbb76pE088UVLjCyQ0JisrS6+88opOOukkPf3003r66ad17rnnas2aNY2uWLds2TLddttt2r9/vwoKCvTBBx8oJydHzz//fKPnnzJlimbMmKF9+/bp/vvv1+zZs/Xyyy9Lks4880y9+eabOuuss7Ry5UotXbpU3bp103vvvafhw4e38KsDAADQcdhM0zTDXcTRVFRUKDk5WT6fr8lfUg8ePKht27ZpwIABSkxMbOcKEQ3OO+88FRcXy+fzqUuXLuEup83xPQEAAH7MKDHk3eaVc4AzKhcXqNWcbCAxogML2r17d4Ntzz33nN577z1lZGR0iJADAADwY0aJIfdytwrWFci93C2jJEqbQYaAe3RgOaeffrrOPPNMnXbaacH+P0VFReratasefPDBcJcHAADQ7rzbvMGmn3abXUXbi6J6VKc5GNGB5UydOlV79uzRM888o4ULF6qkpETXXHON1q1bp8GDB4e7PAAAgHbnHOAMhhy/6Zcj3RHuktoc9+gAFsX3BAAA+DGjxFDR9iI50h1RPZrT3Ht0mLoGAAAAdACuQa6oDjihYuoaAAAAAMsh6AAAAACwHIIOAAAAAMsh6AAAAACwHIIOAAAAEEWMEkPZq7M7RNPPY0HQAQAAAKKEUWLIvdytgnUFci93E3aOgKADAAAARAnvNm+w6afdZlfR9qJwlxSxCDoAAABAlHAOcAZDjt/0y5HuCHdJEYugg3bhcDhks9nCXUazLFu2TDabTcuWLQt3KQAAAPW4Brnkucqj6SOny3OVp0M1AA0VQccibDZbSI/WNnfuXNlsNhUVFbX6uaNRUVGRbDab5s6dG+5SAACAxbgGubQgcwEh5yhiw10AWkdubm6Dbfn5+fL5fI1+rr0988wz+v7778NdBgAAADoIgo5FNDZysGzZMvl8vogYVfj5z38e7hIAAADQgTB1rQOqrq7WggULdNZZZ+m4445T165ddf7558swGi5P6PP5NGfOHJ122mnq0qWLkpKSdOKJJyorK0s7duyQFLj/Zt68eZIkp9MZnB6Xnp4ePE9j9+j8+F6YN998U+ecc446d+6sHj16KCsrS//5z38arf/JJ5/UL37xCyUmJiotLU0zZszQwYMHZbPZ5HA4mv11+OabbzR16lSlpKSoc+fOOvvss/XKK680uf/SpUvldruVnp6uxMREde/eXZmZmfJ6vfX2mzt3rpxOpyRp3rx59aYMbt++XZL0xRdfaMaMGTrrrLPUo0cPJSYm6uSTT9bMmTP13XffNfs9AAAAoHGM6HQwVVVV+tWvfqWioiINHTpU1113nQ4dOqRVq1bJ7XaroKBAN998syTJNE1lZmbqH//4h84991z96le/UkxMjHbs2CHDMDRhwgT1799fkyZNkiStXbtWWVlZwYDTrVu3ZtVkGIZWrVqlsWPH6pxzztE777yjZ555Rl9++aXefffdevvOmTNH99xzj1JSUjRlyhTFxcXpxRdf1ObNm0P6Onz//fdyOBz65JNPNGrUKI0ePVq7du3S+PHjdckllzR6zLRp0zRkyBBlZGTo+OOPV2lpqV599VVlZGRo5cqVcrvdkgKhbvv27Xr66ac1evToeuGr9muycuVKLVmyRE6nUw6HQzU1Nfrggw90//33a+3atXrnnXcUFxcX0nsCAADAj5hRwOfzmZJMn8/X5D4//PCD+fnnn5s//PBDO1YW2fr372/+9J/4jjvuMCWZs2fPNmtqaoLbKyoqzOHDh5vx8fFmaWmpaZqm+a9//cuUZI4bN67BuQ8ePGgeOHAg+HFubq4pyfR6vY3WMnr06Aa1PPXUU6YkMzY21nz33XeD2w8fPmw6HA5TkllcXBzcXlJSYtrtdrNv375meXl5vdpPO+00U5I5evToo39hflTvlClT6m1fvXq1KcmUZD711FP1Prd169YG5/n666/NPn36mCeddFK97V6v15Rk5ubmNvr6X331lVlVVdVg+7x580xJ5nPPPdes93EkfE8AABC5PJs95u//9nvTs9kT7lKiTnOygWmaJlPXWsgoMZS9OjuqutHW1NTo8ccf18CBA4NTqmp17dpVc+bMUXV1tVauXFnvuE6dOjU4V0JCgrp06dIqdV1zzTU699xzgx/b7XZlZWVJkj788MPg9hdeeEF+v1+33XabevXqVa/2u+66K6TXfOaZZxQfH6+777673vbMzExddNFFjR4zYMCABttSU1N1+eWX69///ndwKl9z9O3bV/Hx8Q22146mrVmzptnnAgAA0cUoMeRe7lbBugK5l7uj6vfJaMLUtRaovTjtNrvy/5EfNWuYl5SUaP/+/erTp0/wnpof27t3ryQFp4GdeuqpOuOMM/TCCy/oq6++0rhx4+RwODR06FDFxLReRh42bFiDbf369ZMkffvtt8FtH3/8sSTpvPPOa7D/j4PS0VRUVGjbtm067bTT1Lt37wafP//881VYWNhg+9atW5WXl6e3335bpaWlqqqqqvf5r7/+Wv37929WDaZp6qmnntKyZcv06aefyufzqaampt65AACANXm3eYMNP+02u4q2F0XF75LRhqDTAtF6cX7zzTeSpM8++0yfffZZk/tVVlZKkmJjY/X2229r7ty5evnll3XbbbdJko4//njdfPPNuvPOO2W324+5rqSkpAbbYmMDl6bf7w9uq6iokKR6ozm1UlJSmv16RzpPU+fasmWLRowYoYqKCjmdTo0dO1ZJSUmKiYlRUVGR1q5d2yD4HMn06dO1cOFCpaWlyeVyKTU1VQkJCZICCxiEci4AABBdnAOcyv9HfvD3SUe6I9wlWRJBpwWi9eKsDRSXX365XnrppWYd06NHDxUUFOjRRx/V5s2b9fbbb6ugoEC5ubmKi4vTrFmz2rLkemrr37NnT4ORk/Ly8hadpzGNnevhhx/W/v379eyzz+raa6+t97mpU6dq7dq1zX79PXv2aNGiRTrjjDNUXFyszp07Bz9XVlbW6GgbAACwDtcglzxXeVS0vUiOdEdU/ME8GnGPTgvUXpzTR06PmmlrUmAqWlJSkv75z3/q0KFDIR1rs9l06qmnatq0aXrrrbckqd5y1LUjOz8egWltQ4YMkSS99957DT73/vvvN/s8SUlJGjBggLZs2aKysrIGn//73//eYNuXX34pScGV1WqZptloPUf6emzdulWmaSojI6NeyGnqtQEAgPW4Brm0IHNB1PweGY0IOi0UjRdnbGysbrzxRu3YsUO33357o2Hn008/DY50bN++Pdj35cdqRzwSExOD27p37y5J2rVrVxtUHnDVVVcpJiZGDz30kPbt2xfcXllZqfnz54d0rgkTJqi6ulpz5sypt/3NN99s9P6c2hGkny53fd999+nTTz9tsP+Rvh6153r//ffr3Zfz1VdftesIGQAAgJUxda2DmTdvnjZs2KBHH31Uq1at0gUXXKBevXqptLRUn3zyiT7++GMVFxerV69e2rhxo37zm99oxIgRwRv3a3vHxMTEKDs7O3je2kahd9xxhz777DMlJyerW7duwVXEWsOgQYM0c+ZM3XvvvRo8eLCuvPJKxcbGauXKlRo8eLA+/fTTZi+SMGPGDK1cuVKLFy/WZ599pgsuuEC7du3Siy++qDFjxmjVqlX19p86daqeeuopXX755bryyivVo0cPffDBB9qwYUOj+59yyinq06ePli9froSEBPXr1082m0233HJLcKW2l19+WcOHD9dFF12k8vJyvfbaa7rooouCo0cAAABoOUZ0OpiEhAT97W9/05NPPqnevXvr5ZdfVn5+vt555x2lpqbq8ccf1+DBgyVJw4cP1x//+EfZbDatWrVKDz30kIqKipSRkaH33ntPLlfdaNZpp52mp556Sj179lRBQYFmz56tBx98sNXrnz9/vh577DH97Gc/0xNPPKEXX3xRV1xxhR577DFJjS9s0JjjjjtOa9eu1Q033KB///vfys/P1+bNm7VixQpdccUVDfY/88wz9eabb+qss87SypUrtXTpUnXr1k3vvfeehg8f3mB/u92ulStX6pe//KVeeOEFzZkzR7Nnz9b+/fslScuWLdNtt92m/fv3q6CgQB988IFycnL0/PPPH8NXBwAAALVspmma4S7iaCoqKpScnCyfz9fkL7IHDx7Utm3bNGDAgHpTqtAxrFmzRhdffLFmzJih+++/P9zlRAS+JwAAgBU1JxtIjOggyuzdu7fBDf7ffvtt8N6WcePGhaEqAADQUUVjE/mOgnt0EFX+/Oc/68EHH9SFF16oPn36aPfu3Vq9erX27NmjSZMmadSoUeEuEQAAdBDR2kS+oyDoIKqcc845GjZsmNasWaNvvvlGdrtdp556qmbPnq2bbrop3OUBAIAOJFqbyHcUBB1ElREjRsjj8YS7DAAAgKhtIt9REHQAAACAFqhtIl+0vUiOdAejORGGoAMAAAC0kGuQi4AToVh1DQAAAIDlEHQAAAAAWA5BBwAAAIDlEHQAAAAAWA5BBwAAAB2eUWIoe3W2jBIj3KWglRB0AAAA0KEZJYbcy90qWFcg93I3YcciCDoAAADo0LzbvMGmn3abXUXbi8JdEloBQQdtbvv27bLZbJo0aVK97Q6HQzabrc1eNz09Xenp6W12fgAAYA3OAc5gyPGbfjnSHeEuCa2AoGMxtaHix4/4+HilpaXpmmuu0b/+9a9wl9hqJk2aJJvNpu3bt4e7FAAAEMVcg1zyXOXR9JHT5bnKQwNQi4gNdwFoGwMHDtS1114rSfruu+/0wQcf6IUXXtDKlStVWFioc889N8wVSs8884y+//77Njt/YWFhm50bAABYi2uQi4BjMQQdizrxxBM1d+7cetvuuusuzZ8/X3feeaeKiorCUteP/fznP2/T8w8cOLBNzw8AAIDIxdS1DuSWW26RJH344YeSJJvNJofDodLSUk2cOFG9e/dWTExMvRD0zjvvaOzYserZs6cSEhJ00kkn6a677mp0JMbv9+v+++/XiSeeqMTERJ144onKy8tTTU1No/Uc6R4dj8ejSy65RD169FBiYqLS09M1YcIEffrpp5IC9988/fTTkqQBAwYEp+k5HI7gOZq6R6eyslK5ubk65ZRTlJiYqO7du2vMmDF67733Guw7d+5c2Ww2FRUV6fnnn9fQoUPVqVMnpaam6tZbb9UPP/zQ4JiXX35Zo0ePVq9evZSYmKg+ffooIyNDL7/8cqPvFQAAAK2PEZ0O6Mfh4j//+Y9GjRql7t2766qrrtLBgweVlJQkSXr88cc1bdo0devWTWPHjlWvXr30z3/+U/Pnz5fX65XX61V8fHzwXDfccIOWLl2qAQMGaNq0aTp48KAWLFig999/P6T6brvtNi1YsEDdu3fXuHHj1KtXL+3atUtr1qzRsGHDdPrpp+v3v/+9li1bpo8//li33nqrunXrJklHXXzg4MGDuvDCC7Vu3TqdddZZ+v3vf6/y8nKtWLFCb7zxhl544QX99re/bXDcwoULtXr1arndbl144YVavXq1Hn30Ue3bt09//vOfg/s9/vjjuummm5SamqrLLrtMPXr0UFlZmdatW6dXXnlFl19+eUhfCwAAALSQ2QILFy40+/fvbyYkJJgjRoww//GPfzS5b3V1tTlv3jzzhBNOMBMSEswzzjjD/Nvf/hbS6/l8PlOS6fP5mtznhx9+MD///HPzhx9+COncVrNt2zZTkpmZmdngc3PmzDElmU6n0zRN05RkSjInT55sHj58uN6+n332mRkbG2sOGTLE3LdvX73P5eXlmZLMBx98MLjN6/WakswhQ4aY3333XXD7V199Zfbs2dOUZGZlZdU7z+jRo82fXoJ//etfTUnm4MGDG7zuoUOHzLKysuDHWVlZpiRz27ZtjX4t+vfvb/bv37/etnnz5pmSzP/+7/82a2pqgts3bNhgxsfHm926dTMrKiqC23Nzc01JZnJysrl58+bg9u+//948+eSTzZiYGLO0tDS4/ayzzjLj4+PN8vLyBvX89P20Nb4nAACAFTUnG5imaYY8dW3FihXKyclRbm6uNmzYoCFDhigzM1N79uxpdP+77rpLTz75pAoKCvT5559r6tSpuuyyy/TRRx+1IJZFEMOQsrMDzxFoy5Ytmjt3rubOnas//OEPuuCCC3T33XcrMTFR8+fPD+4XHx+vP/3pT7Lb7fWOf/LJJ3X48GEVFBSoR48e9T43Y8YMHX/88XrhhReC25555hlJ0pw5c3TccccFt/ft21e33nprs+t+7LHHJEmPPPJIg9eNjY1VSkpKs8/VmKefflpxcXG677776o1snXnmmcrKytK3336rV199tcFxt956qwYNGhT8uFOnTrr66qtVU1Oj9evX19s3Li5OcXFxDc7x0/cDAABal1FiKHt1Ng0/IakFU9cWLFigKVOmaPLkyZKkJ554QqtWrdLSpUs1c+bMBvs/++yzuvPOO3XppZdKkm688UatWbNGDz30kJ577rljLD9MDENyuyW7XcrPlzweyRVZq3R8+eWXmjdvnqTAL94pKSm65pprNHPmTA0ePDi434ABA9SzZ88Gx3/wwQeSpDfeeKPR1cvi4uK0efPm4Mcff/yxJOn8889vsG9j25qybt06JSQkaPTo0c0+prkqKiq0detWnXrqqerXr1+DzzudTi1evFgbN27UhAkT6n1u2LBhDfavPce3334b3HbVVVdpxowZOv3003XNNdfI6XTqvPPOC04HBAAAbcMoMeRe7pbdZlf+P/JZJhqhBZ3q6mqtX79es2bNCm6LiYlRRkaGiouLGz2mqqpKiYmJ9bZ16tRJ7777bpOvU1VVpaqqquDHFRUVoZTZ9rzeQMjx+wPPRUURF3QyMzO1evXqo+7X1AjJN998I0n1Rn+OxOfzKSYmptHQFMoojM/nU9++fRUT0/rrZNReR03Vk5qaWm+/H2ssqMTGBr59/H5/cNvtt9+uHj166PHHH9dDDz2kBx98ULGxsRozZowefvhhDRgw4JjfBwAAaMi7zRts+Gm32VW0vYig08GF9Nvkvn375Pf7G/yimJKSorKyskaPyczM1IIFC/Tvf/9bNTU1euutt7Ry5Urt3r27ydfJy8tTcnJy8JGWlhZKmW3P6awLOX6/9KOVvqJNU6ue1f5iX1FRIdM0m3zUSk5OVk1Njfbt29fgXOXl5c2up1u3biorK2typbZjUfuemqqn9ho+ltEXm82m3/3ud/rwww+1d+9evfLKK/rNb34jj8ej//qv/6oXigAAQOtxDnAGQ47f9MuR7gh3SQizNl9e+pFHHtFJJ52kU045RfHx8br55ps1efLkI/7FftasWfL5fMHHrl272rrM0Lhcgelq06dH5LS11jBy5EhJdVPYjmbIkCGSpL///e8NPtfYtqaMGDFCVVVVWrt27VH3rb2vqLnhISkpSSeccIK2bNmi0tLSBp+vXVZ76NChza73SHr06KFx48ZpxYoVuvDCC/X5559ry5YtrXJuAABQn2uQS56rPJo+cjrT1iApxKDTs2dP2e32Bn8RLy8vV+/evRs95vjjj9err76qyspK7dixQ5s3b1aXLl10wgknNPk6CQkJSkpKqveIOC6XtGCBJUOOJN10002KjY3VLbfcop07dzb4/LfffltvQYnae1ruvvtuVVZWBreXlpbqkUceafbrTps2TVLg5v/a6XO1Dh8+XO/a6969uySFFISzsrJ06NAhzZo1q96I1L/+9S8tW7ZMycnJGjduXLPP91NFRUX1zitJhw4dCr6Xn07jBAAArcc1yKUFmQsIOZAU4j068fHxGjZsmAoLC4O/DNbU1KiwsFA333zzEY9NTExU3759dejQIb388su68sorW1w02t7pp5+uxx57TDfeeKMGDRqkSy+9VAMHDtSBAwe0detWrV27VpMmTdITTzwhKXAj/+TJk/XUU09p8ODBuuyyy1RVVaUVK1bol7/8pV577bVmve6ll16q22+/XQ8++KBOOukkXXbZZerVq5dKS0tVWFio22+/Xb///e8lSRdeeKEefPBB3XDDDbr88st13HHHqX///g0WEvixGTNmaNWqVXr22We1adMmXXTRRdqzZ49WrFihw4cPa/HixeratWuLv27jxo1TUlKSfvnLX6p///46dOiQ3nrrLX3++ee64oor1L9//xafGwAAAM0X8qprOTk5ysrK0vDhwzVixAjl5+ersrIyuArbxIkT1bdvX+Xl5UmS/vGPf6i0tFRDhw5VaWmp5s6dq5qaGs2YMaN13wla3ZQpUzR06FAtWLBA77zzjv76178qOTlZP//5z5Wdna2srKx6+y9evFgnn3yyFi9erIULF6pfv37KycnRlVde2eygI0kPPPCARo0apYULF+qll17SwYMHlZqaqgsvvFAXX3xxcL9f//rX+tOf/qTFixfroYce0qFDhzR69OgjBp3ExES9/fbbuv/++7VixQo9/PDD6ty5s0aPHq077rhD5513XuhfqB/Jy8vT6tWrtW7dOv31r3/Vcccdp4EDB+rxxx/Xddddd0znBgAAQPPZzJ/Os2mGhQsX6oEHHlBZWZmGDh2qRx99NHhPh8PhUHp6upYtWyZJWrt2rW688UZt3bpVXbp00aWXXqr77rtPffr0afbrVVRUKDk5WT6fr8lpbAcPHtS2bds0YMAApgcB4nsCAABYU3OygdTCoNPeCDpA6PieAAAAVtTcoNPmq64BAAAAoTBKDGWvzpZRYoS7FEQxgg4AAAAihlFiyL3crYJ1BXIvdxN20GIEHQAAAEQM7zZvsOmn3WZX0faicJeEKEXQAQAAQMRwDnAGQ47f9MuR7gh3SYhSIS8vDQAAALQV1yCXPFd5VLS9SI50B80/0WKWCzpRsIgc0C74XgAARCvXIBcBB8fMMlPX7Ha7JOnQoUNhrgSIDIcPH5YkxcZa7u8ZAAAAR2WZoBMXF6eEhAT5fD7+kg0osMa83W4P/hEAAACgI7HUn3p79uyp0tJSffXVV0pOTlZcXJxsNlu4ywLalWmaqqysVEVFhVJTU/keAAAAHZKlgk5tZ9R9+/aptLQ0zNUA4WOz2dStWzclJyeHuxQAAICwsFTQkQJhJykpSYcOHZLf7w93OUBYxMXFMWUNABBWRokh7zavnAOcLCyAsLBc0KkVFxenuLi4cJcBAADQ4RglhtzL3bLb7Mr/R748V3kIO2h3llmMAAAAAJHBu80bbPhpt9lVtL0o3CWhAyLoAAAAoFU5BziDIcdv+uVId4S7JHRAlp26BgAAgPBwDXLJc5VHRduL5Eh3MG0NYWEzo6DpTEVFhZKTk+Xz+YIrqwEAAADoeJqbDZi6BgAAAMByCDoAAAAALIegAwAAAMByCDoAAAAALIegAwAAgCYZJYayV2fLKDHCXQoQEoIOAAAAGmWUGHIvd6tgXYHcy92EHUQVgg4AAAAa5d3mDTb9tNvsKtpeFO6SgGYj6AAAAKBRzgHOYMjxm3450h3hLglotthwFwAAAIDI5Brkkucqj4q2F8mR7pBrkCvcJQHNZjNN0wx3EUfT3O6nAAAAAKytudmAqWsAAAAALIegAwAAAMByCDoAAAAALIegAwAAAMByCDoAAAAdgGFI2dmBZ6AjIOgAAABYnGFIbrdUUBB4JuygIyDoAAAAWJzXK9ntkt8feC4qCndFQNsj6AAAAFic01kXcvx+yeEId0VA24sNdwEAAABoWy6X5PEERnIcjsDHgNURdAAAADoAl4uAg46FqWsAAAAALIegAwAAAMByCDoAAAAALIegAwAAAMByCDoAAABRwjCk7GwafgLNQdABAACIAoYhud1SQUHgmbADHBlBBwAAIAp4vXUNP+32QE8cAE0j6AAAAEQBp7Mu5Pj9gcafAJpGw1AAAIAo4HJJHk9gJMfhoPkncDQEHQAAgCjhchFwgOZi6hoAAAAAyyHoAAAAALAcgg4AAAAAyyHoAAAAALAcgg4AAEA7MwwpO5umn0BbIugAAAC0I8OQ3G6poCDwTNgB2gZBBwAAoB15vXVNP+32QF8cAK2PoAMAANCOnM66kOP3B5p/Amh9NAwFAABoRy6X5PEERnIcDhqAAm2FoAMAANDOXC4CDtDWmLoGAAAAwHIIOgAAAAAsh6ADAAAAwHIIOgAAAAAsh6ADAADQQoYhZWfT9BOIRC0KOosWLVJ6eroSExM1cuRIrVu37oj75+fna9CgQerUqZPS0tKUnZ2tgwcPtqhgAACASGAYktstFRQEngk7QGQJOeisWLFCOTk5ys3N1YYNGzRkyBBlZmZqz549je7//PPPa+bMmcrNzdWmTZu0ZMkSrVixQnfccccxFw8AABAuXm9d00+7PdAXB0DkCDnoLFiwQFOmTNHkyZN12mmn6YknnlDnzp21dOnSRvd///33de655+qaa65Renq6LrnkEl199dVHHQUCAACIZE5nXcjx+wPNPwFEjpCCTnV1tdavX6+MjIy6E8TEKCMjQ8XFxY0ec84552j9+vXBYLN161a9/vrruvTSS5t8naqqKlVUVNR7AAAARBKXS/J4pOnTA880AAUiS2woO+/bt09+v18pKSn1tqekpGjz5s2NHnPNNddo3759Ou+882Sapg4fPqypU6cecepaXl6e5s2bF0ppAAAA7c7lIuAAkarNV10rKirSvffeq8cee0wbNmzQypUrtWrVKt1zzz1NHjNr1iz5fL7gY9euXW1dJgAAAAALCWlEp2fPnrLb7SovL6+3vby8XL179270mNmzZ2vChAm6/vrrJUmDBw9WZWWlbrjhBt15552KiWmYtRISEpSQkBBKaQAAAAAQFNKITnx8vIYNG6bCwsLgtpqaGhUWFmrUqFGNHvP99983CDN2u12SZJpmqPUCAAAAwFGFNKIjSTk5OcrKytLw4cM1YsQI5efnq7KyUpMnT5YkTZw4UX379lVeXp4kaezYsVqwYIHOPPNMjRw5Ulu2bNHs2bM1duzYYOABAAAAgNYUctAZP3689u7dqzlz5qisrExDhw7V6tWrgwsU7Ny5s94Izl133SWbzaa77rpLpaWlOv744zV27FjNnz+/9d4FAABACxlGoCeO08nCAoCV2MwomD9WUVGh5ORk+Xw+JSUlhbscAABgEYYhud11vXBYJhqIfM3NBm2+6hoAAECk8nrrQo7dLhUVhbsiAK2FoAMAADosp7Mu5Pj9ksMR7ooAtJaQ79EBAACwCpcrMF2tqCgQcpi2BlgHQQcAAHRoLhcBB7Aipq4BAAAAsByCDgAAAADLIegAAAAAsByCDgAAAADLIegAAABLMAwpOzvwDAAEHQAAEPUMQ3K7pYKCwDNhBwBBBwAARD2vt67pp90e6IsDoGMj6AAAgKjndNaFHL8/0PwTQMdGw1AAABD1XC7J4wmM5DgcNAAFQNABAAAW4XIRcADUYeoaAAAAAMsh6AAAAACwHIIOAAAAAMsh6AAAAACwHIIOAACIGIYhZWfT8BPAsSPoAACAiGAYktstFRQEngk7AI4FQQcAAEQEr7eu4afdHuiJAwAtRdABAAARwemsCzl+f6DxJwC0FA1DAQBARHC5JI8nMJLjcND8E8CxIegAAICI4XIRcAC0DqauAQAAALAcgg4AAAAAyyHoAAAAALAcgg4AAAAAyyHoAACAVmcYUnY2TT8BhA9BBwAAtCrDkNxuqaAg8EzYARAOBB0AANCqvN66pp92e6AvDgC0N4IOAABoVU5nXcjx+wPNPwGgvdEwFAAAtCqXS/J4AiM5DgcNQAGEB0EHAAC0OpeLgAMgvJi6BgAAAMByCDoAAAAALIegAwAAAMByCDoAAAAALIegAwAAmmQYUnY2TT8BRB+CDgAAaJRhSG63VFAQeCbsAIgmBB0AANAor7eu6afdHuiLAwDRgqADAAAa5XTWhRy/P9D8EwCiBQ1DAQBAo1wuyeMJjOQ4HDQABRBdCDoAAKBJLhcBB0B0YuoaAAAAAMsh6AAAAACwHIIOAAAAAMsh6AAAAACwHIIOAAAWZxhSdjYNPwF0LAQdAAAszDAkt1sqKAg8E3YAdBQEHQAALMzrrWv4abcHeuIAQEdA0AEAwMKczrqQ4/cHGn8CQEdAw1AAACzM5ZI8nsBIjsNB808AHQdBBwAAi3O5CDgAOh6mrgEAAACwHIIOAAAAAMsh6AAAAACwHIIOAAAAAMsh6AAAECUMQ8rOpuknADQHQQcAgChgGJLbLRUUBJ4JOwBwZC0KOosWLVJ6eroSExM1cuRIrVu3rsl9HQ6HbDZbg8eYMWNaXDQAAB2N11vX9NNuD/TFAQA0LeSgs2LFCuXk5Cg3N1cbNmzQkCFDlJmZqT179jS6/8qVK7V79+7g49NPP5Xdbtdvf/vbYy4eAICOwumsCzl+f6D5JwCgaTbTNM1QDhg5cqTOPvtsLVy4UJJUU1OjtLQ03XLLLZo5c+ZRj8/Pz9ecOXO0e/duHXfccc16zYqKCiUnJ8vn8ykpKSmUcgEAsAzDCIzkOBw0AAXQcTU3G8SGctLq6mqtX79es2bNCm6LiYlRRkaGiouLm3WOJUuW6KqrrjpiyKmqqlJVVVXw44qKilDKBADAklwuAg4ANFdIU9f27dsnv9+vlJSUettTUlJUVlZ21OPXrVunTz/9VNdff/0R98vLy1NycnLwkZaWFkqZAAAAADq4dl11bcmSJRo8eLBGjBhxxP1mzZoln88XfOzataudKgQAAABgBSFNXevZs6fsdrvKy8vrbS8vL1fv3r2PeGxlZaWWL1+uu++++6ivk5CQoISEhFBKAwAAAICgkEZ04uPjNWzYMBUWFga31dTUqLCwUKNGjTrisX/5y19UVVWla6+9tmWVAgAAAEAzhTx1LScnR4sXL9bTTz+tTZs26cYbb1RlZaUmT54sSZo4cWK9xQpqLVmyROPGjVOPHj2OvWoAAKKYYUjZ2TT9BIC2FNLUNUkaP3689u7dqzlz5qisrExDhw7V6tWrgwsU7Ny5UzEx9fNTSUmJ3n33Xb355putUzUAAFHKMCS3O9APJz9f8nhYSQ0A2kLIfXTCgT46AACryM6WCgrqmn9Ony4tWBDuqgAgejQ3G7TrqmsAAHR0TmddyPH7A80/AQCtL+SpawAAoOVcrsB0taKiQMhh2hoAtA2CDgAA7czlIuAAQFtj6hoAAAAAyyHoAAAAALAcgg4AAAAAyyHoAAAAALAcgg4AAC1gGIGeOIYR7koAAI0h6AAAECLDkNzuQONPt5uwAwCRiKADAECIvN66hp92e6AnDgAgshB0AAAIkdNZF3L8/kDjTwBAZKFhKAAAIXK5JI8nMJLjcND8EwAiEUEHAIAWcLkIOAAQyZi6BgAAAMByCDoAAAAALIegAwAAAMByCDoAAAAALIegAwDo0AxDys6m6ScAWA1BBwDQYRmG5HZLBQWBZ8IOAFgHQQcA0GF5vXVNP+32QF8cAIA1EHQAAB2W01kXcvz+QPNPAIA10DAUANBhuVySxxMYyXE4aAAKAFZC0AEAdGguFwEHAKyIqWsAAAAALIegAwAAAMByCDoAAAAALIegAwAAAMByCDoAgKhnGFJ2Ng0/AQB1CDoAgKhmGJLbLRUUBJ4JOwAAiaADAIhyXm9dw0+7PdATBwAAgg4AIKo5nXUhx+8PNP4EAICGoQCAqOZySR5PYCTH4aD5JwAggKADAIh6LhcBBwBQH1PXAAAAAFgOQQcAAACA5RB0AAAAAFgOQQcAAACA5RB0AAARwzCk7GyafgIAjh1BBwAQEQxDcrulgoLAM2EHAHAsCDoAgIjg9dY1/bTbA31xAABoKYIOACAiOJ11IcfvDzT/BACgpWgYCgCICC6X5PEERnIcDhqAAgCODUEHABAxXC4CDgCgdTB1DQAAAIDlEHQAAAAAWA5BBwAAAIDlEHQAAAAAWA5BBwDQ6gxDys6m6ScAIHwIOgCAVmUYktstFRQEngk7AIBwIOgAAFqV11vX9NNuD/TFAQCgvRF0AACtyumsCzl+f6D5JwAA7Y2GoQCAVuVySR5PYCTH4aABKAAgPAg6AIBW53IRcAAA4cXUNQAAAACWQ9ABAAAAYDkEHQAAAACWQ9ABAAAAYDkEHQBAowxDys6m4ScAIDoRdAAADRiG5HZLBQWBZ8IOACDaEHQAAA14vXUNP+32QE8cAACiCUEHANCA01kXcvz+QONPAACiSYuCzqJFi5Senq7ExESNHDlS69atO+L+3377raZNm6bU1FQlJCTo5JNP1uuvv96iggEAbc/lkjweafr0wDPNPwEA0SY21ANWrFihnJwcPfHEExo5cqTy8/OVmZmpkpIS9erVq8H+1dXVuvjii9WrVy+99NJL6tu3r3bs2KFu3bq1Rv0AgDbichFwAADRy2aaphnKASNHjtTZZ5+thQsXSpJqamqUlpamW265RTNnzmyw/xNPPKEHHnhAmzdvVlxcXLNeo6qqSlVVVcGPKyoqlJaWJp/Pp6SkpFDKBQAAAGAhFRUVSk5OPmo2CGnqWnV1tdavX6+MjIy6E8TEKCMjQ8XFxY0eYxiGRo0apWnTpiklJUWnn3667r33Xvn9/iZfJy8vT8nJycFHWlpaKGUCAAAA6OBCCjr79u2T3+9XSkpKve0pKSkqKytr9JitW7fqpZdekt/v1+uvv67Zs2froYce0v/8z/80+TqzZs2Sz+cLPnbt2hVKmQAAAAA6uJDv0QlVTU2NevXqpf/93/+V3W7XsGHDVFpaqgceeEC5ubmNHpOQkKCEhIS2Lg0AAACARYUUdHr27Cm73a7y8vJ628vLy9W7d+9Gj0lNTVVcXJzsdntw26mnnqqysjJVV1crPj6+BWUDAJrLMAJ9cZxOFhcAAHQcIU1di4+P17Bhw1RYWBjcVlNTo8LCQo0aNarRY84991xt2bJFNTU1wW1ffPGFUlNTCTkA0MYMQ3K7pYKCwLNhhLsiAADaR8h9dHJycrR48WI9/fTT2rRpk2688UZVVlZq8uTJkqSJEydq1qxZwf1vvPFGffPNN7r11lv1xRdfaNWqVbr33ns1bdq01nsXAIBGeb11TT/tdqmoKNwVAQDQPkK+R2f8+PHau3ev5syZo7KyMg0dOlSrV68OLlCwc+dOxcTU5ae0tDS98cYbys7O1hlnnKG+ffvq1ltv1R//+MfWexcAgEY5nVJ+fl3YcTjCXREAAO0j5D464dDctbIBAA0ZRmAkx+HgHh0AQPRrbjZo81XXAADh5XIRcAAAHU/I9+gAAAAAQKQj6AAAAACwHIIOAAAAAMsh6AAAAACwHIIOAEQJw5Cys2n6CQBAcxB0ACAKGIbkdksFBYFnwg4AAEdG0AGAKOD11jX9tNsDfXEAAEDTCDoAEAWczrqQ4/cHmn8CAICm0TAUAKKAyyV5PIGRHIeDBqAAABwNQQcAooTLRcABAKC5mLoGAAAAwHIIOgAAAAAsh6ADAAAAwHIIOgAAAAAsh6ADAO3IMKTsbBp+AgDQ1gg6ANBODENyu6WCgsAzYQcAgLZD0AGAduL11jX8tNsDPXEAAEDbIOgAQDtxOutCjt8faPwJAADaBg1DAaCduFySxxMYyXE4aP4JAEBbIugAQDtyuQg4AAC0B6auAQAAALAcgg4AAAAAyyHoAAAAALAcgg4AAAAAyyHoAEALGIaUnU3TTwAAIhVBBwBCZBiS2y0VFASeCTsAAEQegg4AhMjrrWv6abcH+uIAAIDIQtABgBA5nXUhx+8PNP8EAACRhYahABAil0vyeAIjOQ4HDUABAIhEBB0AaAGXi4ADAEAkY+oaAAAAAMsh6AAAAACwHIIOAAAAAMsh6AAAAACwHIIOgA7LMKTsbBp+AgBgRQQdAB2SYUhut1RQEHgm7AAAYC0EHQAdktdb1/DTbg/0xAEAANZB0AHQITmddSHH7w80/gQAANZBw1AAHZLLJXk8gZEch4PmnwAAWA1BB0CH5XIRcAAAsCqmrgEAAACwHIIOAAAAAMsh6AAAAACwHIIOAAAAAMsh6ACIeoYhZWfT9BMAANQh6ACIaoYhud1SQUHgmbADAAAkgg6AKOf11jX9tNsDfXEAAAAIOgCimtNZF3L8/kDzTwAAABqGAohqLpfk8QRGchwOGoACAIAAgg6AqOdyEXAAAEB9TF0DAAAAYDkEHQAAAACWQ9ABAAAAYDkEHQAAAACWQ9ABEDEMQ8rOpuknAAA4dgQdABHBMCS3WyooCDwTdgAAwLEg6ACICF5vXdNPuz3QFwcAAKClCDoAIoLTWRdy/P5A808AAICWomEogIjgckkeT2Akx+GgASgAADg2LRrRWbRokdLT05WYmKiRI0dq3bp1Te67bNky2Wy2eo/ExMQWFwzAulwuacECQg4AADh2IQedFStWKCcnR7m5udqwYYOGDBmizMxM7dmzp8ljkpKStHv37uBjx44dx1Q0AAAAABxJyEFnwYIFmjJliiZPnqzTTjtNTzzxhDp37qylS5c2eYzNZlPv3r2Dj5SUlGMqGgAAAACOJKSgU11drfXr1ysjI6PuBDExysjIUHFxcZPHfffdd+rfv7/S0tLkdrv12WefHfF1qqqqVFFRUe8BAAAAAM0VUtDZt2+f/H5/gxGZlJQUlZWVNXrMoEGDtHTpUnk8Hj333HOqqanROeeco6+++qrJ18nLy1NycnLwkZaWFkqZAAAAADq4Nl9eetSoUZo4caKGDh2q0aNHa+XKlTr++OP15JNPNnnMrFmz5PP5go9du3a1dZkAWolhSNnZNPwEAADhFdLy0j179pTdbld5eXm97eXl5erdu3ezzhEXF6czzzxTW7ZsaXKfhIQEJSQkhFIagAhgGJLbHeiFk58fWC6aFdQAAEA4hDSiEx8fr2HDhqmwsDC4raamRoWFhRo1alSzzuH3+/XJJ58oNTU1tEoBRDyvt67hp90e6IkDAAAQDiFPXcvJydHixYv19NNPa9OmTbrxxhtVWVmpyZMnS5ImTpyoWbNmBfe/++679eabb2rr1q3asGGDrr32Wu3YsUPXX399670LABHB6awLOX5/oPEnAABAOIQ0dU2Sxo8fr71792rOnDkqKyvT0KFDtXr16uACBTt37lRMTF1+2r9/v6ZMmaKysjL97Gc/07Bhw/T+++/rtNNOa713ASAiuFyB6WpFRYGQw7Q1AAAQLjbTNM1wF3E0FRUVSk5Ols/nU1JSUrjLAQAAABAmzc0Gbb7qGgAAAAC0N4IOAAAAAMsh6AAAAACwHIIOAAAAAMsh6ABolGFI2dmBZwAAgGhD0AHQgGFIbrdUUBB4JuwAAIBoQ9AB0IDXW9f0024P9MUBAACIJgQdAA04nXUhx+8PNP8EAACIJrHhLgBA5HG5JI8nMJLjcAQ+BgAAiCYEHQCNcrkIOAAAIHoxdQ0AAACA5RB0AAAAAFgOQQcAAACA5RB0AAAAAFgOQQewMMOQsrNp+AkAADoegg5gUYYhud1SQUHgmbADAAA6EoIOYFFeb13DT7s90BMHAACgoyDoABbldNaFHL8/0PgTAACgo6BhKGBRLpfk8QRGchwOmn8CAICOhaADWJjLRcABAAAdE1PXAAAAADQtSpdxJegAAAAAaFwUL+NK0AEAAADQuChexpWgAwAAAKBxUbyMK4sRAFHAMAJ/UHE6WVwAAAC0oyhextVmmqYZ7iKOpqKiQsnJyfL5fEpKSgp3OUC7qp0aW/uHFI8nqv4fAwAAIoVF/nLa3GzA1DUgwkXx1FgAABAponhRgZYi6AARLoqnxgIAgEjRAf9yStABIlzt1Njp05m2BgAAWqgD/uWUe3QAAACAjsAwonJRgZ9qbjZg1TUAAAAgmrR0UQGXK6oDTqiYugYAAABEiw64qEBLEXQAAACAaNEBFxVoKYIOAAAAEC064KICLcU9OkA7skifLgAAEC61y7FaYFGBtsaqa0A7qZ1SW/sHGJaKBgCgA+Ovny3W3GzA1DWgnTClFgAASGJBgXZC0AHaCVNqAQCAJP762U4IOkA7qZ1SO30609YAAOjQ+Otnu+AeHQAAAKC9GQYLCrRQc7MBq64BAAAALdXSRQVcLgJOG2PqGgAAANASLCoQ0Qg6AAAAQEuwqEBEI+gAAAAALcGiAhGNe3SAENHfCwAAC2rJD/jaJVVZVCAiseoaEILaqbi1f7hhmWgAACyAH/BRpbnZgKlrQAiYigsAgAXxA96SCDpACJiKCwCABfED3pK4RwcIAVNxAQCwIH7AWxL36AAAAMAaWDGoQ+AeHQAAAHQcNO/ETxB0AAAAEP1YUAA/QdABAABA9GNBAfwEixEAAAAg+rGgAH6CoIMOi/sVAQCIUC39Ie1y8UMdQay6hg6JBsgAAEQofkjjKFh1DTgC7lcEACBC8UMarYSggw6J+xUBAIhQ/JBGK+EeHXRI3K8IAECE4oc0Wgn36AAAAKD1seoP2gj36AAAACA8ahcUKCgIPBtGuCtCB9SioLNo0SKlp6crMTFRI0eO1Lp165p13PLly2Wz2TRu3LiWvCwAAACiAQsKIAKEHHRWrFihnJwc5ebmasOGDRoyZIgyMzO1Z8+eIx63fft23X777Tr//PNbXCwAAACiAAsKIAKEfI/OyJEjdfbZZ2vhwoWSpJqaGqWlpemWW27RzJkzGz3G7/frggsu0O9+9zv9/e9/17fffqtXX321ydeoqqpSVVVV8OOKigqlpaVxjw4AAEC0MAwWFECbaJN7dKqrq7V+/XplZGTUnSAmRhkZGSouLm7yuLvvvlu9evXSdddd16zXycvLU3JycvCRlpYWSpnoYAxDys5m+i8AAG2ipT9oXS5pwQJCDsImpKCzb98++f1+paSk1NuekpKisrKyRo959913tWTJEi1evLjZrzNr1iz5fL7gY9euXaGUiQ6Eex0BAGhD/KBFFGvTVdcOHDigCRMmaPHixerZs2ezj0tISFBSUlK9B9AY7nUEAKAN8YMWUSykoNOzZ0/Z7XaVl5fX215eXq7evXs32P/LL7/U9u3bNXbsWMXGxio2NlbPPPOMDMNQbGysvvzyy2OrHh0e9zoCANCG+EGLKBYbys7x8fEaNmyYCgsLg0tE19TUqLCwUDfffHOD/U855RR98skn9bbdddddOnDggB555BHuvcExo3kyAABtiB+0iGIhBR1JysnJUVZWloYPH64RI0YoPz9flZWVmjx5siRp4sSJ6tu3r/Ly8pSYmKjTTz+93vHdunWTpAbbgZZyufj/LgAAbYYftIhSIQed8ePHa+/evZozZ47Kyso0dOhQrV69OrhAwc6dOxUT06a3/gAAAADAEYXcRyccmrtWNgAAAABra5M+OgAAAAAQDQg6AAAAACyHoIOI0NKmywAAAEBjCDoIO5ouAwAAoLURdBB2NF0GAABAayPoIOxougwAAIDWFnIfHaC10XQZAAAArY2gg4hA02UAAAC0JqauAQAAALAcgg4AAAAAyyHoAAAAALAcgg4AAAAAyyHooFUZhpSdTdNPAAAAhBdBB63GMCS3WyooCDwTdgAAABAuBB20Gq+3rumn3R7oiwMAAACEA0EHrcbprAs5fn+g+ScAAAAQDjQMRatxuSSPJzCS43DQABQAAADhQ9BBq3K5CDgAAAAIP6auAQAAALAcgg4AAAAAyyHoAAAAALAcgg4AAAAAyyHooAHDkLKzafgJAACA6EXQQT2GIbndUkFB4JmwAwAAgGhE0EE9Xm9dw0+7PdATBwAAAIg2BB3U43TWhRy/P9D4EwAAAIg2NAxFPS6X5PEERnIcDpp/AgAAIDoRdNCAy0XAAQAAQHRj6hoAAAAAyyHoAAAAALAcgg4AAAAAyyHoAAAAALAcgo6FGYaUnU3TTwAAAHQ8BB2LMgzJ7ZYKCgLPhB0AAAB0JAQdi/J665p+2u2BvjgAAABAR0HQsSinsy7k+P2B5p8AAABAR0HDUItyuSSPJzCS43DQABQAAAAdC0HHwlwuAg4AAAA6JqauAQAAALAcgg4AAAAAyyHoAAAAALAcgg4AAAAAyyHoRAHDkLKzafoJAAAANBdBJ8IZhuR2SwUFgWfCDgAAAHB0BJ0I5/XWNf202wN9cQAAAAAcGUEnwjmddSHH7w80/wQAAABwZDQMjXAul+TxBEZyHA4agAIAAADNQdCJAi4XAQcAAAAIBVPXAAAAAFgOQQcAAACA5RB0AAAAAFgOQQcAAACA5RB02olhSNnZNPwEAAAA2gNBpx0YhuR2SwUFgWfCDgAAANC2CDrtwOuta/hptwd64gAAAABoOwSdduB01oUcvz/Q+BMAAABA26FhaDtwuSSPJzCS43DQ/BMAAABoawSdduJyEXAAAACA9sLUNQAAAACWQ9ABAAAAYDktCjqLFi1Senq6EhMTNXLkSK1bt67JfVeuXKnhw4erW7duOu644zR06FA9++yzLS4YAAAAAI4m5KCzYsUK5eTkKDc3Vxs2bNCQIUOUmZmpPXv2NLp/9+7ddeedd6q4uFj/+te/NHnyZE2ePFlvvPHGMRcPAAAAAI2xmaZphnLAyJEjdfbZZ2vhwoWSpJqaGqWlpemWW27RzJkzm3WOs846S2PGjNE999zTrP0rKiqUnJwsn8+npKSkUMptdYYR6IvjdLK4AAAAANDempsNQhrRqa6u1vr165WRkVF3gpgYZWRkqLi4+KjHm6apwsJClZSU6IILLmhyv6qqKlVUVNR7RALDkNxuqaAg8GwY4a4IAAAAQGNCCjr79u2T3+9XSkpKve0pKSkqKytr8jifz6cuXbooPj5eY8aMUUFBgS6++OIm98/Ly1NycnLwkZaWFkqZbcbrrWv6abcH+uIAAAAAiDztsupa165dtXHjRn344YeaP3++cnJyVHSElDBr1iz5fL7gY9euXe1R5lE5nXUhx+8PNP8EAAAAEHlCahjas2dP2e12lZeX19teXl6u3r17N3lcTEyMTjzxREnS0KFDtWnTJuXl5cnRRFJISEhQQkJCKKW1C5dL8ngCIzkOB/foAAAAAJEqpBGd+Ph4DRs2TIWFhcFtNTU1Kiws1KhRo5p9npqaGlVVVYXy0hHD5ZIWLCDkAAAAAJEspBEdScrJyVFWVpaGDx+uESNGKD8/X5WVlZo8ebIkaeLEierbt6/y8vIkBe63GT58uAYOHKiqqiq9/vrrevbZZ/X444+37jsBAAAAgP9fyEFn/Pjx2rt3r+bMmaOysjINHTpUq1evDi5QsHPnTsXE1A0UVVZW6qabbtJXX32lTp066ZRTTtFzzz2n8ePHt967AAAAAIAfCbmPTjhEUh8dAAAAAOHTJn10AAAAACAaEHQAAAAAWA5BBwAAAIDlEHQAAAAAWA5BBwAAAIDlEHQAAAAAWA5BBwAAAIDlEHQAAAAAWA5BBwAAAIDlEHQAAAAAWA5BBwAAAIDlEHQAAAAAWA5BBwAAAIDlEHQAAAAAWA5BBwAAAIDlEHQAAAAAWE5suAtoDtM0JUkVFRVhrgQAAABAONVmgtqM0JSoCDoHDhyQJKWlpYW5EgAAAACR4MCBA0pOTm7y8zbzaFEoAtTU1Ojrr79W165dZbPZwlpLRUWF0tLStGvXLiUlJYW1FkQfrh8cC64ftBTXDo4F1w+ORVtcP6Zp6sCBA+rTp49iYpq+EycqRnRiYmLUr1+/cJdRT1JSEt/saDGuHxwLrh+0FNcOjgXXD45Fa18/RxrJqcViBAAAAAAsh6ADAAAAwHIIOiFKSEhQbm6uEhISwl0KohDXD44F1w9aimsHx4LrB8cinNdPVCxGAAAAAAChYEQHAAAAgOUQdAAAAABYDkEHAAAAgOUQdAAAAABYDkEHAAAAgOUQdBqxaNEipaenKzExUSNHjtS6deuOuP9f/vIXnXLKKUpMTNTgwYP1+uuvt1OliEShXD+LFy/W+eefr5/97Gf62c9+poyMjKNeb7CuUP/fU2v58uWy2WwaN25c2xaIiBbq9fPtt99q2rRpSk1NVUJCgk4++WR+fnVgoV4/+fn5GjRokDp16qS0tDRlZ2fr4MGD7VQtIsU777yjsWPHqk+fPrLZbHr11VePekxRUZHOOussJSQk6MQTT9SyZcvarD6Czk+sWLFCOTk5ys3N1YYNGzRkyBBlZmZqz549je7//vvv6+qrr9Z1112njz76SOPGjdO4ceP06aeftnPliAShXj9FRUW6+uqr5fV6VVxcrLS0NF1yySUqLS1t58oRbqFeO7W2b9+u22+/Xeeff347VYpIFOr1U11drYsvvljbt2/XSy+9pJKSEi1evFh9+/Zt58oRCUK9fp5//nnNnDlTubm52rRpk5YsWaIVK1bojjvuaOfKEW6VlZUaMmSIFi1a1Kz9t23bpjFjxsjpdGrjxo36/e9/r+uvv15vvPFG2xRoop4RI0aY06ZNC37s9/vNPn36mHl5eY3uf+WVV5pjxoypt23kyJHm//t//69N60RkCvX6+anDhw+bXbt2NZ9++um2KhERqiXXzuHDh81zzjnH/L//+z8zKyvLdLvd7VApIlGo18/jjz9unnDCCWZ1dXV7lYgIFur1M23aNPPCCy+sty0nJ8c899xz27RORDZJ5iuvvHLEfWbMmGH+4he/qLdt/PjxZmZmZpvUxIjOj1RXV2v9+vXKyMgIbouJiVFGRoaKi4sbPaa4uLje/pKUmZnZ5P6wrpZcPz/1/fff69ChQ+revXtblYkI1NJr5+6771avXr103XXXtUeZiFAtuX4Mw9CoUaM0bdo0paSk6PTTT9e9994rv9/fXmUjQrTk+jnnnHO0fv364PS2rVu36vXXX9ell17aLjUjerX3782xbXLWKLVv3z75/X6lpKTU256SkqLNmzc3ekxZWVmj+5eVlbVZnYhMLbl+fuqPf/yj+vTp0+B/ArC2llw77777rpYsWaKNGze2Q4WIZC25frZu3aq3335b//3f/63XX39dW7Zs0U033aRDhw4pNze3PcpGhGjJ9XPNNddo3759Ou+882Sapg4fPqypU6cydQ1H1dTvzRUVFfrhhx/UqVOnVn09RnSACHHfffdp+fLleuWVV5SYmBjuchDBDhw4oAkTJmjx4sXq2bNnuMtBFKqpqVGvXr30v//7vxo2bJjGjx+vO++8U0888US4S0MUKCoq0r333qvHHntMGzZs0MqVK7Vq1Srdc8894S4NqIcRnR/p2bOn7Ha7ysvL620vLy9X7969Gz2md+/eIe0P62rJ9VPrwQcf1H333ac1a9bojDPOaMsyEYFCvXa+/PJLbd++XWPHjg1uq6mpkSTFxsaqpKREAwcObNuiETFa8v+e1NRUxcXFyW63B7edeuqpKisrU3V1teLj49u0ZkSOllw/s2fP1oQJE3T99ddLkgYPHqzKykrdcMMNuvPOOxUTw9/R0bimfm9OSkpq9dEciRGdeuLj4zVs2DAVFhYGt9XU1KiwsFCjRo1q9JhRo0bV21+S3nrrrSb3h3W15PqRpD/96U+65557tHr1ag0fPrw9SkWECfXaOeWUU/TJJ59o48aNwYfL5QquYpOWltae5SPMWvL/nnPPPVdbtmwJBmRJ+uKLL5SamkrI6WBacv18//33DcJMbWgO3JMONK7df29ukyUOotjy5cvNhIQEc9myZebnn39u3nDDDWa3bt3MsrIy0zRNc8KECebMmTOD+7/33ntmbGys+eCDD5qbNm0yc3Nzzbi4OPOTTz4J11tAGIV6/dx3331mfHy8+dJLL5m7d+8OPg4cOBCut4AwCfXa+SlWXevYQr1+du7caXbt2tW8+eabzZKSEvO1114ze/XqZf7P//xPuN4CwijU6yc3N9fs2rWr+cILL5hbt24133zzTXPgwIHmlVdeGa63gDA5cOCA+dFHH5kfffSRKclcsGCB+dFHH5k7duwwTdM0Z86caU6YMCG4/9atW83OnTubf/jDH8xNmzaZixYtMu12u7l69eo2qY+g04iCggLz5z//uRkfH2+OGDHC/OCDD4KfGz16tJmVlVVv/xdffNE8+eSTzfj4ePMXv/iFuWrVqnauGJEklOunf//+pqQGj9zc3PYvHGEX6v97foygg1Cvn/fff98cOXKkmZCQYJ5wwgnm/PnzzcOHD7dz1YgUoVw/hw4dMufOnWsOHDjQTExMNNPS0sybbrrJ3L9/f/sXjrDyer2N/h5Te71kZWWZo0ePbnDM0KFDzfj4ePOEE04wn3rqqTarz2aajDECAAAAsBbu0QEAAABgOQQdAAAAAJZD0AEAAABgOQQdAAAAAJZD0AEAAABgOQQdAAAAAJZD0AEAAABgOQQdAAAAAJZD0AEAAABgOQQdAAAAAJZD0AEAAABgOf8f2Jzj1ZAxxOsAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Train model\n",
        "\n",
        "The hole idea of training is for a model to   move from some *unknown* parameters ( these may be random ) to some *known* paramaters.\n",
        "\n",
        "From a poor representation of data to a better representation of data.\n",
        "\n",
        "One way to mesure how poor or good our models predictions are, we could use a loss function.\n",
        "\n",
        "Things we need to train:\n",
        "\n",
        "* **Loss function** A function to mesure how wrong your models predictions are compared to the actual answer (value).\n",
        "\n",
        "* **Optimizer:** Takes into account the loss of a model, and adjusts the model's parameters ( e.g weight and bias) to improve the loss function.\n",
        "\n",
        "Inside the **Optimizer** you will often have to set two parameters:\n",
        "* params - the model parameters you'd like to optimize, for example `params=model_0.parameters()`\n",
        "* lr ( learning rate ) - the learning rate is a hyperparameter that defines how big/small the optimizer changes the parameters with each step ( a small `lr` resultss in small changes )\n",
        "\n",
        "And specifically for PyTorch, we need:\n",
        "* A training loop\n",
        "* A testing loop"
      ],
      "metadata": {
        "id": "rAx5vyj1xSre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up a loss function\n",
        "loss_fn = nn.L1Loss()\n",
        "\n",
        "# Set up an optimizer ( stochastic gradient descent )\n",
        "optimizer = torch.optim.SGD(params=model_0.parameters(),\n",
        "                            lr=0.0001) # Learning rate = possibly the most important hyperparameter you can set"
      ],
      "metadata": {
        "id": "gjclreFMxoVo"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building a training and testing loop in PyTorch\n",
        "A couple of things we need in a training loop:\n",
        "0. Loop through the data\n",
        "1. Forward pass ( this involves data moving through our model's `forward()` functions) to make predictions on data\n",
        "2. Calculate the loss (compare forward pass predictions to ground truth labels)\n",
        "3. Optimizer zero grad\n",
        "4. Loss baackward - move backward through the network to calculate the gradients of each of the parameters of our model with respect to the loss **backpropagation**\n",
        "5. Optimizer step - use the optimizer to adjust our model's parameter to improve the loss (**gradient descent**)"
      ],
      "metadata": {
        "id": "A7Cyzf5U8mQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up a manual seed\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# An epoch is one loop through the data... ( this is a hyperparameter, because we set it our selfes)\n",
        "epochs = 20000\n",
        "\n",
        "# Track experiments\n",
        "epoch_count = []\n",
        "loss_values = []\n",
        "test_loss_values = []\n",
        "\n",
        "# 0. Loop through the data\n",
        "for epoch in range(epochs):\n",
        "  # Set the model to trainign mode\n",
        "  model_0.train() # Train mode in PyTorch set all params that require gradients\n",
        "\n",
        "  # 1. Forward pass\n",
        "  y_pred = model_0(X_train)\n",
        "  \n",
        "  # 2. Calculate the loss\n",
        "  loss = loss_fn(y_pred, y_train)\n",
        "\n",
        "  # 3. Optimizer 0 grad\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  # 4. Loss backwards ( backpropagation )\n",
        "  loss.backward()\n",
        "\n",
        "  # 5. Step the optimizer (perform gradient descent)\n",
        "  optimizer.step() # by default how the optimizer changes will acumulate through the loop. We have to 0 them again at the begining of a new loop ( step 3 )\n",
        "\n",
        "  ### Testing\n",
        "  model_0.eval() # Turns off gradient tracking\n",
        "  with torch.inference_mode(): # turns off gradient tracking and some other things not needed for testing\n",
        "    # 1. In training, we still use the forward pass method\n",
        "    test_pred = model_0(X_test)\n",
        "    # 2. Calculate the loss\n",
        "    test_loss = loss_fn(test_pred, y_test)\n",
        "\n",
        "  # Prints out what is happening\n",
        "  if epoch % 10 == 0:\n",
        "    # Track for future experiments\n",
        "    epoch_count.append(epoch)\n",
        "    loss_values.append(loss)\n",
        "    test_loss_values.append(test_loss)\n",
        "    print(f\"Epoch: {epoch} \\n Test: {loss} \\n Test loss: {test_loss} \\n\")\n",
        "    print(model_0.state_dict())"
      ],
      "metadata": {
        "id": "svmT00yD8srL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dd3f2a5-eeb9-413d-e481-84efa5f5c9f5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch: 10000 \n",
            " Test: 0.024338718503713608 \n",
            " Test loss: 0.056904666125774384 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5789])), ('bias', tensor([0.3508]))])\n",
            "Epoch: 10010 \n",
            " Test: 0.024304334074258804 \n",
            " Test loss: 0.05682197958230972 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5791])), ('bias', tensor([0.3508]))])\n",
            "Epoch: 10020 \n",
            " Test: 0.024269994348287582 \n",
            " Test loss: 0.056742750108242035 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5793])), ('bias', tensor([0.3507]))])\n",
            "Epoch: 10030 \n",
            " Test: 0.024235591292381287 \n",
            " Test loss: 0.05666007846593857 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5794])), ('bias', tensor([0.3506]))])\n",
            "Epoch: 10040 \n",
            " Test: 0.024201255291700363 \n",
            " Test loss: 0.05658425763249397 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5796])), ('bias', tensor([0.3506]))])\n",
            "Epoch: 10050 \n",
            " Test: 0.024166909977793694 \n",
            " Test loss: 0.05650157853960991 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5798])), ('bias', tensor([0.3505]))])\n",
            "Epoch: 10060 \n",
            " Test: 0.024132560938596725 \n",
            " Test loss: 0.05642234534025192 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5800])), ('bias', tensor([0.3504]))])\n",
            "Epoch: 10070 \n",
            " Test: 0.02409818209707737 \n",
            " Test loss: 0.05634312704205513 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5801])), ('bias', tensor([0.3503]))])\n",
            "Epoch: 10080 \n",
            " Test: 0.024063829332590103 \n",
            " Test loss: 0.05626044422388077 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5803])), ('bias', tensor([0.3503]))])\n",
            "Epoch: 10090 \n",
            " Test: 0.024029428139328957 \n",
            " Test loss: 0.05617775395512581 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5805])), ('bias', tensor([0.3502]))])\n",
            "Epoch: 10100 \n",
            " Test: 0.02399510145187378 \n",
            " Test loss: 0.05610193684697151 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5806])), ('bias', tensor([0.3501]))])\n",
            "Epoch: 10110 \n",
            " Test: 0.023960720747709274 \n",
            " Test loss: 0.05602271109819412 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5808])), ('bias', tensor([0.3501]))])\n",
            "Epoch: 10120 \n",
            " Test: 0.023926356807351112 \n",
            " Test loss: 0.055936623364686966 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5810])), ('bias', tensor([0.3500]))])\n",
            "Epoch: 10130 \n",
            " Test: 0.023892030119895935 \n",
            " Test loss: 0.055860795080661774 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5812])), ('bias', tensor([0.3499]))])\n",
            "Epoch: 10140 \n",
            " Test: 0.02385767176747322 \n",
            " Test loss: 0.05577811598777771 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5813])), ('bias', tensor([0.3498]))])\n",
            "Epoch: 10150 \n",
            " Test: 0.023823264986276627 \n",
            " Test loss: 0.05569542571902275 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5815])), ('bias', tensor([0.3498]))])\n",
            "Epoch: 10160 \n",
            " Test: 0.02378893457353115 \n",
            " Test loss: 0.05561620742082596 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5817])), ('bias', tensor([0.3497]))])\n",
            "Epoch: 10170 \n",
            " Test: 0.02375456877052784 \n",
            " Test loss: 0.05554039031267166 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5818])), ('bias', tensor([0.3496]))])\n",
            "Epoch: 10180 \n",
            " Test: 0.023720186203718185 \n",
            " Test loss: 0.055454302579164505 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5820])), ('bias', tensor([0.3496]))])\n",
            "Epoch: 10190 \n",
            " Test: 0.023685883730649948 \n",
            " Test loss: 0.05537847802042961 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5822])), ('bias', tensor([0.3495]))])\n",
            "Epoch: 10200 \n",
            " Test: 0.02365151047706604 \n",
            " Test loss: 0.05529579520225525 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5823])), ('bias', tensor([0.3494]))])\n",
            "Epoch: 10210 \n",
            " Test: 0.023617107421159744 \n",
            " Test loss: 0.05521656945347786 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5825])), ('bias', tensor([0.3493]))])\n",
            "Epoch: 10220 \n",
            " Test: 0.023582767695188522 \n",
            " Test loss: 0.0551338866353035 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5827])), ('bias', tensor([0.3493]))])\n",
            "Epoch: 10230 \n",
            " Test: 0.023548435419797897 \n",
            " Test loss: 0.05505465343594551 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5829])), ('bias', tensor([0.3492]))])\n",
            "Epoch: 10240 \n",
            " Test: 0.0235140360891819 \n",
            " Test loss: 0.054978836327791214 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5830])), ('bias', tensor([0.3491]))])\n",
            "Epoch: 10250 \n",
            " Test: 0.023479729890823364 \n",
            " Test loss: 0.05489616468548775 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5832])), ('bias', tensor([0.3491]))])\n",
            "Epoch: 10260 \n",
            " Test: 0.02344534918665886 \n",
            " Test loss: 0.05481347441673279 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5834])), ('bias', tensor([0.3490]))])\n",
            "Epoch: 10270 \n",
            " Test: 0.023411009460687637 \n",
            " Test loss: 0.0547342412173748 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5835])), ('bias', tensor([0.3489]))])\n",
            "Epoch: 10280 \n",
            " Test: 0.02337666228413582 \n",
            " Test loss: 0.05465502291917801 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5837])), ('bias', tensor([0.3488]))])\n",
            "Epoch: 10290 \n",
            " Test: 0.023342274129390717 \n",
            " Test loss: 0.05457235127687454 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5839])), ('bias', tensor([0.3488]))])\n",
            "Epoch: 10300 \n",
            " Test: 0.023307884112000465 \n",
            " Test loss: 0.05449651926755905 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5841])), ('bias', tensor([0.3487]))])\n",
            "Epoch: 10310 \n",
            " Test: 0.02327357791364193 \n",
            " Test loss: 0.054413843899965286 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5842])), ('bias', tensor([0.3486]))])\n",
            "Epoch: 10320 \n",
            " Test: 0.023239195346832275 \n",
            " Test loss: 0.0543346107006073 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5844])), ('bias', tensor([0.3486]))])\n",
            "Epoch: 10330 \n",
            " Test: 0.023204851895570755 \n",
            " Test loss: 0.05425193905830383 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5846])), ('bias', tensor([0.3485]))])\n",
            "Epoch: 10340 \n",
            " Test: 0.023170508444309235 \n",
            " Test loss: 0.05417270213365555 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5847])), ('bias', tensor([0.3484]))])\n",
            "Epoch: 10350 \n",
            " Test: 0.02313610538840294 \n",
            " Test loss: 0.054090023040771484 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5849])), ('bias', tensor([0.3483]))])\n",
            "Epoch: 10360 \n",
            " Test: 0.02310173586010933 \n",
            " Test loss: 0.05401420593261719 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5851])), ('bias', tensor([0.3483]))])\n",
            "Epoch: 10370 \n",
            " Test: 0.023067370057106018 \n",
            " Test loss: 0.05392811447381973 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5853])), ('bias', tensor([0.3482]))])\n",
            "Epoch: 10380 \n",
            " Test: 0.02303304336965084 \n",
            " Test loss: 0.05385228991508484 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5854])), ('bias', tensor([0.3481]))])\n",
            "Epoch: 10390 \n",
            " Test: 0.022998664528131485 \n",
            " Test loss: 0.05377305671572685 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5856])), ('bias', tensor([0.3480]))])\n",
            "Epoch: 10400 \n",
            " Test: 0.02296435460448265 \n",
            " Test loss: 0.05369038134813309 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5858])), ('bias', tensor([0.3480]))])\n",
            "Epoch: 10410 \n",
            " Test: 0.022929945960640907 \n",
            " Test loss: 0.053607694804668427 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5859])), ('bias', tensor([0.3479]))])\n",
            "Epoch: 10420 \n",
            " Test: 0.022895583882927895 \n",
            " Test loss: 0.05353188514709473 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5861])), ('bias', tensor([0.3478]))])\n",
            "Epoch: 10430 \n",
            " Test: 0.022861208766698837 \n",
            " Test loss: 0.05344577878713608 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5863])), ('bias', tensor([0.3478]))])\n",
            "Epoch: 10440 \n",
            " Test: 0.022826874628663063 \n",
            " Test loss: 0.053366560488939285 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5865])), ('bias', tensor([0.3477]))])\n",
            "Epoch: 10450 \n",
            " Test: 0.022792508825659752 \n",
            " Test loss: 0.053290754556655884 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5866])), ('bias', tensor([0.3476]))])\n",
            "Epoch: 10460 \n",
            " Test: 0.02275819331407547 \n",
            " Test loss: 0.05320807173848152 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5868])), ('bias', tensor([0.3475]))])\n",
            "Epoch: 10470 \n",
            " Test: 0.022723788395524025 \n",
            " Test loss: 0.053125374019145966 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5870])), ('bias', tensor([0.3475]))])\n",
            "Epoch: 10480 \n",
            " Test: 0.0226894523948431 \n",
            " Test loss: 0.05304615572094917 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5871])), ('bias', tensor([0.3474]))])\n",
            "Epoch: 10490 \n",
            " Test: 0.022655043751001358 \n",
            " Test loss: 0.05296692997217178 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5873])), ('bias', tensor([0.3473]))])\n",
            "Epoch: 10500 \n",
            " Test: 0.022620711475610733 \n",
            " Test loss: 0.052884239703416824 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5875])), ('bias', tensor([0.3473]))])\n",
            "Epoch: 10510 \n",
            " Test: 0.022586364299058914 \n",
            " Test loss: 0.05280842259526253 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5876])), ('bias', tensor([0.3472]))])\n",
            "Epoch: 10520 \n",
            " Test: 0.02255203202366829 \n",
            " Test loss: 0.052725739777088165 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5878])), ('bias', tensor([0.3471]))])\n",
            "Epoch: 10530 \n",
            " Test: 0.022517677396535873 \n",
            " Test loss: 0.05264651030302048 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5880])), ('bias', tensor([0.3470]))])\n",
            "Epoch: 10540 \n",
            " Test: 0.02248329296708107 \n",
            " Test loss: 0.05256382375955582 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5882])), ('bias', tensor([0.3470]))])\n",
            "Epoch: 10550 \n",
            " Test: 0.022448953241109848 \n",
            " Test loss: 0.05248459428548813 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5883])), ('bias', tensor([0.3469]))])\n",
            "Epoch: 10560 \n",
            " Test: 0.022414550185203552 \n",
            " Test loss: 0.05240192264318466 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5885])), ('bias', tensor([0.3468]))])\n",
            "Epoch: 10570 \n",
            " Test: 0.02238021418452263 \n",
            " Test loss: 0.052326101809740067 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5887])), ('bias', tensor([0.3468]))])\n",
            "Epoch: 10580 \n",
            " Test: 0.02234586887061596 \n",
            " Test loss: 0.052243418991565704 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5888])), ('bias', tensor([0.3467]))])\n",
            "Epoch: 10590 \n",
            " Test: 0.02231152169406414 \n",
            " Test loss: 0.052164189517498016 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5890])), ('bias', tensor([0.3466]))])\n",
            "Epoch: 10600 \n",
            " Test: 0.022277140989899635 \n",
            " Test loss: 0.052084971219301224 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5892])), ('bias', tensor([0.3465]))])\n",
            "Epoch: 10610 \n",
            " Test: 0.022242791950702667 \n",
            " Test loss: 0.05200228840112686 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5894])), ('bias', tensor([0.3465]))])\n",
            "Epoch: 10620 \n",
            " Test: 0.022208387032151222 \n",
            " Test loss: 0.0519195981323719 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5895])), ('bias', tensor([0.3464]))])\n",
            "Epoch: 10630 \n",
            " Test: 0.022174060344696045 \n",
            " Test loss: 0.051843781024217606 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5897])), ('bias', tensor([0.3463]))])\n",
            "Epoch: 10640 \n",
            " Test: 0.02213967964053154 \n",
            " Test loss: 0.051764555275440216 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5899])), ('bias', tensor([0.3463]))])\n",
            "Epoch: 10650 \n",
            " Test: 0.022105315700173378 \n",
            " Test loss: 0.05167846754193306 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5900])), ('bias', tensor([0.3462]))])\n",
            "Epoch: 10660 \n",
            " Test: 0.0220709890127182 \n",
            " Test loss: 0.05160263925790787 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5902])), ('bias', tensor([0.3461]))])\n",
            "Epoch: 10670 \n",
            " Test: 0.022036630660295486 \n",
            " Test loss: 0.051519960165023804 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5904])), ('bias', tensor([0.3460]))])\n",
            "Epoch: 10680 \n",
            " Test: 0.022002223879098892 \n",
            " Test loss: 0.051437269896268845 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5906])), ('bias', tensor([0.3460]))])\n",
            "Epoch: 10690 \n",
            " Test: 0.021967893466353416 \n",
            " Test loss: 0.05135805159807205 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5907])), ('bias', tensor([0.3459]))])\n",
            "Epoch: 10700 \n",
            " Test: 0.021933527663350105 \n",
            " Test loss: 0.051282234489917755 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5909])), ('bias', tensor([0.3458]))])\n",
            "Epoch: 10710 \n",
            " Test: 0.0218991469591856 \n",
            " Test loss: 0.0511961467564106 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5911])), ('bias', tensor([0.3458]))])\n",
            "Epoch: 10720 \n",
            " Test: 0.021864842623472214 \n",
            " Test loss: 0.051120322197675705 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5912])), ('bias', tensor([0.3457]))])\n",
            "Epoch: 10730 \n",
            " Test: 0.021830469369888306 \n",
            " Test loss: 0.05103763937950134 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5914])), ('bias', tensor([0.3456]))])\n",
            "Epoch: 10740 \n",
            " Test: 0.02179606631398201 \n",
            " Test loss: 0.05095841363072395 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5916])), ('bias', tensor([0.3455]))])\n",
            "Epoch: 10750 \n",
            " Test: 0.021761726588010788 \n",
            " Test loss: 0.05087573081254959 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5918])), ('bias', tensor([0.3455]))])\n",
            "Epoch: 10760 \n",
            " Test: 0.021727394312620163 \n",
            " Test loss: 0.050796497613191605 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5919])), ('bias', tensor([0.3454]))])\n",
            "Epoch: 10770 \n",
            " Test: 0.021692994982004166 \n",
            " Test loss: 0.05072068050503731 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5921])), ('bias', tensor([0.3453]))])\n",
            "Epoch: 10780 \n",
            " Test: 0.02165868878364563 \n",
            " Test loss: 0.05063800886273384 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5923])), ('bias', tensor([0.3452]))])\n",
            "Epoch: 10790 \n",
            " Test: 0.021624308079481125 \n",
            " Test loss: 0.05055531859397888 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5924])), ('bias', tensor([0.3452]))])\n",
            "Epoch: 10800 \n",
            " Test: 0.021589968353509903 \n",
            " Test loss: 0.050476085394620895 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5926])), ('bias', tensor([0.3451]))])\n",
            "Epoch: 10810 \n",
            " Test: 0.021555621176958084 \n",
            " Test loss: 0.0503968670964241 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5928])), ('bias', tensor([0.3450]))])\n",
            "Epoch: 10820 \n",
            " Test: 0.021521233022212982 \n",
            " Test loss: 0.050314195454120636 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5929])), ('bias', tensor([0.3450]))])\n",
            "Epoch: 10830 \n",
            " Test: 0.02148684300482273 \n",
            " Test loss: 0.050238363444805145 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5931])), ('bias', tensor([0.3449]))])\n",
            "Epoch: 10840 \n",
            " Test: 0.021452536806464195 \n",
            " Test loss: 0.05015568807721138 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5933])), ('bias', tensor([0.3448]))])\n",
            "Epoch: 10850 \n",
            " Test: 0.02141815423965454 \n",
            " Test loss: 0.050076454877853394 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5935])), ('bias', tensor([0.3447]))])\n",
            "Epoch: 10860 \n",
            " Test: 0.02138381078839302 \n",
            " Test loss: 0.04999377578496933 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5936])), ('bias', tensor([0.3447]))])\n",
            "Epoch: 10870 \n",
            " Test: 0.0213494673371315 \n",
            " Test loss: 0.04991454631090164 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5938])), ('bias', tensor([0.3446]))])\n",
            "Epoch: 10880 \n",
            " Test: 0.021315064281225204 \n",
            " Test loss: 0.04983186721801758 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5940])), ('bias', tensor([0.3445]))])\n",
            "Epoch: 10890 \n",
            " Test: 0.021280691027641296 \n",
            " Test loss: 0.04975605010986328 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5941])), ('bias', tensor([0.3445]))])\n",
            "Epoch: 10900 \n",
            " Test: 0.021246328949928284 \n",
            " Test loss: 0.049669958651065826 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5943])), ('bias', tensor([0.3444]))])\n",
            "Epoch: 10910 \n",
            " Test: 0.021212005987763405 \n",
            " Test loss: 0.04959413409233093 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5945])), ('bias', tensor([0.3443]))])\n",
            "Epoch: 10920 \n",
            " Test: 0.02117762342095375 \n",
            " Test loss: 0.049514900892972946 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5947])), ('bias', tensor([0.3442]))])\n",
            "Epoch: 10930 \n",
            " Test: 0.021143313497304916 \n",
            " Test loss: 0.04943222552537918 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5948])), ('bias', tensor([0.3442]))])\n",
            "Epoch: 10940 \n",
            " Test: 0.021108904853463173 \n",
            " Test loss: 0.04934953898191452 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5950])), ('bias', tensor([0.3441]))])\n",
            "Epoch: 10950 \n",
            " Test: 0.02107454277575016 \n",
            " Test loss: 0.04927372932434082 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5952])), ('bias', tensor([0.3440]))])\n",
            "Epoch: 10960 \n",
            " Test: 0.021040167659521103 \n",
            " Test loss: 0.04918763041496277 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5953])), ('bias', tensor([0.3440]))])\n",
            "Epoch: 10970 \n",
            " Test: 0.02100583352148533 \n",
            " Test loss: 0.04910840839147568 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5955])), ('bias', tensor([0.3439]))])\n",
            "Epoch: 10980 \n",
            " Test: 0.020971467718482018 \n",
            " Test loss: 0.04903259873390198 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5957])), ('bias', tensor([0.3438]))])\n",
            "Epoch: 10990 \n",
            " Test: 0.020937152206897736 \n",
            " Test loss: 0.048949915915727615 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5959])), ('bias', tensor([0.3437]))])\n",
            "Epoch: 11000 \n",
            " Test: 0.02090274728834629 \n",
            " Test loss: 0.04886721819639206 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5960])), ('bias', tensor([0.3437]))])\n",
            "Epoch: 11010 \n",
            " Test: 0.020868411287665367 \n",
            " Test loss: 0.04878799989819527 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5962])), ('bias', tensor([0.3436]))])\n",
            "Epoch: 11020 \n",
            " Test: 0.020834006369113922 \n",
            " Test loss: 0.04870877414941788 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5964])), ('bias', tensor([0.3435]))])\n",
            "Epoch: 11030 \n",
            " Test: 0.020799672231078148 \n",
            " Test loss: 0.04862608388066292 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5965])), ('bias', tensor([0.3435]))])\n",
            "Epoch: 11040 \n",
            " Test: 0.02076532319188118 \n",
            " Test loss: 0.04855026677250862 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5967])), ('bias', tensor([0.3434]))])\n",
            "Epoch: 11050 \n",
            " Test: 0.020730990916490555 \n",
            " Test loss: 0.04846758395433426 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5969])), ('bias', tensor([0.3433]))])\n",
            "Epoch: 11060 \n",
            " Test: 0.02069663628935814 \n",
            " Test loss: 0.04838835448026657 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5970])), ('bias', tensor([0.3432]))])\n",
            "Epoch: 11070 \n",
            " Test: 0.020662251859903336 \n",
            " Test loss: 0.04830567166209221 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5972])), ('bias', tensor([0.3432]))])\n",
            "Epoch: 11080 \n",
            " Test: 0.020627912133932114 \n",
            " Test loss: 0.04822644591331482 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5974])), ('bias', tensor([0.3431]))])\n",
            "Epoch: 11090 \n",
            " Test: 0.020593509078025818 \n",
            " Test loss: 0.04814376309514046 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5976])), ('bias', tensor([0.3430]))])\n",
            "Epoch: 11100 \n",
            " Test: 0.020559173077344894 \n",
            " Test loss: 0.04806794598698616 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5977])), ('bias', tensor([0.3430]))])\n",
            "Epoch: 11110 \n",
            " Test: 0.020524829626083374 \n",
            " Test loss: 0.0479852631688118 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5979])), ('bias', tensor([0.3429]))])\n",
            "Epoch: 11120 \n",
            " Test: 0.020490480586886406 \n",
            " Test loss: 0.04790603369474411 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5981])), ('bias', tensor([0.3428]))])\n",
            "Epoch: 11130 \n",
            " Test: 0.0204560998827219 \n",
            " Test loss: 0.04782681539654732 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5982])), ('bias', tensor([0.3427]))])\n",
            "Epoch: 11140 \n",
            " Test: 0.020421750843524933 \n",
            " Test loss: 0.047744132578372955 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5984])), ('bias', tensor([0.3427]))])\n",
            "Epoch: 11150 \n",
            " Test: 0.020387345924973488 \n",
            " Test loss: 0.047661442309617996 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5986])), ('bias', tensor([0.3426]))])\n",
            "Epoch: 11160 \n",
            " Test: 0.02035301923751831 \n",
            " Test loss: 0.0475856252014637 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5988])), ('bias', tensor([0.3425]))])\n",
            "Epoch: 11170 \n",
            " Test: 0.020318638533353806 \n",
            " Test loss: 0.04750639945268631 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5989])), ('bias', tensor([0.3424]))])\n",
            "Epoch: 11180 \n",
            " Test: 0.020284274592995644 \n",
            " Test loss: 0.04742031544446945 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5991])), ('bias', tensor([0.3424]))])\n",
            "Epoch: 11190 \n",
            " Test: 0.020249947905540466 \n",
            " Test loss: 0.04734448716044426 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5993])), ('bias', tensor([0.3423]))])\n",
            "Epoch: 11200 \n",
            " Test: 0.020215589553117752 \n",
            " Test loss: 0.0472618043422699 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5994])), ('bias', tensor([0.3422]))])\n",
            "Epoch: 11210 \n",
            " Test: 0.020181182771921158 \n",
            " Test loss: 0.04717911407351494 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5996])), ('bias', tensor([0.3422]))])\n",
            "Epoch: 11220 \n",
            " Test: 0.020146852359175682 \n",
            " Test loss: 0.047099895775318146 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.5998])), ('bias', tensor([0.3421]))])\n",
            "Epoch: 11230 \n",
            " Test: 0.02011248655617237 \n",
            " Test loss: 0.04702407866716385 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6000])), ('bias', tensor([0.3420]))])\n",
            "Epoch: 11240 \n",
            " Test: 0.020078105852007866 \n",
            " Test loss: 0.04693799093365669 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6001])), ('bias', tensor([0.3419]))])\n",
            "Epoch: 11250 \n",
            " Test: 0.02004380151629448 \n",
            " Test loss: 0.0468621663749218 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6003])), ('bias', tensor([0.3419]))])\n",
            "Epoch: 11260 \n",
            " Test: 0.02000942826271057 \n",
            " Test loss: 0.046779483556747437 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6005])), ('bias', tensor([0.3418]))])\n",
            "Epoch: 11270 \n",
            " Test: 0.019975025206804276 \n",
            " Test loss: 0.04670025780797005 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6006])), ('bias', tensor([0.3417]))])\n",
            "Epoch: 11280 \n",
            " Test: 0.019940685480833054 \n",
            " Test loss: 0.04661757871508598 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6008])), ('bias', tensor([0.3417]))])\n",
            "Epoch: 11290 \n",
            " Test: 0.01990635320544243 \n",
            " Test loss: 0.046538345515728 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6010])), ('bias', tensor([0.3416]))])\n",
            "Epoch: 11300 \n",
            " Test: 0.01987195387482643 \n",
            " Test loss: 0.0464625246822834 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6012])), ('bias', tensor([0.3415]))])\n",
            "Epoch: 11310 \n",
            " Test: 0.019837647676467896 \n",
            " Test loss: 0.046379853039979935 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6013])), ('bias', tensor([0.3414]))])\n",
            "Epoch: 11320 \n",
            " Test: 0.01980326697230339 \n",
            " Test loss: 0.046297162771224976 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6015])), ('bias', tensor([0.3414]))])\n",
            "Epoch: 11330 \n",
            " Test: 0.01976892724633217 \n",
            " Test loss: 0.04621792957186699 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6017])), ('bias', tensor([0.3413]))])\n",
            "Epoch: 11340 \n",
            " Test: 0.01973458006978035 \n",
            " Test loss: 0.0461387112736702 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6018])), ('bias', tensor([0.3412]))])\n",
            "Epoch: 11350 \n",
            " Test: 0.0197001900523901 \n",
            " Test loss: 0.04605603963136673 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6020])), ('bias', tensor([0.3412]))])\n",
            "Epoch: 11360 \n",
            " Test: 0.019665801897644997 \n",
            " Test loss: 0.04598020762205124 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6022])), ('bias', tensor([0.3411]))])\n",
            "Epoch: 11370 \n",
            " Test: 0.01963149569928646 \n",
            " Test loss: 0.045897532254457474 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6023])), ('bias', tensor([0.3410]))])\n",
            "Epoch: 11380 \n",
            " Test: 0.019597113132476807 \n",
            " Test loss: 0.04581829905509949 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6025])), ('bias', tensor([0.3409]))])\n",
            "Epoch: 11390 \n",
            " Test: 0.019562769681215286 \n",
            " Test loss: 0.04573562741279602 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6027])), ('bias', tensor([0.3409]))])\n",
            "Epoch: 11400 \n",
            " Test: 0.019528424367308617 \n",
            " Test loss: 0.045656394213438034 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6029])), ('bias', tensor([0.3408]))])\n",
            "Epoch: 11410 \n",
            " Test: 0.01949402317404747 \n",
            " Test loss: 0.045573703944683075 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6030])), ('bias', tensor([0.3407]))])\n",
            "Epoch: 11420 \n",
            " Test: 0.019459649920463562 \n",
            " Test loss: 0.045497894287109375 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6032])), ('bias', tensor([0.3407]))])\n",
            "Epoch: 11430 \n",
            " Test: 0.01942528784275055 \n",
            " Test loss: 0.04541180282831192 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6034])), ('bias', tensor([0.3406]))])\n",
            "Epoch: 11440 \n",
            " Test: 0.01939096488058567 \n",
            " Test loss: 0.045335978269577026 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6035])), ('bias', tensor([0.3405]))])\n",
            "Epoch: 11450 \n",
            " Test: 0.019356582313776016 \n",
            " Test loss: 0.04525674507021904 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6037])), ('bias', tensor([0.3404]))])\n",
            "Epoch: 11460 \n",
            " Test: 0.019322272390127182 \n",
            " Test loss: 0.045174069702625275 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6039])), ('bias', tensor([0.3404]))])\n",
            "Epoch: 11470 \n",
            " Test: 0.01928786374628544 \n",
            " Test loss: 0.045091383159160614 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6041])), ('bias', tensor([0.3403]))])\n",
            "Epoch: 11480 \n",
            " Test: 0.019253501668572426 \n",
            " Test loss: 0.045015573501586914 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6042])), ('bias', tensor([0.3402]))])\n",
            "Epoch: 11490 \n",
            " Test: 0.01921912655234337 \n",
            " Test loss: 0.04492947459220886 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6044])), ('bias', tensor([0.3402]))])\n",
            "Epoch: 11500 \n",
            " Test: 0.019184792414307594 \n",
            " Test loss: 0.04485025256872177 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6046])), ('bias', tensor([0.3401]))])\n",
            "Epoch: 11510 \n",
            " Test: 0.019150426611304283 \n",
            " Test loss: 0.04477444291114807 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6047])), ('bias', tensor([0.3400]))])\n",
            "Epoch: 11520 \n",
            " Test: 0.01911611109972 \n",
            " Test loss: 0.04469176009297371 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6049])), ('bias', tensor([0.3399]))])\n",
            "Epoch: 11530 \n",
            " Test: 0.019081706181168556 \n",
            " Test loss: 0.04460906237363815 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6051])), ('bias', tensor([0.3399]))])\n",
            "Epoch: 11540 \n",
            " Test: 0.019047370180487633 \n",
            " Test loss: 0.04452984407544136 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6053])), ('bias', tensor([0.3398]))])\n",
            "Epoch: 11550 \n",
            " Test: 0.019012965261936188 \n",
            " Test loss: 0.04445061832666397 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6054])), ('bias', tensor([0.3397]))])\n",
            "Epoch: 11560 \n",
            " Test: 0.018978631123900414 \n",
            " Test loss: 0.04436792805790901 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6056])), ('bias', tensor([0.3397]))])\n",
            "Epoch: 11570 \n",
            " Test: 0.018944282084703445 \n",
            " Test loss: 0.044292110949754715 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6058])), ('bias', tensor([0.3396]))])\n",
            "Epoch: 11580 \n",
            " Test: 0.01890994980931282 \n",
            " Test loss: 0.04420942813158035 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6059])), ('bias', tensor([0.3395]))])\n",
            "Epoch: 11590 \n",
            " Test: 0.018875595182180405 \n",
            " Test loss: 0.044130198657512665 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6061])), ('bias', tensor([0.3394]))])\n",
            "Epoch: 11600 \n",
            " Test: 0.0188412107527256 \n",
            " Test loss: 0.0440475158393383 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6063])), ('bias', tensor([0.3394]))])\n",
            "Epoch: 11610 \n",
            " Test: 0.01880687102675438 \n",
            " Test loss: 0.04396829009056091 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6065])), ('bias', tensor([0.3393]))])\n",
            "Epoch: 11620 \n",
            " Test: 0.018772467970848083 \n",
            " Test loss: 0.04388560727238655 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6066])), ('bias', tensor([0.3392]))])\n",
            "Epoch: 11630 \n",
            " Test: 0.01873813197016716 \n",
            " Test loss: 0.043809790164232254 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6068])), ('bias', tensor([0.3391]))])\n",
            "Epoch: 11640 \n",
            " Test: 0.01870378851890564 \n",
            " Test loss: 0.04372710734605789 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6070])), ('bias', tensor([0.3391]))])\n",
            "Epoch: 11650 \n",
            " Test: 0.01866944134235382 \n",
            " Test loss: 0.043647877871990204 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6071])), ('bias', tensor([0.3390]))])\n",
            "Epoch: 11660 \n",
            " Test: 0.018635058775544167 \n",
            " Test loss: 0.04356865957379341 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6073])), ('bias', tensor([0.3389]))])\n",
            "Epoch: 11670 \n",
            " Test: 0.0186007097363472 \n",
            " Test loss: 0.04348597675561905 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6075])), ('bias', tensor([0.3389]))])\n",
            "Epoch: 11680 \n",
            " Test: 0.018566306680440903 \n",
            " Test loss: 0.04340328648686409 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6076])), ('bias', tensor([0.3388]))])\n",
            "Epoch: 11690 \n",
            " Test: 0.018531978130340576 \n",
            " Test loss: 0.04332746937870979 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6078])), ('bias', tensor([0.3387]))])\n",
            "Epoch: 11700 \n",
            " Test: 0.018497595563530922 \n",
            " Test loss: 0.043248243629932404 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6080])), ('bias', tensor([0.3386]))])\n",
            "Epoch: 11710 \n",
            " Test: 0.01846323348581791 \n",
            " Test loss: 0.04316215589642525 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6082])), ('bias', tensor([0.3386]))])\n",
            "Epoch: 11720 \n",
            " Test: 0.018428906798362732 \n",
            " Test loss: 0.04308633133769035 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6083])), ('bias', tensor([0.3385]))])\n",
            "Epoch: 11730 \n",
            " Test: 0.018394548445940018 \n",
            " Test loss: 0.043003641068935394 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6085])), ('bias', tensor([0.3384]))])\n",
            "Epoch: 11740 \n",
            " Test: 0.018360141664743423 \n",
            " Test loss: 0.04292095825076103 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6087])), ('bias', tensor([0.3384]))])\n",
            "Epoch: 11750 \n",
            " Test: 0.018325811251997948 \n",
            " Test loss: 0.04284173995256424 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6088])), ('bias', tensor([0.3383]))])\n",
            "Epoch: 11760 \n",
            " Test: 0.018291447311639786 \n",
            " Test loss: 0.04276592284440994 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6090])), ('bias', tensor([0.3382]))])\n",
            "Epoch: 11770 \n",
            " Test: 0.01825706660747528 \n",
            " Test loss: 0.042679835110902786 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6092])), ('bias', tensor([0.3381]))])\n",
            "Epoch: 11780 \n",
            " Test: 0.018222760409116745 \n",
            " Test loss: 0.04260401055216789 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6094])), ('bias', tensor([0.3381]))])\n",
            "Epoch: 11790 \n",
            " Test: 0.018188387155532837 \n",
            " Test loss: 0.04252132773399353 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6095])), ('bias', tensor([0.3380]))])\n",
            "Epoch: 11800 \n",
            " Test: 0.01815398409962654 \n",
            " Test loss: 0.04244210198521614 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6097])), ('bias', tensor([0.3379]))])\n",
            "Epoch: 11810 \n",
            " Test: 0.01811964437365532 \n",
            " Test loss: 0.04235942289233208 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6099])), ('bias', tensor([0.3379]))])\n",
            "Epoch: 11820 \n",
            " Test: 0.018085310235619545 \n",
            " Test loss: 0.04228019714355469 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6100])), ('bias', tensor([0.3378]))])\n",
            "Epoch: 11830 \n",
            " Test: 0.018050912767648697 \n",
            " Test loss: 0.042204368859529495 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6102])), ('bias', tensor([0.3377]))])\n",
            "Epoch: 11840 \n",
            " Test: 0.01801660656929016 \n",
            " Test loss: 0.04212169721722603 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6104])), ('bias', tensor([0.3376]))])\n",
            "Epoch: 11850 \n",
            " Test: 0.017982225865125656 \n",
            " Test loss: 0.042039014399051666 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6106])), ('bias', tensor([0.3376]))])\n",
            "Epoch: 11860 \n",
            " Test: 0.017947886139154434 \n",
            " Test loss: 0.04195977374911308 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6107])), ('bias', tensor([0.3375]))])\n",
            "Epoch: 11870 \n",
            " Test: 0.017913538962602615 \n",
            " Test loss: 0.04188055545091629 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6109])), ('bias', tensor([0.3374]))])\n",
            "Epoch: 11880 \n",
            " Test: 0.017879148945212364 \n",
            " Test loss: 0.04179787635803223 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6111])), ('bias', tensor([0.3374]))])\n",
            "Epoch: 11890 \n",
            " Test: 0.01784476265311241 \n",
            " Test loss: 0.04172205179929733 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6112])), ('bias', tensor([0.3373]))])\n",
            "Epoch: 11900 \n",
            " Test: 0.017810454592108727 \n",
            " Test loss: 0.04163937643170357 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6114])), ('bias', tensor([0.3372]))])\n",
            "Epoch: 11910 \n",
            " Test: 0.017776072025299072 \n",
            " Test loss: 0.04156014323234558 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6116])), ('bias', tensor([0.3371]))])\n",
            "Epoch: 11920 \n",
            " Test: 0.017741728574037552 \n",
            " Test loss: 0.041477471590042114 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6118])), ('bias', tensor([0.3371]))])\n",
            "Epoch: 11930 \n",
            " Test: 0.017707383260130882 \n",
            " Test loss: 0.04139823839068413 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6119])), ('bias', tensor([0.3370]))])\n",
            "Epoch: 11940 \n",
            " Test: 0.017672982066869736 \n",
            " Test loss: 0.04131554812192917 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6121])), ('bias', tensor([0.3369]))])\n",
            "Epoch: 11950 \n",
            " Test: 0.017638608813285828 \n",
            " Test loss: 0.04123973846435547 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6123])), ('bias', tensor([0.3368]))])\n",
            "Epoch: 11960 \n",
            " Test: 0.017604246735572815 \n",
            " Test loss: 0.041153647005558014 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6124])), ('bias', tensor([0.3368]))])\n",
            "Epoch: 11970 \n",
            " Test: 0.017569923773407936 \n",
            " Test loss: 0.04107782989740372 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6126])), ('bias', tensor([0.3367]))])\n",
            "Epoch: 11980 \n",
            " Test: 0.017535541206598282 \n",
            " Test loss: 0.040998589247465134 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6128])), ('bias', tensor([0.3366]))])\n",
            "Epoch: 11990 \n",
            " Test: 0.017501231282949448 \n",
            " Test loss: 0.04091591387987137 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6129])), ('bias', tensor([0.3366]))])\n",
            "Epoch: 12000 \n",
            " Test: 0.017466822639107704 \n",
            " Test loss: 0.040833234786987305 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6131])), ('bias', tensor([0.3365]))])\n",
            "Epoch: 12010 \n",
            " Test: 0.01743246056139469 \n",
            " Test loss: 0.04075741767883301 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6133])), ('bias', tensor([0.3364]))])\n",
            "Epoch: 12020 \n",
            " Test: 0.017398085445165634 \n",
            " Test loss: 0.040671318769454956 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6135])), ('bias', tensor([0.3363]))])\n",
            "Epoch: 12030 \n",
            " Test: 0.01736375130712986 \n",
            " Test loss: 0.040592093020677567 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6136])), ('bias', tensor([0.3363]))])\n",
            "Epoch: 12040 \n",
            " Test: 0.01732938550412655 \n",
            " Test loss: 0.040516287088394165 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6138])), ('bias', tensor([0.3362]))])\n",
            "Epoch: 12050 \n",
            " Test: 0.017295069992542267 \n",
            " Test loss: 0.0404336042702198 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6140])), ('bias', tensor([0.3361]))])\n",
            "Epoch: 12060 \n",
            " Test: 0.017260665073990822 \n",
            " Test loss: 0.04035090655088425 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6141])), ('bias', tensor([0.3361]))])\n",
            "Epoch: 12070 \n",
            " Test: 0.0172263290733099 \n",
            " Test loss: 0.040271688252687454 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6143])), ('bias', tensor([0.3360]))])\n",
            "Epoch: 12080 \n",
            " Test: 0.017191924154758453 \n",
            " Test loss: 0.040192462503910065 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6145])), ('bias', tensor([0.3359]))])\n",
            "Epoch: 12090 \n",
            " Test: 0.01715759001672268 \n",
            " Test loss: 0.040109772235155106 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6147])), ('bias', tensor([0.3358]))])\n",
            "Epoch: 12100 \n",
            " Test: 0.01712324097752571 \n",
            " Test loss: 0.04003395512700081 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6148])), ('bias', tensor([0.3358]))])\n",
            "Epoch: 12110 \n",
            " Test: 0.017088908702135086 \n",
            " Test loss: 0.03995126485824585 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6150])), ('bias', tensor([0.3357]))])\n",
            "Epoch: 12120 \n",
            " Test: 0.01705455407500267 \n",
            " Test loss: 0.039872050285339355 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6152])), ('bias', tensor([0.3356]))])\n",
            "Epoch: 12130 \n",
            " Test: 0.017020169645547867 \n",
            " Test loss: 0.039789360016584396 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6153])), ('bias', tensor([0.3356]))])\n",
            "Epoch: 12140 \n",
            " Test: 0.016985829919576645 \n",
            " Test loss: 0.03971013426780701 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6155])), ('bias', tensor([0.3355]))])\n",
            "Epoch: 12150 \n",
            " Test: 0.01695142686367035 \n",
            " Test loss: 0.03962745517492294 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6157])), ('bias', tensor([0.3354]))])\n",
            "Epoch: 12160 \n",
            " Test: 0.016917090862989426 \n",
            " Test loss: 0.03955163434147835 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6159])), ('bias', tensor([0.3353]))])\n",
            "Epoch: 12170 \n",
            " Test: 0.016882747411727905 \n",
            " Test loss: 0.039468951523303986 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6160])), ('bias', tensor([0.3353]))])\n",
            "Epoch: 12180 \n",
            " Test: 0.016848400235176086 \n",
            " Test loss: 0.0393897220492363 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6162])), ('bias', tensor([0.3352]))])\n",
            "Epoch: 12190 \n",
            " Test: 0.016814017668366432 \n",
            " Test loss: 0.039310503751039505 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6164])), ('bias', tensor([0.3351]))])\n",
            "Epoch: 12200 \n",
            " Test: 0.016779668629169464 \n",
            " Test loss: 0.03922782093286514 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6165])), ('bias', tensor([0.3351]))])\n",
            "Epoch: 12210 \n",
            " Test: 0.01674526557326317 \n",
            " Test loss: 0.039145130664110184 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6167])), ('bias', tensor([0.3350]))])\n",
            "Epoch: 12220 \n",
            " Test: 0.016710937023162842 \n",
            " Test loss: 0.03906931355595589 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6169])), ('bias', tensor([0.3349]))])\n",
            "Epoch: 12230 \n",
            " Test: 0.016676554456353188 \n",
            " Test loss: 0.0389900803565979 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6170])), ('bias', tensor([0.3348]))])\n",
            "Epoch: 12240 \n",
            " Test: 0.016642192378640175 \n",
            " Test loss: 0.03890400007367134 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6172])), ('bias', tensor([0.3348]))])\n",
            "Epoch: 12250 \n",
            " Test: 0.016607865691184998 \n",
            " Test loss: 0.03882817551493645 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6174])), ('bias', tensor([0.3347]))])\n",
            "Epoch: 12260 \n",
            " Test: 0.016573509201407433 \n",
            " Test loss: 0.03874548152089119 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6176])), ('bias', tensor([0.3346]))])\n",
            "Epoch: 12270 \n",
            " Test: 0.01653910055756569 \n",
            " Test loss: 0.038662802428007126 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6177])), ('bias', tensor([0.3346]))])\n",
            "Epoch: 12280 \n",
            " Test: 0.016504770144820213 \n",
            " Test loss: 0.03858358412981033 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6179])), ('bias', tensor([0.3345]))])\n",
            "Epoch: 12290 \n",
            " Test: 0.01647040620446205 \n",
            " Test loss: 0.038507767021656036 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6181])), ('bias', tensor([0.3344]))])\n",
            "Epoch: 12300 \n",
            " Test: 0.016436023637652397 \n",
            " Test loss: 0.03842168301343918 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6182])), ('bias', tensor([0.3343]))])\n",
            "Epoch: 12310 \n",
            " Test: 0.01640171930193901 \n",
            " Test loss: 0.038345854729413986 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6184])), ('bias', tensor([0.3343]))])\n",
            "Epoch: 12320 \n",
            " Test: 0.016367346048355103 \n",
            " Test loss: 0.038263171911239624 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6186])), ('bias', tensor([0.3342]))])\n",
            "Epoch: 12330 \n",
            " Test: 0.016332942992448807 \n",
            " Test loss: 0.038183946162462234 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6188])), ('bias', tensor([0.3341]))])\n",
            "Epoch: 12340 \n",
            " Test: 0.016298603266477585 \n",
            " Test loss: 0.03810126706957817 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6189])), ('bias', tensor([0.3341]))])\n",
            "Epoch: 12350 \n",
            " Test: 0.01626426912844181 \n",
            " Test loss: 0.03802204132080078 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6191])), ('bias', tensor([0.3340]))])\n",
            "Epoch: 12360 \n",
            " Test: 0.016229871660470963 \n",
            " Test loss: 0.03794621676206589 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6193])), ('bias', tensor([0.3339]))])\n",
            "Epoch: 12370 \n",
            " Test: 0.016195565462112427 \n",
            " Test loss: 0.03786354139447212 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6194])), ('bias', tensor([0.3338]))])\n",
            "Epoch: 12380 \n",
            " Test: 0.016161184757947922 \n",
            " Test loss: 0.03778085857629776 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6196])), ('bias', tensor([0.3338]))])\n",
            "Epoch: 12390 \n",
            " Test: 0.0161268450319767 \n",
            " Test loss: 0.03770161792635918 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6198])), ('bias', tensor([0.3337]))])\n",
            "Epoch: 12400 \n",
            " Test: 0.01609249785542488 \n",
            " Test loss: 0.037622399628162384 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6200])), ('bias', tensor([0.3336]))])\n",
            "Epoch: 12410 \n",
            " Test: 0.01605810783803463 \n",
            " Test loss: 0.03753972053527832 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6201])), ('bias', tensor([0.3335]))])\n",
            "Epoch: 12420 \n",
            " Test: 0.016023719683289528 \n",
            " Test loss: 0.037463895976543427 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6203])), ('bias', tensor([0.3335]))])\n",
            "Epoch: 12430 \n",
            " Test: 0.015989413484930992 \n",
            " Test loss: 0.03738122060894966 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6205])), ('bias', tensor([0.3334]))])\n",
            "Epoch: 12440 \n",
            " Test: 0.015955032780766487 \n",
            " Test loss: 0.037301987409591675 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6206])), ('bias', tensor([0.3333]))])\n",
            "Epoch: 12450 \n",
            " Test: 0.015920687466859818 \n",
            " Test loss: 0.03721931576728821 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6208])), ('bias', tensor([0.3333]))])\n",
            "Epoch: 12460 \n",
            " Test: 0.015886342152953148 \n",
            " Test loss: 0.03714008256793022 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6210])), ('bias', tensor([0.3332]))])\n",
            "Epoch: 12470 \n",
            " Test: 0.015851940959692 \n",
            " Test loss: 0.03705739229917526 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6212])), ('bias', tensor([0.3331]))])\n",
            "Epoch: 12480 \n",
            " Test: 0.015817567706108093 \n",
            " Test loss: 0.03698158264160156 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6213])), ('bias', tensor([0.3330]))])\n",
            "Epoch: 12490 \n",
            " Test: 0.01578320562839508 \n",
            " Test loss: 0.03689549118280411 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6215])), ('bias', tensor([0.3330]))])\n",
            "Epoch: 12500 \n",
            " Test: 0.0157488826662302 \n",
            " Test loss: 0.03681967407464981 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6217])), ('bias', tensor([0.3329]))])\n",
            "Epoch: 12510 \n",
            " Test: 0.015714500099420547 \n",
            " Test loss: 0.03674043342471123 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6218])), ('bias', tensor([0.3328]))])\n",
            "Epoch: 12520 \n",
            " Test: 0.015680190175771713 \n",
            " Test loss: 0.03665775805711746 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6220])), ('bias', tensor([0.3328]))])\n",
            "Epoch: 12530 \n",
            " Test: 0.01564578153192997 \n",
            " Test loss: 0.0365750789642334 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6222])), ('bias', tensor([0.3327]))])\n",
            "Epoch: 12540 \n",
            " Test: 0.015611419454216957 \n",
            " Test loss: 0.0364992618560791 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6223])), ('bias', tensor([0.3326]))])\n",
            "Epoch: 12550 \n",
            " Test: 0.015577045269310474 \n",
            " Test loss: 0.03641316294670105 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6225])), ('bias', tensor([0.3325]))])\n",
            "Epoch: 12560 \n",
            " Test: 0.0155427111312747 \n",
            " Test loss: 0.03633393719792366 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6227])), ('bias', tensor([0.3325]))])\n",
            "Epoch: 12570 \n",
            " Test: 0.01550834160298109 \n",
            " Test loss: 0.03625813126564026 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6229])), ('bias', tensor([0.3324]))])\n",
            "Epoch: 12580 \n",
            " Test: 0.015474028885364532 \n",
            " Test loss: 0.0361754484474659 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6230])), ('bias', tensor([0.3323]))])\n",
            "Epoch: 12590 \n",
            " Test: 0.015439623966813087 \n",
            " Test loss: 0.03609275072813034 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6232])), ('bias', tensor([0.3323]))])\n",
            "Epoch: 12600 \n",
            " Test: 0.015405287966132164 \n",
            " Test loss: 0.03601353242993355 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6234])), ('bias', tensor([0.3322]))])\n",
            "Epoch: 12610 \n",
            " Test: 0.015370883047580719 \n",
            " Test loss: 0.03593430668115616 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6235])), ('bias', tensor([0.3321]))])\n",
            "Epoch: 12620 \n",
            " Test: 0.015336548909544945 \n",
            " Test loss: 0.0358516164124012 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6237])), ('bias', tensor([0.3320]))])\n",
            "Epoch: 12630 \n",
            " Test: 0.015302200801670551 \n",
            " Test loss: 0.0357757993042469 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6239])), ('bias', tensor([0.3320]))])\n",
            "Epoch: 12640 \n",
            " Test: 0.015267866663634777 \n",
            " Test loss: 0.03569310903549194 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6241])), ('bias', tensor([0.3319]))])\n",
            "Epoch: 12650 \n",
            " Test: 0.01523351389914751 \n",
            " Test loss: 0.03561389446258545 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6242])), ('bias', tensor([0.3318]))])\n",
            "Epoch: 12660 \n",
            " Test: 0.015199127607047558 \n",
            " Test loss: 0.03553120419383049 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6244])), ('bias', tensor([0.3318]))])\n",
            "Epoch: 12670 \n",
            " Test: 0.015164789743721485 \n",
            " Test loss: 0.0354519784450531 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6246])), ('bias', tensor([0.3317]))])\n",
            "Epoch: 12680 \n",
            " Test: 0.015130385756492615 \n",
            " Test loss: 0.03536929935216904 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6247])), ('bias', tensor([0.3316]))])\n",
            "Epoch: 12690 \n",
            " Test: 0.015096050687134266 \n",
            " Test loss: 0.03529347851872444 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6249])), ('bias', tensor([0.3315]))])\n",
            "Epoch: 12700 \n",
            " Test: 0.015061704441905022 \n",
            " Test loss: 0.03521079570055008 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6251])), ('bias', tensor([0.3315]))])\n",
            "Epoch: 12710 \n",
            " Test: 0.015027357265353203 \n",
            " Test loss: 0.03513156250119209 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6253])), ('bias', tensor([0.3314]))])\n",
            "Epoch: 12720 \n",
            " Test: 0.014992979355156422 \n",
            " Test loss: 0.035052340477705 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6254])), ('bias', tensor([0.3313]))])\n",
            "Epoch: 12730 \n",
            " Test: 0.014958630315959454 \n",
            " Test loss: 0.03496966511011124 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6256])), ('bias', tensor([0.3312]))])\n",
            "Epoch: 12740 \n",
            " Test: 0.014924225397408009 \n",
            " Test loss: 0.03488697484135628 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6258])), ('bias', tensor([0.3312]))])\n",
            "Epoch: 12750 \n",
            " Test: 0.014889898709952831 \n",
            " Test loss: 0.03481115773320198 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6259])), ('bias', tensor([0.3311]))])\n",
            "Epoch: 12760 \n",
            " Test: 0.014855511486530304 \n",
            " Test loss: 0.034731924533843994 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6261])), ('bias', tensor([0.3310]))])\n",
            "Epoch: 12770 \n",
            " Test: 0.01482115127146244 \n",
            " Test loss: 0.03464584797620773 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6263])), ('bias', tensor([0.3310]))])\n",
            "Epoch: 12780 \n",
            " Test: 0.014786824584007263 \n",
            " Test loss: 0.03457002714276314 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6265])), ('bias', tensor([0.3309]))])\n",
            "Epoch: 12790 \n",
            " Test: 0.014752468094229698 \n",
            " Test loss: 0.03448732942342758 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6266])), ('bias', tensor([0.3308]))])\n",
            "Epoch: 12800 \n",
            " Test: 0.01471805851906538 \n",
            " Test loss: 0.034404654055833817 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6268])), ('bias', tensor([0.3307]))])\n",
            "Epoch: 12810 \n",
            " Test: 0.014683729037642479 \n",
            " Test loss: 0.03432542830705643 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6270])), ('bias', tensor([0.3307]))])\n",
            "Epoch: 12820 \n",
            " Test: 0.014649364165961742 \n",
            " Test loss: 0.03424961119890213 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6271])), ('bias', tensor([0.3306]))])\n",
            "Epoch: 12830 \n",
            " Test: 0.014614982530474663 \n",
            " Test loss: 0.034163523465394974 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6273])), ('bias', tensor([0.3305]))])\n",
            "Epoch: 12840 \n",
            " Test: 0.014580677263438702 \n",
            " Test loss: 0.03408769518136978 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6275])), ('bias', tensor([0.3305]))])\n",
            "Epoch: 12850 \n",
            " Test: 0.014546304941177368 \n",
            " Test loss: 0.03400500863790512 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6276])), ('bias', tensor([0.3304]))])\n",
            "Epoch: 12860 \n",
            " Test: 0.014511900953948498 \n",
            " Test loss: 0.03392579033970833 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6278])), ('bias', tensor([0.3303]))])\n",
            "Epoch: 12870 \n",
            " Test: 0.014477565884590149 \n",
            " Test loss: 0.033843111246824265 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6280])), ('bias', tensor([0.3302]))])\n",
            "Epoch: 12880 \n",
            " Test: 0.014443226158618927 \n",
            " Test loss: 0.033763885498046875 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6282])), ('bias', tensor([0.3302]))])\n",
            "Epoch: 12890 \n",
            " Test: 0.014408831484615803 \n",
            " Test loss: 0.03368806093931198 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6283])), ('bias', tensor([0.3301]))])\n",
            "Epoch: 12900 \n",
            " Test: 0.014374524354934692 \n",
            " Test loss: 0.033605385571718216 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6285])), ('bias', tensor([0.3300]))])\n",
            "Epoch: 12910 \n",
            " Test: 0.014340144582092762 \n",
            " Test loss: 0.033522702753543854 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6287])), ('bias', tensor([0.3300]))])\n",
            "Epoch: 12920 \n",
            " Test: 0.01430580299347639 \n",
            " Test loss: 0.03344346955418587 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6288])), ('bias', tensor([0.3299]))])\n",
            "Epoch: 12930 \n",
            " Test: 0.014271457679569721 \n",
            " Test loss: 0.03336424380540848 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6290])), ('bias', tensor([0.3298]))])\n",
            "Epoch: 12940 \n",
            " Test: 0.014237066730856895 \n",
            " Test loss: 0.033281564712524414 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6292])), ('bias', tensor([0.3297]))])\n",
            "Epoch: 12950 \n",
            " Test: 0.014202679507434368 \n",
            " Test loss: 0.03320574015378952 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6294])), ('bias', tensor([0.3297]))])\n",
            "Epoch: 12960 \n",
            " Test: 0.014168371446430683 \n",
            " Test loss: 0.033123064786195755 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6295])), ('bias', tensor([0.3296]))])\n",
            "Epoch: 12970 \n",
            " Test: 0.014133989810943604 \n",
            " Test loss: 0.03304382413625717 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6297])), ('bias', tensor([0.3295]))])\n",
            "Epoch: 12980 \n",
            " Test: 0.014099645428359509 \n",
            " Test loss: 0.032961152493953705 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6299])), ('bias', tensor([0.3295]))])\n",
            "Epoch: 12990 \n",
            " Test: 0.014065304771065712 \n",
            " Test loss: 0.032881926745176315 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6300])), ('bias', tensor([0.3294]))])\n",
            "Epoch: 13000 \n",
            " Test: 0.014030903577804565 \n",
            " Test loss: 0.032799236476421356 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6302])), ('bias', tensor([0.3293]))])\n",
            "Epoch: 13010 \n",
            " Test: 0.013996528461575508 \n",
            " Test loss: 0.032723426818847656 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6304])), ('bias', tensor([0.3292]))])\n",
            "Epoch: 13020 \n",
            " Test: 0.013962164521217346 \n",
            " Test loss: 0.0326373353600502 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6306])), ('bias', tensor([0.3292]))])\n",
            "Epoch: 13030 \n",
            " Test: 0.013927841559052467 \n",
            " Test loss: 0.032561518251895905 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6307])), ('bias', tensor([0.3291]))])\n",
            "Epoch: 13040 \n",
            " Test: 0.013893458060920238 \n",
            " Test loss: 0.03248228505253792 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6309])), ('bias', tensor([0.3290]))])\n",
            "Epoch: 13050 \n",
            " Test: 0.013859149999916553 \n",
            " Test loss: 0.032399605959653854 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6311])), ('bias', tensor([0.3290]))])\n",
            "Epoch: 13060 \n",
            " Test: 0.013824740424752235 \n",
            " Test loss: 0.03231693059206009 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6312])), ('bias', tensor([0.3289]))])\n",
            "Epoch: 13070 \n",
            " Test: 0.013790378347039223 \n",
            " Test loss: 0.03224111348390579 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6314])), ('bias', tensor([0.3288]))])\n",
            "Epoch: 13080 \n",
            " Test: 0.01375600229948759 \n",
            " Test loss: 0.032155007123947144 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6316])), ('bias', tensor([0.3287]))])\n",
            "Epoch: 13090 \n",
            " Test: 0.013721669092774391 \n",
            " Test loss: 0.03207577392458916 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6318])), ('bias', tensor([0.3287]))])\n",
            "Epoch: 13100 \n",
            " Test: 0.013687300495803356 \n",
            " Test loss: 0.031999967992305756 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6319])), ('bias', tensor([0.3286]))])\n",
            "Epoch: 13110 \n",
            " Test: 0.013652987778186798 \n",
            " Test loss: 0.03191728517413139 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6321])), ('bias', tensor([0.3285]))])\n",
            "Epoch: 13120 \n",
            " Test: 0.013618582859635353 \n",
            " Test loss: 0.031834591180086136 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6323])), ('bias', tensor([0.3285]))])\n",
            "Epoch: 13130 \n",
            " Test: 0.013584248721599579 \n",
            " Test loss: 0.031755369156599045 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6324])), ('bias', tensor([0.3284]))])\n",
            "Epoch: 13140 \n",
            " Test: 0.013549843803048134 \n",
            " Test loss: 0.03167615085840225 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6326])), ('bias', tensor([0.3283]))])\n",
            "Epoch: 13150 \n",
            " Test: 0.01351550780236721 \n",
            " Test loss: 0.03159346058964729 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6328])), ('bias', tensor([0.3282]))])\n",
            "Epoch: 13160 \n",
            " Test: 0.013481159694492817 \n",
            " Test loss: 0.031517643481492996 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6329])), ('bias', tensor([0.3282]))])\n",
            "Epoch: 13170 \n",
            " Test: 0.013446825556457043 \n",
            " Test loss: 0.03143495321273804 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6331])), ('bias', tensor([0.3281]))])\n",
            "Epoch: 13180 \n",
            " Test: 0.013412472791969776 \n",
            " Test loss: 0.03135574609041214 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6333])), ('bias', tensor([0.3280]))])\n",
            "Epoch: 13190 \n",
            " Test: 0.013378086499869823 \n",
            " Test loss: 0.03127305582165718 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6335])), ('bias', tensor([0.3279]))])\n",
            "Epoch: 13200 \n",
            " Test: 0.013343746773898602 \n",
            " Test loss: 0.031193822622299194 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6336])), ('bias', tensor([0.3279]))])\n",
            "Epoch: 13210 \n",
            " Test: 0.01330934464931488 \n",
            " Test loss: 0.03111114539206028 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6338])), ('bias', tensor([0.3278]))])\n",
            "Epoch: 13220 \n",
            " Test: 0.013275009579956532 \n",
            " Test loss: 0.031035322695970535 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6340])), ('bias', tensor([0.3277]))])\n",
            "Epoch: 13230 \n",
            " Test: 0.013240665197372437 \n",
            " Test loss: 0.030952638015151024 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6341])), ('bias', tensor([0.3277]))])\n",
            "Epoch: 13240 \n",
            " Test: 0.013206318020820618 \n",
            " Test loss: 0.030873406678438187 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6343])), ('bias', tensor([0.3276]))])\n",
            "Epoch: 13250 \n",
            " Test: 0.013171938247978687 \n",
            " Test loss: 0.030794184654951096 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6345])), ('bias', tensor([0.3275]))])\n",
            "Epoch: 13260 \n",
            " Test: 0.013137588277459145 \n",
            " Test loss: 0.03071150742471218 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6347])), ('bias', tensor([0.3274]))])\n",
            "Epoch: 13270 \n",
            " Test: 0.013103184290230274 \n",
            " Test loss: 0.03062881901860237 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6348])), ('bias', tensor([0.3274]))])\n",
            "Epoch: 13280 \n",
            " Test: 0.013068857602775097 \n",
            " Test loss: 0.030553001910448074 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6350])), ('bias', tensor([0.3273]))])\n",
            "Epoch: 13290 \n",
            " Test: 0.01303447037935257 \n",
            " Test loss: 0.030473768711090088 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6352])), ('bias', tensor([0.3272]))])\n",
            "Epoch: 13300 \n",
            " Test: 0.013000110164284706 \n",
            " Test loss: 0.030387694016098976 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6353])), ('bias', tensor([0.3272]))])\n",
            "Epoch: 13310 \n",
            " Test: 0.012965783476829529 \n",
            " Test loss: 0.03031187132000923 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6355])), ('bias', tensor([0.3271]))])\n",
            "Epoch: 13320 \n",
            " Test: 0.012931426987051964 \n",
            " Test loss: 0.030229175463318825 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6357])), ('bias', tensor([0.3270]))])\n",
            "Epoch: 13330 \n",
            " Test: 0.012897017411887646 \n",
            " Test loss: 0.03014649823307991 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6359])), ('bias', tensor([0.3269]))])\n",
            "Epoch: 13340 \n",
            " Test: 0.012862687930464745 \n",
            " Test loss: 0.03006727062165737 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6360])), ('bias', tensor([0.3269]))])\n",
            "Epoch: 13350 \n",
            " Test: 0.012828323058784008 \n",
            " Test loss: 0.029991453513503075 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6362])), ('bias', tensor([0.3268]))])\n",
            "Epoch: 13360 \n",
            " Test: 0.012793943285942078 \n",
            " Test loss: 0.029905367642641068 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6364])), ('bias', tensor([0.3267]))])\n",
            "Epoch: 13370 \n",
            " Test: 0.012759635224938393 \n",
            " Test loss: 0.029829537495970726 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6365])), ('bias', tensor([0.3267]))])\n",
            "Epoch: 13380 \n",
            " Test: 0.012725263833999634 \n",
            " Test loss: 0.029746854677796364 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6367])), ('bias', tensor([0.3266]))])\n",
            "Epoch: 13390 \n",
            " Test: 0.012690859846770763 \n",
            " Test loss: 0.029667634516954422 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6369])), ('bias', tensor([0.3265]))])\n",
            "Epoch: 13400 \n",
            " Test: 0.012656524777412415 \n",
            " Test loss: 0.02958495542407036 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6370])), ('bias', tensor([0.3264]))])\n",
            "Epoch: 13410 \n",
            " Test: 0.012622185051441193 \n",
            " Test loss: 0.02950572967529297 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6372])), ('bias', tensor([0.3264]))])\n",
            "Epoch: 13420 \n",
            " Test: 0.012587790377438068 \n",
            " Test loss: 0.029429906979203224 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6374])), ('bias', tensor([0.3263]))])\n",
            "Epoch: 13430 \n",
            " Test: 0.012553483247756958 \n",
            " Test loss: 0.02934722974896431 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6376])), ('bias', tensor([0.3262]))])\n",
            "Epoch: 13440 \n",
            " Test: 0.012519103474915028 \n",
            " Test loss: 0.0292645450681448 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6377])), ('bias', tensor([0.3262]))])\n",
            "Epoch: 13450 \n",
            " Test: 0.012484761886298656 \n",
            " Test loss: 0.02918531373143196 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6379])), ('bias', tensor([0.3261]))])\n",
            "Epoch: 13460 \n",
            " Test: 0.012450416572391987 \n",
            " Test loss: 0.029106086120009422 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6381])), ('bias', tensor([0.3260]))])\n",
            "Epoch: 13470 \n",
            " Test: 0.012416025623679161 \n",
            " Test loss: 0.029023408889770508 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6382])), ('bias', tensor([0.3259]))])\n",
            "Epoch: 13480 \n",
            " Test: 0.012381638400256634 \n",
            " Test loss: 0.028947586193680763 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6384])), ('bias', tensor([0.3259]))])\n",
            "Epoch: 13490 \n",
            " Test: 0.012347330339252949 \n",
            " Test loss: 0.02886490896344185 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6386])), ('bias', tensor([0.3258]))])\n",
            "Epoch: 13500 \n",
            " Test: 0.012312949635088444 \n",
            " Test loss: 0.028785670176148415 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6388])), ('bias', tensor([0.3257]))])\n",
            "Epoch: 13510 \n",
            " Test: 0.0122786033898592 \n",
            " Test loss: 0.028702998533844948 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6389])), ('bias', tensor([0.3256]))])\n",
            "Epoch: 13520 \n",
            " Test: 0.012244263663887978 \n",
            " Test loss: 0.02862377092242241 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6391])), ('bias', tensor([0.3256]))])\n",
            "Epoch: 13530 \n",
            " Test: 0.012209863401949406 \n",
            " Test loss: 0.0285410825163126 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6393])), ('bias', tensor([0.3255]))])\n",
            "Epoch: 13540 \n",
            " Test: 0.012175487354397774 \n",
            " Test loss: 0.02846527099609375 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6394])), ('bias', tensor([0.3254]))])\n",
            "Epoch: 13550 \n",
            " Test: 0.012141122482717037 \n",
            " Test loss: 0.028379177674651146 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6396])), ('bias', tensor([0.3254]))])\n",
            "Epoch: 13560 \n",
            " Test: 0.012106799520552158 \n",
            " Test loss: 0.02830336056649685 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6398])), ('bias', tensor([0.3253]))])\n",
            "Epoch: 13570 \n",
            " Test: 0.012072416953742504 \n",
            " Test loss: 0.028224129229784012 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6400])), ('bias', tensor([0.3252]))])\n",
            "Epoch: 13580 \n",
            " Test: 0.012038109824061394 \n",
            " Test loss: 0.028141450136899948 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6401])), ('bias', tensor([0.3251]))])\n",
            "Epoch: 13590 \n",
            " Test: 0.012003699317574501 \n",
            " Test loss: 0.028058772906661034 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6403])), ('bias', tensor([0.3251]))])\n",
            "Epoch: 13600 \n",
            " Test: 0.011969337239861488 \n",
            " Test loss: 0.027982955798506737 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6405])), ('bias', tensor([0.3250]))])\n",
            "Epoch: 13610 \n",
            " Test: 0.011934961192309856 \n",
            " Test loss: 0.027896851301193237 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6406])), ('bias', tensor([0.3249]))])\n",
            "Epoch: 13620 \n",
            " Test: 0.011900627985596657 \n",
            " Test loss: 0.02781761810183525 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6408])), ('bias', tensor([0.3249]))])\n",
            "Epoch: 13630 \n",
            " Test: 0.011866260319948196 \n",
            " Test loss: 0.027741814032197 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6410])), ('bias', tensor([0.3248]))])\n",
            "Epoch: 13640 \n",
            " Test: 0.011831946671009064 \n",
            " Test loss: 0.027659129351377487 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6412])), ('bias', tensor([0.3247]))])\n",
            "Epoch: 13650 \n",
            " Test: 0.011797541752457619 \n",
            " Test loss: 0.02757643535733223 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6413])), ('bias', tensor([0.3246]))])\n",
            "Epoch: 13660 \n",
            " Test: 0.011763207614421844 \n",
            " Test loss: 0.02749721333384514 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6415])), ('bias', tensor([0.3246]))])\n",
            "Epoch: 13670 \n",
            " Test: 0.011728804558515549 \n",
            " Test loss: 0.027417993173003197 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6417])), ('bias', tensor([0.3245]))])\n",
            "Epoch: 13680 \n",
            " Test: 0.011694466695189476 \n",
            " Test loss: 0.027335304766893387 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6418])), ('bias', tensor([0.3244]))])\n",
            "Epoch: 13690 \n",
            " Test: 0.011660118587315083 \n",
            " Test loss: 0.02725948765873909 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6420])), ('bias', tensor([0.3244]))])\n",
            "Epoch: 13700 \n",
            " Test: 0.011625783517956734 \n",
            " Test loss: 0.02717679738998413 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6422])), ('bias', tensor([0.3243]))])\n",
            "Epoch: 13710 \n",
            " Test: 0.011591430753469467 \n",
            " Test loss: 0.027097588405013084 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6423])), ('bias', tensor([0.3242]))])\n",
            "Epoch: 13720 \n",
            " Test: 0.011557047255337238 \n",
            " Test loss: 0.027014899998903275 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6425])), ('bias', tensor([0.3241]))])\n",
            "Epoch: 13730 \n",
            " Test: 0.011522707529366016 \n",
            " Test loss: 0.026935666799545288 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6427])), ('bias', tensor([0.3241]))])\n",
            "Epoch: 13740 \n",
            " Test: 0.011488303542137146 \n",
            " Test loss: 0.026852989569306374 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6429])), ('bias', tensor([0.3240]))])\n",
            "Epoch: 13750 \n",
            " Test: 0.011453966610133648 \n",
            " Test loss: 0.02677716687321663 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6430])), ('bias', tensor([0.3239]))])\n",
            "Epoch: 13760 \n",
            " Test: 0.011419623158872128 \n",
            " Test loss: 0.026694482192397118 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6432])), ('bias', tensor([0.3239]))])\n",
            "Epoch: 13770 \n",
            " Test: 0.011385277844965458 \n",
            " Test loss: 0.02661525085568428 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6434])), ('bias', tensor([0.3238]))])\n",
            "Epoch: 13780 \n",
            " Test: 0.011350896209478378 \n",
            " Test loss: 0.02653602883219719 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6435])), ('bias', tensor([0.3237]))])\n",
            "Epoch: 13790 \n",
            " Test: 0.01131654903292656 \n",
            " Test loss: 0.026453351601958275 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6437])), ('bias', tensor([0.3236]))])\n",
            "Epoch: 13800 \n",
            " Test: 0.01128214318305254 \n",
            " Test loss: 0.026370663195848465 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6439])), ('bias', tensor([0.3236]))])\n",
            "Epoch: 13810 \n",
            " Test: 0.011247815564274788 \n",
            " Test loss: 0.026294846087694168 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6441])), ('bias', tensor([0.3235]))])\n",
            "Epoch: 13820 \n",
            " Test: 0.011213429272174835 \n",
            " Test loss: 0.02621561288833618 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6442])), ('bias', tensor([0.3234]))])\n",
            "Epoch: 13830 \n",
            " Test: 0.011179068125784397 \n",
            " Test loss: 0.02612953819334507 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6444])), ('bias', tensor([0.3234]))])\n",
            "Epoch: 13840 \n",
            " Test: 0.011144742369651794 \n",
            " Test loss: 0.026053715497255325 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6446])), ('bias', tensor([0.3233]))])\n",
            "Epoch: 13850 \n",
            " Test: 0.01111038587987423 \n",
            " Test loss: 0.02597101964056492 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6447])), ('bias', tensor([0.3232]))])\n",
            "Epoch: 13860 \n",
            " Test: 0.011075976304709911 \n",
            " Test loss: 0.025888342410326004 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6449])), ('bias', tensor([0.3231]))])\n",
            "Epoch: 13870 \n",
            " Test: 0.011041645891964436 \n",
            " Test loss: 0.025809114798903465 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6451])), ('bias', tensor([0.3231]))])\n",
            "Epoch: 13880 \n",
            " Test: 0.011007281951606274 \n",
            " Test loss: 0.02573329769074917 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6453])), ('bias', tensor([0.3230]))])\n",
            "Epoch: 13890 \n",
            " Test: 0.010972903110086918 \n",
            " Test loss: 0.02564721181988716 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6454])), ('bias', tensor([0.3229]))])\n",
            "Epoch: 13900 \n",
            " Test: 0.010938594117760658 \n",
            " Test loss: 0.02557138167321682 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6456])), ('bias', tensor([0.3228]))])\n",
            "Epoch: 13910 \n",
            " Test: 0.010904221795499325 \n",
            " Test loss: 0.025488698855042458 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6458])), ('bias', tensor([0.3228]))])\n",
            "Epoch: 13920 \n",
            " Test: 0.010869819670915604 \n",
            " Test loss: 0.025409478694200516 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6459])), ('bias', tensor([0.3227]))])\n",
            "Epoch: 13930 \n",
            " Test: 0.010835482738912106 \n",
            " Test loss: 0.025326799601316452 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6461])), ('bias', tensor([0.3226]))])\n",
            "Epoch: 13940 \n",
            " Test: 0.010801143944263458 \n",
            " Test loss: 0.025247573852539062 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6463])), ('bias', tensor([0.3226]))])\n",
            "Epoch: 13950 \n",
            " Test: 0.010766749270260334 \n",
            " Test loss: 0.025171751156449318 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6465])), ('bias', tensor([0.3225]))])\n",
            "Epoch: 13960 \n",
            " Test: 0.010732441209256649 \n",
            " Test loss: 0.025089073926210403 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6466])), ('bias', tensor([0.3224]))])\n",
            "Epoch: 13970 \n",
            " Test: 0.010698061436414719 \n",
            " Test loss: 0.025006389245390892 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6468])), ('bias', tensor([0.3223]))])\n",
            "Epoch: 13980 \n",
            " Test: 0.010663720779120922 \n",
            " Test loss: 0.024927157908678055 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6470])), ('bias', tensor([0.3223]))])\n",
            "Epoch: 13990 \n",
            " Test: 0.010629375465214252 \n",
            " Test loss: 0.024847930297255516 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6471])), ('bias', tensor([0.3222]))])\n",
            "Epoch: 14000 \n",
            " Test: 0.010594984516501427 \n",
            " Test loss: 0.024765247479081154 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6473])), ('bias', tensor([0.3221]))])\n",
            "Epoch: 14010 \n",
            " Test: 0.010560599155724049 \n",
            " Test loss: 0.024689430370926857 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6475])), ('bias', tensor([0.3221]))])\n",
            "Epoch: 14020 \n",
            " Test: 0.010526290163397789 \n",
            " Test loss: 0.024606745690107346 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6476])), ('bias', tensor([0.3220]))])\n",
            "Epoch: 14030 \n",
            " Test: 0.01049190852791071 \n",
            " Test loss: 0.02452751435339451 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6478])), ('bias', tensor([0.3219]))])\n",
            "Epoch: 14040 \n",
            " Test: 0.010457564145326614 \n",
            " Test loss: 0.02444484271109104 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6480])), ('bias', tensor([0.3218]))])\n",
            "Epoch: 14050 \n",
            " Test: 0.010423222556710243 \n",
            " Test loss: 0.024365615099668503 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6482])), ('bias', tensor([0.3218]))])\n",
            "Epoch: 14060 \n",
            " Test: 0.010388822294771671 \n",
            " Test loss: 0.024282926693558693 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6483])), ('bias', tensor([0.3217]))])\n",
            "Epoch: 14070 \n",
            " Test: 0.01035444624722004 \n",
            " Test loss: 0.024207115173339844 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6485])), ('bias', tensor([0.3216]))])\n",
            "Epoch: 14080 \n",
            " Test: 0.010320081375539303 \n",
            " Test loss: 0.02412102185189724 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6487])), ('bias', tensor([0.3216]))])\n",
            "Epoch: 14090 \n",
            " Test: 0.010285758413374424 \n",
            " Test loss: 0.024045204743742943 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6488])), ('bias', tensor([0.3215]))])\n",
            "Epoch: 14100 \n",
            " Test: 0.01025137584656477 \n",
            " Test loss: 0.023965973407030106 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6490])), ('bias', tensor([0.3214]))])\n",
            "Epoch: 14110 \n",
            " Test: 0.01021706685423851 \n",
            " Test loss: 0.023883294314146042 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6492])), ('bias', tensor([0.3213]))])\n",
            "Epoch: 14120 \n",
            " Test: 0.010182658210396767 \n",
            " Test loss: 0.023800617083907127 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6494])), ('bias', tensor([0.3213]))])\n",
            "Epoch: 14130 \n",
            " Test: 0.010148297064006329 \n",
            " Test loss: 0.02372479997575283 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6495])), ('bias', tensor([0.3212]))])\n",
            "Epoch: 14140 \n",
            " Test: 0.010113921947777271 \n",
            " Test loss: 0.02363869547843933 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6497])), ('bias', tensor([0.3211]))])\n",
            "Epoch: 14150 \n",
            " Test: 0.010079586878418922 \n",
            " Test loss: 0.023559462279081345 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6499])), ('bias', tensor([0.3211]))])\n",
            "Epoch: 14160 \n",
            " Test: 0.010045217350125313 \n",
            " Test loss: 0.023483658209443092 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6500])), ('bias', tensor([0.3210]))])\n",
            "Epoch: 14170 \n",
            " Test: 0.01001090556383133 \n",
            " Test loss: 0.02340097352862358 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6502])), ('bias', tensor([0.3209]))])\n",
            "Epoch: 14180 \n",
            " Test: 0.009976500645279884 \n",
            " Test loss: 0.023318279534578323 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6504])), ('bias', tensor([0.3208]))])\n",
            "Epoch: 14190 \n",
            " Test: 0.00994216650724411 \n",
            " Test loss: 0.023239057511091232 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6506])), ('bias', tensor([0.3208]))])\n",
            "Epoch: 14200 \n",
            " Test: 0.009907763451337814 \n",
            " Test loss: 0.02315983735024929 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6507])), ('bias', tensor([0.3207]))])\n",
            "Epoch: 14210 \n",
            " Test: 0.009873425588011742 \n",
            " Test loss: 0.02307714894413948 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6509])), ('bias', tensor([0.3206]))])\n",
            "Epoch: 14220 \n",
            " Test: 0.009839077480137348 \n",
            " Test loss: 0.023001331835985184 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6511])), ('bias', tensor([0.3206]))])\n",
            "Epoch: 14230 \n",
            " Test: 0.009804742410779 \n",
            " Test loss: 0.022918641567230225 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6512])), ('bias', tensor([0.3205]))])\n",
            "Epoch: 14240 \n",
            " Test: 0.009770389646291733 \n",
            " Test loss: 0.022839432582259178 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6514])), ('bias', tensor([0.3204]))])\n",
            "Epoch: 14250 \n",
            " Test: 0.009736006148159504 \n",
            " Test loss: 0.02275674417614937 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6516])), ('bias', tensor([0.3203]))])\n",
            "Epoch: 14260 \n",
            " Test: 0.009701666422188282 \n",
            " Test loss: 0.022677510976791382 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6518])), ('bias', tensor([0.3203]))])\n",
            "Epoch: 14270 \n",
            " Test: 0.009667262434959412 \n",
            " Test loss: 0.022594833746552467 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6519])), ('bias', tensor([0.3202]))])\n",
            "Epoch: 14280 \n",
            " Test: 0.009632925502955914 \n",
            " Test loss: 0.022519011050462723 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6521])), ('bias', tensor([0.3201]))])\n",
            "Epoch: 14290 \n",
            " Test: 0.009598582051694393 \n",
            " Test loss: 0.02243632636964321 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6523])), ('bias', tensor([0.3200]))])\n",
            "Epoch: 14300 \n",
            " Test: 0.009564236737787724 \n",
            " Test loss: 0.022357095032930374 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6524])), ('bias', tensor([0.3200]))])\n",
            "Epoch: 14310 \n",
            " Test: 0.009529855102300644 \n",
            " Test loss: 0.022277873009443283 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6526])), ('bias', tensor([0.3199]))])\n",
            "Epoch: 14320 \n",
            " Test: 0.009495507925748825 \n",
            " Test loss: 0.02219519577920437 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6528])), ('bias', tensor([0.3198]))])\n",
            "Epoch: 14330 \n",
            " Test: 0.009461102075874805 \n",
            " Test loss: 0.02211250737309456 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6529])), ('bias', tensor([0.3198]))])\n",
            "Epoch: 14340 \n",
            " Test: 0.009426774457097054 \n",
            " Test loss: 0.022036690264940262 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6531])), ('bias', tensor([0.3197]))])\n",
            "Epoch: 14350 \n",
            " Test: 0.0093923881649971 \n",
            " Test loss: 0.021957457065582275 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6533])), ('bias', tensor([0.3196]))])\n",
            "Epoch: 14360 \n",
            " Test: 0.009358027018606663 \n",
            " Test loss: 0.021871382370591164 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6535])), ('bias', tensor([0.3195]))])\n",
            "Epoch: 14370 \n",
            " Test: 0.00932370126247406 \n",
            " Test loss: 0.02179555967450142 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6536])), ('bias', tensor([0.3195]))])\n",
            "Epoch: 14380 \n",
            " Test: 0.009289344772696495 \n",
            " Test loss: 0.021712863817811012 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6538])), ('bias', tensor([0.3194]))])\n",
            "Epoch: 14390 \n",
            " Test: 0.009254935197532177 \n",
            " Test loss: 0.021630186587572098 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6540])), ('bias', tensor([0.3193]))])\n",
            "Epoch: 14400 \n",
            " Test: 0.009220604784786701 \n",
            " Test loss: 0.02155095897614956 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6541])), ('bias', tensor([0.3193]))])\n",
            "Epoch: 14410 \n",
            " Test: 0.00918624084442854 \n",
            " Test loss: 0.021475141867995262 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6543])), ('bias', tensor([0.3192]))])\n",
            "Epoch: 14420 \n",
            " Test: 0.009151862002909184 \n",
            " Test loss: 0.021389055997133255 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6545])), ('bias', tensor([0.3191]))])\n",
            "Epoch: 14430 \n",
            " Test: 0.009117553010582924 \n",
            " Test loss: 0.021313225850462914 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6547])), ('bias', tensor([0.3190]))])\n",
            "Epoch: 14440 \n",
            " Test: 0.00908318068832159 \n",
            " Test loss: 0.02123054303228855 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6548])), ('bias', tensor([0.3190]))])\n",
            "Epoch: 14450 \n",
            " Test: 0.00904877856373787 \n",
            " Test loss: 0.02115132287144661 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6550])), ('bias', tensor([0.3189]))])\n",
            "Epoch: 14460 \n",
            " Test: 0.009014441631734371 \n",
            " Test loss: 0.021068643778562546 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6552])), ('bias', tensor([0.3188]))])\n",
            "Epoch: 14470 \n",
            " Test: 0.008980102837085724 \n",
            " Test loss: 0.020989418029785156 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6553])), ('bias', tensor([0.3188]))])\n",
            "Epoch: 14480 \n",
            " Test: 0.0089457081630826 \n",
            " Test loss: 0.02091359533369541 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6555])), ('bias', tensor([0.3187]))])\n",
            "Epoch: 14490 \n",
            " Test: 0.008911400102078915 \n",
            " Test loss: 0.020830918103456497 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6557])), ('bias', tensor([0.3186]))])\n",
            "Epoch: 14500 \n",
            " Test: 0.008877020329236984 \n",
            " Test loss: 0.020748233422636986 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6559])), ('bias', tensor([0.3185]))])\n",
            "Epoch: 14510 \n",
            " Test: 0.008842679671943188 \n",
            " Test loss: 0.02066900208592415 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6560])), ('bias', tensor([0.3185]))])\n",
            "Epoch: 14520 \n",
            " Test: 0.008808334358036518 \n",
            " Test loss: 0.02058977447450161 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6562])), ('bias', tensor([0.3184]))])\n",
            "Epoch: 14530 \n",
            " Test: 0.008773943409323692 \n",
            " Test loss: 0.020507091656327248 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6564])), ('bias', tensor([0.3183]))])\n",
            "Epoch: 14540 \n",
            " Test: 0.008739558048546314 \n",
            " Test loss: 0.02043127454817295 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6565])), ('bias', tensor([0.3183]))])\n",
            "Epoch: 14550 \n",
            " Test: 0.008705249056220055 \n",
            " Test loss: 0.02034858986735344 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6567])), ('bias', tensor([0.3182]))])\n",
            "Epoch: 14560 \n",
            " Test: 0.008670867420732975 \n",
            " Test loss: 0.020269358530640602 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6569])), ('bias', tensor([0.3181]))])\n",
            "Epoch: 14570 \n",
            " Test: 0.00863652303814888 \n",
            " Test loss: 0.020186686888337135 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6570])), ('bias', tensor([0.3180]))])\n",
            "Epoch: 14580 \n",
            " Test: 0.008602181449532509 \n",
            " Test loss: 0.020107459276914597 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6572])), ('bias', tensor([0.3180]))])\n",
            "Epoch: 14590 \n",
            " Test: 0.008567781187593937 \n",
            " Test loss: 0.020024770870804787 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6574])), ('bias', tensor([0.3179]))])\n",
            "Epoch: 14600 \n",
            " Test: 0.008533405140042305 \n",
            " Test loss: 0.019948959350585938 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6576])), ('bias', tensor([0.3178]))])\n",
            "Epoch: 14610 \n",
            " Test: 0.008499040268361568 \n",
            " Test loss: 0.019862866029143333 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6577])), ('bias', tensor([0.3178]))])\n",
            "Epoch: 14620 \n",
            " Test: 0.00846471730619669 \n",
            " Test loss: 0.019787048920989037 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6579])), ('bias', tensor([0.3177]))])\n",
            "Epoch: 14630 \n",
            " Test: 0.008430334739387035 \n",
            " Test loss: 0.0197078175842762 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6581])), ('bias', tensor([0.3176]))])\n",
            "Epoch: 14640 \n",
            " Test: 0.008396025747060776 \n",
            " Test loss: 0.019625138491392136 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6582])), ('bias', tensor([0.3175]))])\n",
            "Epoch: 14650 \n",
            " Test: 0.008361617103219032 \n",
            " Test loss: 0.01954246126115322 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6584])), ('bias', tensor([0.3175]))])\n",
            "Epoch: 14660 \n",
            " Test: 0.008327255956828594 \n",
            " Test loss: 0.019466644152998924 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6586])), ('bias', tensor([0.3174]))])\n",
            "Epoch: 14670 \n",
            " Test: 0.008292880840599537 \n",
            " Test loss: 0.019380539655685425 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6588])), ('bias', tensor([0.3173]))])\n",
            "Epoch: 14680 \n",
            " Test: 0.008258545771241188 \n",
            " Test loss: 0.01930130645632744 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6589])), ('bias', tensor([0.3173]))])\n",
            "Epoch: 14690 \n",
            " Test: 0.008224176242947578 \n",
            " Test loss: 0.019225502386689186 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6591])), ('bias', tensor([0.3172]))])\n",
            "Epoch: 14700 \n",
            " Test: 0.008189864456653595 \n",
            " Test loss: 0.019142817705869675 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6593])), ('bias', tensor([0.3171]))])\n",
            "Epoch: 14710 \n",
            " Test: 0.00815545953810215 \n",
            " Test loss: 0.019060123711824417 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6594])), ('bias', tensor([0.3170]))])\n",
            "Epoch: 14720 \n",
            " Test: 0.008121125400066376 \n",
            " Test loss: 0.018980901688337326 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6596])), ('bias', tensor([0.3170]))])\n",
            "Epoch: 14730 \n",
            " Test: 0.00808672234416008 \n",
            " Test loss: 0.018901681527495384 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6598])), ('bias', tensor([0.3169]))])\n",
            "Epoch: 14740 \n",
            " Test: 0.008052384480834007 \n",
            " Test loss: 0.018818993121385574 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6600])), ('bias', tensor([0.3168]))])\n",
            "Epoch: 14750 \n",
            " Test: 0.008018036372959614 \n",
            " Test loss: 0.018743176013231277 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6601])), ('bias', tensor([0.3167]))])\n",
            "Epoch: 14760 \n",
            " Test: 0.007983701303601265 \n",
            " Test loss: 0.01866048574447632 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6603])), ('bias', tensor([0.3167]))])\n",
            "Epoch: 14770 \n",
            " Test: 0.007949348539113998 \n",
            " Test loss: 0.018581276759505272 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6605])), ('bias', tensor([0.3166]))])\n",
            "Epoch: 14780 \n",
            " Test: 0.00791496504098177 \n",
            " Test loss: 0.018498588353395462 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6606])), ('bias', tensor([0.3165]))])\n",
            "Epoch: 14790 \n",
            " Test: 0.007880625315010548 \n",
            " Test loss: 0.018419355154037476 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6608])), ('bias', tensor([0.3165]))])\n",
            "Epoch: 14800 \n",
            " Test: 0.007846221327781677 \n",
            " Test loss: 0.01833667792379856 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6610])), ('bias', tensor([0.3164]))])\n",
            "Epoch: 14810 \n",
            " Test: 0.007811884395778179 \n",
            " Test loss: 0.018260855227708817 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6612])), ('bias', tensor([0.3163]))])\n",
            "Epoch: 14820 \n",
            " Test: 0.007777539547532797 \n",
            " Test loss: 0.018178170546889305 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6613])), ('bias', tensor([0.3162]))])\n",
            "Epoch: 14830 \n",
            " Test: 0.007743195630609989 \n",
            " Test loss: 0.018098939210176468 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6615])), ('bias', tensor([0.3162]))])\n",
            "Epoch: 14840 \n",
            " Test: 0.0077088139951229095 \n",
            " Test loss: 0.018019717186689377 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6617])), ('bias', tensor([0.3161]))])\n",
            "Epoch: 14850 \n",
            " Test: 0.007674466818571091 \n",
            " Test loss: 0.017937039956450462 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6618])), ('bias', tensor([0.3160]))])\n",
            "Epoch: 14860 \n",
            " Test: 0.007640059106051922 \n",
            " Test loss: 0.017854351550340652 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6620])), ('bias', tensor([0.3160]))])\n",
            "Epoch: 14870 \n",
            " Test: 0.0076057338155806065 \n",
            " Test loss: 0.017778534442186356 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6622])), ('bias', tensor([0.3159]))])\n",
            "Epoch: 14880 \n",
            " Test: 0.0075713470578193665 \n",
            " Test loss: 0.01769930124282837 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6623])), ('bias', tensor([0.3158]))])\n",
            "Epoch: 14890 \n",
            " Test: 0.007536985911428928 \n",
            " Test loss: 0.017613226547837257 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6625])), ('bias', tensor([0.3157]))])\n",
            "Epoch: 14900 \n",
            " Test: 0.007502660155296326 \n",
            " Test loss: 0.017537403851747513 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6627])), ('bias', tensor([0.3157]))])\n",
            "Epoch: 14910 \n",
            " Test: 0.007468304131180048 \n",
            " Test loss: 0.017454707995057106 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6629])), ('bias', tensor([0.3156]))])\n",
            "Epoch: 14920 \n",
            " Test: 0.007433895952999592 \n",
            " Test loss: 0.01737203076481819 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6630])), ('bias', tensor([0.3155]))])\n",
            "Epoch: 14930 \n",
            " Test: 0.007399563677608967 \n",
            " Test loss: 0.017292803153395653 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6632])), ('bias', tensor([0.3155]))])\n",
            "Epoch: 14940 \n",
            " Test: 0.007365201599895954 \n",
            " Test loss: 0.017216986045241356 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6634])), ('bias', tensor([0.3154]))])\n",
            "Epoch: 14950 \n",
            " Test: 0.007330820895731449 \n",
            " Test loss: 0.01713090017437935 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6635])), ('bias', tensor([0.3153]))])\n",
            "Epoch: 14960 \n",
            " Test: 0.007296513766050339 \n",
            " Test loss: 0.017055070027709007 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6637])), ('bias', tensor([0.3152]))])\n",
            "Epoch: 14970 \n",
            " Test: 0.007262139581143856 \n",
            " Test loss: 0.016972387209534645 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6639])), ('bias', tensor([0.3152]))])\n",
            "Epoch: 14980 \n",
            " Test: 0.007227737456560135 \n",
            " Test loss: 0.016893167048692703 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6641])), ('bias', tensor([0.3151]))])\n",
            "Epoch: 14990 \n",
            " Test: 0.007193400524556637 \n",
            " Test loss: 0.01681048795580864 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6642])), ('bias', tensor([0.3150]))])\n",
            "Epoch: 15000 \n",
            " Test: 0.0071590617299079895 \n",
            " Test loss: 0.01673126220703125 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6644])), ('bias', tensor([0.3150]))])\n",
            "Epoch: 15010 \n",
            " Test: 0.007124667055904865 \n",
            " Test loss: 0.016655439510941505 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6646])), ('bias', tensor([0.3149]))])\n",
            "Epoch: 15020 \n",
            " Test: 0.00709035899490118 \n",
            " Test loss: 0.01657276228070259 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6647])), ('bias', tensor([0.3148]))])\n",
            "Epoch: 15030 \n",
            " Test: 0.00705597922205925 \n",
            " Test loss: 0.01649007759988308 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6649])), ('bias', tensor([0.3147]))])\n",
            "Epoch: 15040 \n",
            " Test: 0.007021640427410603 \n",
            " Test loss: 0.016410846263170242 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6651])), ('bias', tensor([0.3147]))])\n",
            "Epoch: 15050 \n",
            " Test: 0.006987293250858784 \n",
            " Test loss: 0.016331618651747704 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6653])), ('bias', tensor([0.3146]))])\n",
            "Epoch: 15060 \n",
            " Test: 0.006952904164791107 \n",
            " Test loss: 0.01624893583357334 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6654])), ('bias', tensor([0.3145]))])\n",
            "Epoch: 15070 \n",
            " Test: 0.00691851694136858 \n",
            " Test loss: 0.016173118725419044 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6656])), ('bias', tensor([0.3144]))])\n",
            "Epoch: 15080 \n",
            " Test: 0.0068842084147036076 \n",
            " Test loss: 0.016090434044599533 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6658])), ('bias', tensor([0.3144]))])\n",
            "Epoch: 15090 \n",
            " Test: 0.006849826313555241 \n",
            " Test loss: 0.016011202707886696 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6659])), ('bias', tensor([0.3143]))])\n",
            "Epoch: 15100 \n",
            " Test: 0.006815481930971146 \n",
            " Test loss: 0.01592853106558323 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6661])), ('bias', tensor([0.3142]))])\n",
            "Epoch: 15110 \n",
            " Test: 0.006781139876693487 \n",
            " Test loss: 0.01584930345416069 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6663])), ('bias', tensor([0.3142]))])\n",
            "Epoch: 15120 \n",
            " Test: 0.0067467400804162025 \n",
            " Test loss: 0.01576661504805088 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6665])), ('bias', tensor([0.3141]))])\n",
            "Epoch: 15130 \n",
            " Test: 0.006712363567203283 \n",
            " Test loss: 0.01569080352783203 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6666])), ('bias', tensor([0.3140]))])\n",
            "Epoch: 15140 \n",
            " Test: 0.006677999161183834 \n",
            " Test loss: 0.015604710206389427 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6668])), ('bias', tensor([0.3139]))])\n",
            "Epoch: 15150 \n",
            " Test: 0.006643676199018955 \n",
            " Test loss: 0.01552889309823513 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6670])), ('bias', tensor([0.3139]))])\n",
            "Epoch: 15160 \n",
            " Test: 0.006609293632209301 \n",
            " Test loss: 0.015449660830199718 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6671])), ('bias', tensor([0.3138]))])\n",
            "Epoch: 15170 \n",
            " Test: 0.006574986036866903 \n",
            " Test loss: 0.015366983599960804 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6673])), ('bias', tensor([0.3137]))])\n",
            "Epoch: 15180 \n",
            " Test: 0.006540576461702585 \n",
            " Test loss: 0.015284305438399315 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6675])), ('bias', tensor([0.3137]))])\n",
            "Epoch: 15190 \n",
            " Test: 0.00650621484965086 \n",
            " Test loss: 0.01520848274230957 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6676])), ('bias', tensor([0.3136]))])\n",
            "Epoch: 15200 \n",
            " Test: 0.0064718397334218025 \n",
            " Test loss: 0.015122383832931519 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6678])), ('bias', tensor([0.3135]))])\n",
            "Epoch: 15210 \n",
            " Test: 0.006437505129724741 \n",
            " Test loss: 0.015043151564896107 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6680])), ('bias', tensor([0.3134]))])\n",
            "Epoch: 15220 \n",
            " Test: 0.006403135601431131 \n",
            " Test loss: 0.01496734656393528 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6682])), ('bias', tensor([0.3134]))])\n",
            "Epoch: 15230 \n",
            " Test: 0.006368823349475861 \n",
            " Test loss: 0.014884662814438343 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6683])), ('bias', tensor([0.3133]))])\n",
            "Epoch: 15240 \n",
            " Test: 0.006334418896585703 \n",
            " Test loss: 0.014801966957747936 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6685])), ('bias', tensor([0.3132]))])\n",
            "Epoch: 15250 \n",
            " Test: 0.006300084292888641 \n",
            " Test loss: 0.014722746796905994 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6687])), ('bias', tensor([0.3132]))])\n",
            "Epoch: 15260 \n",
            " Test: 0.006265681236982346 \n",
            " Test loss: 0.014643525704741478 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6688])), ('bias', tensor([0.3131]))])\n",
            "Epoch: 15270 \n",
            " Test: 0.006231342907994986 \n",
            " Test loss: 0.014560836367309093 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6690])), ('bias', tensor([0.3130]))])\n",
            "Epoch: 15280 \n",
            " Test: 0.006196995265781879 \n",
            " Test loss: 0.014485019259154797 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6692])), ('bias', tensor([0.3129]))])\n",
            "Epoch: 15290 \n",
            " Test: 0.006162659730762243 \n",
            " Test loss: 0.014402329921722412 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6694])), ('bias', tensor([0.3129]))])\n",
            "Epoch: 15300 \n",
            " Test: 0.006128307431936264 \n",
            " Test loss: 0.014323120936751366 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6695])), ('bias', tensor([0.3128]))])\n",
            "Epoch: 15310 \n",
            " Test: 0.006093923933804035 \n",
            " Test loss: 0.014240431599318981 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6697])), ('bias', tensor([0.3127]))])\n",
            "Epoch: 15320 \n",
            " Test: 0.006059584207832813 \n",
            " Test loss: 0.01416119933128357 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6699])), ('bias', tensor([0.3127]))])\n",
            "Epoch: 15330 \n",
            " Test: 0.006025180220603943 \n",
            " Test loss: 0.014078522101044655 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6700])), ('bias', tensor([0.3126]))])\n",
            "Epoch: 15340 \n",
            " Test: 0.005990843288600445 \n",
            " Test loss: 0.014002698473632336 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6702])), ('bias', tensor([0.3125]))])\n",
            "Epoch: 15350 \n",
            " Test: 0.0059564984403550625 \n",
            " Test loss: 0.013920014724135399 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6704])), ('bias', tensor([0.3124]))])\n",
            "Epoch: 15360 \n",
            " Test: 0.005922154523432255 \n",
            " Test loss: 0.013840782456099987 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6706])), ('bias', tensor([0.3124]))])\n",
            "Epoch: 15370 \n",
            " Test: 0.005887772887945175 \n",
            " Test loss: 0.013761562295258045 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6707])), ('bias', tensor([0.3123]))])\n",
            "Epoch: 15380 \n",
            " Test: 0.005853425711393356 \n",
            " Test loss: 0.013678884133696556 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6709])), ('bias', tensor([0.3122]))])\n",
            "Epoch: 15390 \n",
            " Test: 0.0058190179988741875 \n",
            " Test loss: 0.013596194796264172 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6711])), ('bias', tensor([0.3122]))])\n",
            "Epoch: 15400 \n",
            " Test: 0.005784692708402872 \n",
            " Test loss: 0.013520377688109875 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6712])), ('bias', tensor([0.3121]))])\n",
            "Epoch: 15410 \n",
            " Test: 0.005750305950641632 \n",
            " Test loss: 0.013441145420074463 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6714])), ('bias', tensor([0.3120]))])\n",
            "Epoch: 15420 \n",
            " Test: 0.005715944804251194 \n",
            " Test loss: 0.013355064205825329 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6716])), ('bias', tensor([0.3119]))])\n",
            "Epoch: 15430 \n",
            " Test: 0.005681618116796017 \n",
            " Test loss: 0.013279247097671032 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6717])), ('bias', tensor([0.3119]))])\n",
            "Epoch: 15440 \n",
            " Test: 0.005647263024002314 \n",
            " Test loss: 0.0131965521723032 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6719])), ('bias', tensor([0.3118]))])\n",
            "Epoch: 15450 \n",
            " Test: 0.005612855311483145 \n",
            " Test loss: 0.01311387401074171 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6721])), ('bias', tensor([0.3117]))])\n",
            "Epoch: 15460 \n",
            " Test: 0.0055785225704312325 \n",
            " Test loss: 0.013034647330641747 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6723])), ('bias', tensor([0.3117]))])\n",
            "Epoch: 15470 \n",
            " Test: 0.00554416049271822 \n",
            " Test loss: 0.01295883022248745 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6724])), ('bias', tensor([0.3116]))])\n",
            "Epoch: 15480 \n",
            " Test: 0.005509779788553715 \n",
            " Test loss: 0.012872743420302868 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6726])), ('bias', tensor([0.3115]))])\n",
            "Epoch: 15490 \n",
            " Test: 0.005475472658872604 \n",
            " Test loss: 0.012796914204955101 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6728])), ('bias', tensor([0.3114]))])\n",
            "Epoch: 15500 \n",
            " Test: 0.005441098473966122 \n",
            " Test loss: 0.012714231386780739 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6729])), ('bias', tensor([0.3114]))])\n",
            "Epoch: 15510 \n",
            " Test: 0.0054066963493824005 \n",
            " Test loss: 0.012635010294616222 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6731])), ('bias', tensor([0.3113]))])\n",
            "Epoch: 15520 \n",
            " Test: 0.005372360348701477 \n",
            " Test loss: 0.012552333064377308 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6733])), ('bias', tensor([0.3112]))])\n",
            "Epoch: 15530 \n",
            " Test: 0.005338020622730255 \n",
            " Test loss: 0.012473106384277344 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6735])), ('bias', tensor([0.3111]))])\n",
            "Epoch: 15540 \n",
            " Test: 0.005303625948727131 \n",
            " Test loss: 0.0123972836881876 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6736])), ('bias', tensor([0.3111]))])\n",
            "Epoch: 15550 \n",
            " Test: 0.0052693188190460205 \n",
            " Test loss: 0.01231460552662611 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6738])), ('bias', tensor([0.3110]))])\n",
            "Epoch: 15560 \n",
            " Test: 0.0052349381148815155 \n",
            " Test loss: 0.012231921777129173 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6740])), ('bias', tensor([0.3109]))])\n",
            "Epoch: 15570 \n",
            " Test: 0.0052005997858941555 \n",
            " Test loss: 0.012152689509093761 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6741])), ('bias', tensor([0.3109]))])\n",
            "Epoch: 15580 \n",
            " Test: 0.005166252143681049 \n",
            " Test loss: 0.012073462828993797 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6743])), ('bias', tensor([0.3108]))])\n",
            "Epoch: 15590 \n",
            " Test: 0.005131863057613373 \n",
            " Test loss: 0.011990780010819435 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6745])), ('bias', tensor([0.3107]))])\n",
            "Epoch: 15600 \n",
            " Test: 0.0050974758341908455 \n",
            " Test loss: 0.011914962902665138 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6747])), ('bias', tensor([0.3106]))])\n",
            "Epoch: 15610 \n",
            " Test: 0.005063167307525873 \n",
            " Test loss: 0.011832279153168201 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6748])), ('bias', tensor([0.3106]))])\n",
            "Epoch: 15620 \n",
            " Test: 0.0050287856720387936 \n",
            " Test loss: 0.01175304688513279 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6750])), ('bias', tensor([0.3105]))])\n",
            "Epoch: 15630 \n",
            " Test: 0.004994440823793411 \n",
            " Test loss: 0.011670375242829323 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6752])), ('bias', tensor([0.3104]))])\n",
            "Epoch: 15640 \n",
            " Test: 0.004960098769515753 \n",
            " Test loss: 0.011591148562729359 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6753])), ('bias', tensor([0.3104]))])\n",
            "Epoch: 15650 \n",
            " Test: 0.004925698973238468 \n",
            " Test loss: 0.011508459225296974 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6755])), ('bias', tensor([0.3103]))])\n",
            "Epoch: 15660 \n",
            " Test: 0.004891322460025549 \n",
            " Test loss: 0.011432647705078125 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6757])), ('bias', tensor([0.3102]))])\n",
            "Epoch: 15670 \n",
            " Test: 0.004856960382312536 \n",
            " Test loss: 0.011346554383635521 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6759])), ('bias', tensor([0.3101]))])\n",
            "Epoch: 15680 \n",
            " Test: 0.004822633229196072 \n",
            " Test loss: 0.011270737275481224 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6760])), ('bias', tensor([0.3101]))])\n",
            "Epoch: 15690 \n",
            " Test: 0.004788252525031567 \n",
            " Test loss: 0.011191505007445812 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6762])), ('bias', tensor([0.3100]))])\n",
            "Epoch: 15700 \n",
            " Test: 0.004753944929689169 \n",
            " Test loss: 0.011108827777206898 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6764])), ('bias', tensor([0.3099]))])\n",
            "Epoch: 15710 \n",
            " Test: 0.004719535354524851 \n",
            " Test loss: 0.011026149615645409 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6765])), ('bias', tensor([0.3099]))])\n",
            "Epoch: 15720 \n",
            " Test: 0.004685175605118275 \n",
            " Test loss: 0.010950326919555664 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6767])), ('bias', tensor([0.3098]))])\n",
            "Epoch: 15730 \n",
            " Test: 0.004650798626244068 \n",
            " Test loss: 0.010864228010177612 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6769])), ('bias', tensor([0.3097]))])\n",
            "Epoch: 15740 \n",
            " Test: 0.004616464488208294 \n",
            " Test loss: 0.0107849957421422 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6771])), ('bias', tensor([0.3096]))])\n",
            "Epoch: 15750 \n",
            " Test: 0.004582094494253397 \n",
            " Test loss: 0.010709190741181374 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6772])), ('bias', tensor([0.3096]))])\n",
            "Epoch: 15760 \n",
            " Test: 0.004547781310975552 \n",
            " Test loss: 0.010626506991684437 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6774])), ('bias', tensor([0.3095]))])\n",
            "Epoch: 15770 \n",
            " Test: 0.0045133777894079685 \n",
            " Test loss: 0.01054381113499403 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6776])), ('bias', tensor([0.3094]))])\n",
            "Epoch: 15780 \n",
            " Test: 0.004479043185710907 \n",
            " Test loss: 0.010464590974152088 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6777])), ('bias', tensor([0.3094]))])\n",
            "Epoch: 15790 \n",
            " Test: 0.004444638732820749 \n",
            " Test loss: 0.010385369881987572 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6779])), ('bias', tensor([0.3093]))])\n",
            "Epoch: 15800 \n",
            " Test: 0.004410300403833389 \n",
            " Test loss: 0.010302680544555187 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6781])), ('bias', tensor([0.3092]))])\n",
            "Epoch: 15810 \n",
            " Test: 0.004375954624265432 \n",
            " Test loss: 0.01022686343640089 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6782])), ('bias', tensor([0.3091]))])\n",
            "Epoch: 15820 \n",
            " Test: 0.004341618623584509 \n",
            " Test loss: 0.010144174098968506 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6784])), ('bias', tensor([0.3091]))])\n",
            "Epoch: 15830 \n",
            " Test: 0.00430726632475853 \n",
            " Test loss: 0.01006496511399746 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6786])), ('bias', tensor([0.3090]))])\n",
            "Epoch: 15840 \n",
            " Test: 0.004272884223610163 \n",
            " Test loss: 0.009982275776565075 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6788])), ('bias', tensor([0.3089]))])\n",
            "Epoch: 15850 \n",
            " Test: 0.004238543100655079 \n",
            " Test loss: 0.009903043508529663 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6789])), ('bias', tensor([0.3088]))])\n",
            "Epoch: 15860 \n",
            " Test: 0.004204140044748783 \n",
            " Test loss: 0.009820366278290749 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6791])), ('bias', tensor([0.3088]))])\n",
            "Epoch: 15870 \n",
            " Test: 0.00416980218142271 \n",
            " Test loss: 0.00974454265087843 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6793])), ('bias', tensor([0.3087]))])\n",
            "Epoch: 15880 \n",
            " Test: 0.004135458264499903 \n",
            " Test loss: 0.009661858901381493 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6794])), ('bias', tensor([0.3086]))])\n",
            "Epoch: 15890 \n",
            " Test: 0.00410111341625452 \n",
            " Test loss: 0.00958262663334608 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6796])), ('bias', tensor([0.3086]))])\n",
            "Epoch: 15900 \n",
            " Test: 0.004066733177751303 \n",
            " Test loss: 0.009503406472504139 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6798])), ('bias', tensor([0.3085]))])\n",
            "Epoch: 15910 \n",
            " Test: 0.00403238320723176 \n",
            " Test loss: 0.00942072831094265 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6800])), ('bias', tensor([0.3084]))])\n",
            "Epoch: 15920 \n",
            " Test: 0.003997978754341602 \n",
            " Test loss: 0.009338038973510265 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6801])), ('bias', tensor([0.3083]))])\n",
            "Epoch: 15930 \n",
            " Test: 0.003963652066886425 \n",
            " Test loss: 0.009262221865355968 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6803])), ('bias', tensor([0.3083]))])\n",
            "Epoch: 15940 \n",
            " Test: 0.003929264843463898 \n",
            " Test loss: 0.009182989597320557 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6805])), ('bias', tensor([0.3082]))])\n",
            "Epoch: 15950 \n",
            " Test: 0.003894903464242816 \n",
            " Test loss: 0.009096908383071423 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6806])), ('bias', tensor([0.3081]))])\n",
            "Epoch: 15960 \n",
            " Test: 0.003860577242448926 \n",
            " Test loss: 0.009021091274917126 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6808])), ('bias', tensor([0.3081]))])\n",
            "Epoch: 15970 \n",
            " Test: 0.003826223313808441 \n",
            " Test loss: 0.008938396349549294 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6810])), ('bias', tensor([0.3080]))])\n",
            "Epoch: 15980 \n",
            " Test: 0.0037918128073215485 \n",
            " Test loss: 0.008855718187987804 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6812])), ('bias', tensor([0.3079]))])\n",
            "Epoch: 15990 \n",
            " Test: 0.0037574812304228544 \n",
            " Test loss: 0.00877649150788784 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6813])), ('bias', tensor([0.3078]))])\n",
            "Epoch: 16000 \n",
            " Test: 0.0037231191527098417 \n",
            " Test loss: 0.008700674399733543 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6815])), ('bias', tensor([0.3078]))])\n",
            "Epoch: 16010 \n",
            " Test: 0.0036887384485453367 \n",
            " Test loss: 0.008614587597548962 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6817])), ('bias', tensor([0.3077]))])\n",
            "Epoch: 16020 \n",
            " Test: 0.003654430154711008 \n",
            " Test loss: 0.008538758382201195 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6818])), ('bias', tensor([0.3076]))])\n",
            "Epoch: 16030 \n",
            " Test: 0.003620057599619031 \n",
            " Test loss: 0.008456075564026833 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6820])), ('bias', tensor([0.3076]))])\n",
            "Epoch: 16040 \n",
            " Test: 0.003585655242204666 \n",
            " Test loss: 0.008376854471862316 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6822])), ('bias', tensor([0.3075]))])\n",
            "Epoch: 16050 \n",
            " Test: 0.0035513192415237427 \n",
            " Test loss: 0.008294177241623402 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6823])), ('bias', tensor([0.3074]))])\n",
            "Epoch: 16060 \n",
            " Test: 0.0035169795155525208 \n",
            " Test loss: 0.008214950561523438 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6825])), ('bias', tensor([0.3073]))])\n",
            "Epoch: 16070 \n",
            " Test: 0.003482584608718753 \n",
            " Test loss: 0.008139127865433693 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6827])), ('bias', tensor([0.3073]))])\n",
            "Epoch: 16080 \n",
            " Test: 0.003448277711868286 \n",
            " Test loss: 0.008056449703872204 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6829])), ('bias', tensor([0.3072]))])\n",
            "Epoch: 16090 \n",
            " Test: 0.003413895610719919 \n",
            " Test loss: 0.007973765954375267 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6830])), ('bias', tensor([0.3071]))])\n",
            "Epoch: 16100 \n",
            " Test: 0.003379557281732559 \n",
            " Test loss: 0.007894533686339855 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6832])), ('bias', tensor([0.3071]))])\n",
            "Epoch: 16110 \n",
            " Test: 0.0033452108036726713 \n",
            " Test loss: 0.007815307006239891 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6834])), ('bias', tensor([0.3070]))])\n",
            "Epoch: 16120 \n",
            " Test: 0.0033108219504356384 \n",
            " Test loss: 0.0077326237224042416 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6835])), ('bias', tensor([0.3069]))])\n",
            "Epoch: 16130 \n",
            " Test: 0.0032764344941824675 \n",
            " Test loss: 0.007656806614249945 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6837])), ('bias', tensor([0.3068]))])\n",
            "Epoch: 16140 \n",
            " Test: 0.003242126200348139 \n",
            " Test loss: 0.007574123330414295 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6839])), ('bias', tensor([0.3068]))])\n",
            "Epoch: 16150 \n",
            " Test: 0.003207744564861059 \n",
            " Test loss: 0.007494890596717596 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6841])), ('bias', tensor([0.3067]))])\n",
            "Epoch: 16160 \n",
            " Test: 0.003173399716615677 \n",
            " Test loss: 0.007412218954414129 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6842])), ('bias', tensor([0.3066]))])\n",
            "Epoch: 16170 \n",
            " Test: 0.0031390576623380184 \n",
            " Test loss: 0.007332992739975452 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6844])), ('bias', tensor([0.3066]))])\n",
            "Epoch: 16180 \n",
            " Test: 0.00310465763323009 \n",
            " Test loss: 0.007250302936881781 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6846])), ('bias', tensor([0.3065]))])\n",
            "Epoch: 16190 \n",
            " Test: 0.0030702813528478146 \n",
            " Test loss: 0.007174491882324219 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6847])), ('bias', tensor([0.3064]))])\n",
            "Epoch: 16200 \n",
            " Test: 0.0030359209049493074 \n",
            " Test loss: 0.007088399026542902 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6849])), ('bias', tensor([0.3063]))])\n",
            "Epoch: 16210 \n",
            " Test: 0.003001592354848981 \n",
            " Test loss: 0.007012581918388605 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6851])), ('bias', tensor([0.3063]))])\n",
            "Epoch: 16220 \n",
            " Test: 0.002967211650684476 \n",
            " Test loss: 0.006933349184691906 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6853])), ('bias', tensor([0.3062]))])\n",
            "Epoch: 16230 \n",
            " Test: 0.0029329038225114346 \n",
            " Test loss: 0.0068506719544529915 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6854])), ('bias', tensor([0.3061]))])\n",
            "Epoch: 16240 \n",
            " Test: 0.0028984942473471165 \n",
            " Test loss: 0.00676799425855279 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6856])), ('bias', tensor([0.3061]))])\n",
            "Epoch: 16250 \n",
            " Test: 0.0028641342651098967 \n",
            " Test loss: 0.006692171096801758 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6858])), ('bias', tensor([0.3060]))])\n",
            "Epoch: 16260 \n",
            " Test: 0.00282975728623569 \n",
            " Test loss: 0.006606072187423706 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6859])), ('bias', tensor([0.3059]))])\n",
            "Epoch: 16270 \n",
            " Test: 0.002795423613861203 \n",
            " Test loss: 0.006526833865791559 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6861])), ('bias', tensor([0.3058]))])\n",
            "Epoch: 16280 \n",
            " Test: 0.0027610533870756626 \n",
            " Test loss: 0.00645103445276618 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6863])), ('bias', tensor([0.3058]))])\n",
            "Epoch: 16290 \n",
            " Test: 0.002726740436628461 \n",
            " Test loss: 0.0063683451153337955 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6865])), ('bias', tensor([0.3057]))])\n",
            "Epoch: 16300 \n",
            " Test: 0.002692336682230234 \n",
            " Test loss: 0.006285655312240124 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6866])), ('bias', tensor([0.3056]))])\n",
            "Epoch: 16310 \n",
            " Test: 0.0026580020785331726 \n",
            " Test loss: 0.006206435151398182 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6868])), ('bias', tensor([0.3055]))])\n",
            "Epoch: 16320 \n",
            " Test: 0.002623597625643015 \n",
            " Test loss: 0.006127214524894953 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6870])), ('bias', tensor([0.3055]))])\n",
            "Epoch: 16330 \n",
            " Test: 0.002589259296655655 \n",
            " Test loss: 0.006044524721801281 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6871])), ('bias', tensor([0.3054]))])\n",
            "Epoch: 16340 \n",
            " Test: 0.002554913517087698 \n",
            " Test loss: 0.005968707613646984 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6873])), ('bias', tensor([0.3053]))])\n",
            "Epoch: 16350 \n",
            " Test: 0.0025205775164067745 \n",
            " Test loss: 0.0058860182762146 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6875])), ('bias', tensor([0.3053]))])\n",
            "Epoch: 16360 \n",
            " Test: 0.0024862252175807953 \n",
            " Test loss: 0.0058068097569048405 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6876])), ('bias', tensor([0.3052]))])\n",
            "Epoch: 16370 \n",
            " Test: 0.0024518431164324284 \n",
            " Test loss: 0.005724119953811169 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6878])), ('bias', tensor([0.3051]))])\n",
            "Epoch: 16380 \n",
            " Test: 0.002417501760646701 \n",
            " Test loss: 0.005644887685775757 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6880])), ('bias', tensor([0.3050]))])\n",
            "Epoch: 16390 \n",
            " Test: 0.002383098704740405 \n",
            " Test loss: 0.00556220393627882 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6882])), ('bias', tensor([0.3050]))])\n",
            "Epoch: 16400 \n",
            " Test: 0.0023487613070756197 \n",
            " Test loss: 0.005486386828124523 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6883])), ('bias', tensor([0.3049]))])\n",
            "Epoch: 16410 \n",
            " Test: 0.0023144171573221684 \n",
            " Test loss: 0.005403703544288874 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6885])), ('bias', tensor([0.3048]))])\n",
            "Epoch: 16420 \n",
            " Test: 0.0022800720762461424 \n",
            " Test loss: 0.0053244708105921745 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6887])), ('bias', tensor([0.3048]))])\n",
            "Epoch: 16430 \n",
            " Test: 0.0022456920705735683 \n",
            " Test loss: 0.005245250649750233 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6888])), ('bias', tensor([0.3047]))])\n",
            "Epoch: 16440 \n",
            " Test: 0.0022113421000540257 \n",
            " Test loss: 0.005162572953850031 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6890])), ('bias', tensor([0.3046]))])\n",
            "Epoch: 16450 \n",
            " Test: 0.0021769374143332243 \n",
            " Test loss: 0.005079883150756359 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6892])), ('bias', tensor([0.3045]))])\n",
            "Epoch: 16460 \n",
            " Test: 0.0021426111925393343 \n",
            " Test loss: 0.005004066042602062 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6894])), ('bias', tensor([0.3045]))])\n",
            "Epoch: 16470 \n",
            " Test: 0.0021082237362861633 \n",
            " Test loss: 0.00492483377456665 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6895])), ('bias', tensor([0.3044]))])\n",
            "Epoch: 16480 \n",
            " Test: 0.0020738623570650816 \n",
            " Test loss: 0.004838752560317516 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6897])), ('bias', tensor([0.3043]))])\n",
            "Epoch: 16490 \n",
            " Test: 0.0020395361352711916 \n",
            " Test loss: 0.0047629354521632195 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6899])), ('bias', tensor([0.3043]))])\n",
            "Epoch: 16500 \n",
            " Test: 0.002005182206630707 \n",
            " Test loss: 0.0046802400611341 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6900])), ('bias', tensor([0.3042]))])\n",
            "Epoch: 16510 \n",
            " Test: 0.001970771700143814 \n",
            " Test loss: 0.004597562365233898 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6902])), ('bias', tensor([0.3041]))])\n",
            "Epoch: 16520 \n",
            " Test: 0.00193644012324512 \n",
            " Test loss: 0.004518336150795221 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6904])), ('bias', tensor([0.3040]))])\n",
            "Epoch: 16530 \n",
            " Test: 0.0019020780455321074 \n",
            " Test loss: 0.0044425190426409245 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6906])), ('bias', tensor([0.3040]))])\n",
            "Epoch: 16540 \n",
            " Test: 0.0018676973413676023 \n",
            " Test loss: 0.004356431774795055 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6907])), ('bias', tensor([0.3039]))])\n",
            "Epoch: 16550 \n",
            " Test: 0.0018333889311179519 \n",
            " Test loss: 0.004280603025108576 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6909])), ('bias', tensor([0.3038]))])\n",
            "Epoch: 16560 \n",
            " Test: 0.0017990164924412966 \n",
            " Test loss: 0.004197919275611639 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6911])), ('bias', tensor([0.3038]))])\n",
            "Epoch: 16570 \n",
            " Test: 0.0017646141350269318 \n",
            " Test loss: 0.00411869864910841 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6912])), ('bias', tensor([0.3037]))])\n",
            "Epoch: 16580 \n",
            " Test: 0.0017302781343460083 \n",
            " Test loss: 0.004036021418869495 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6914])), ('bias', tensor([0.3036]))])\n",
            "Epoch: 16590 \n",
            " Test: 0.0016959384083747864 \n",
            " Test loss: 0.003956794738769531 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6916])), ('bias', tensor([0.3035]))])\n",
            "Epoch: 16600 \n",
            " Test: 0.0016615435015410185 \n",
            " Test loss: 0.0038809715770184994 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6917])), ('bias', tensor([0.3035]))])\n",
            "Epoch: 16610 \n",
            " Test: 0.0016272366046905518 \n",
            " Test loss: 0.0037982941139489412 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6919])), ('bias', tensor([0.3034]))])\n",
            "Epoch: 16620 \n",
            " Test: 0.001592854387126863 \n",
            " Test loss: 0.003715610597282648 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6921])), ('bias', tensor([0.3033]))])\n",
            "Epoch: 16630 \n",
            " Test: 0.0015585161745548248 \n",
            " Test loss: 0.0036363780964165926 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6923])), ('bias', tensor([0.3032]))])\n",
            "Epoch: 16640 \n",
            " Test: 0.001524169696494937 \n",
            " Test loss: 0.003557151649147272 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6924])), ('bias', tensor([0.3032]))])\n",
            "Epoch: 16650 \n",
            " Test: 0.001489780843257904 \n",
            " Test loss: 0.0034744678996503353 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6926])), ('bias', tensor([0.3031]))])\n",
            "Epoch: 16660 \n",
            " Test: 0.001455393387004733 \n",
            " Test loss: 0.0033986507914960384 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6928])), ('bias', tensor([0.3030]))])\n",
            "Epoch: 16670 \n",
            " Test: 0.0014210849767550826 \n",
            " Test loss: 0.0033159672748297453 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6929])), ('bias', tensor([0.3030]))])\n",
            "Epoch: 16680 \n",
            " Test: 0.0013867035740986466 \n",
            " Test loss: 0.00323673477396369 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6931])), ('bias', tensor([0.3029]))])\n",
            "Epoch: 16690 \n",
            " Test: 0.0013523586094379425 \n",
            " Test loss: 0.003154063131660223 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6933])), ('bias', tensor([0.3028]))])\n",
            "Epoch: 16700 \n",
            " Test: 0.0013180166715756059 \n",
            " Test loss: 0.0030748366843909025 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6935])), ('bias', tensor([0.3027]))])\n",
            "Epoch: 16710 \n",
            " Test: 0.0012836165260523558 \n",
            " Test loss: 0.0029921471141278744 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6936])), ('bias', tensor([0.3027]))])\n",
            "Epoch: 16720 \n",
            " Test: 0.001249240362085402 \n",
            " Test loss: 0.0029163360595703125 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6938])), ('bias', tensor([0.3026]))])\n",
            "Epoch: 16730 \n",
            " Test: 0.001214879797771573 \n",
            " Test loss: 0.0028302432037889957 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6940])), ('bias', tensor([0.3025]))])\n",
            "Epoch: 16740 \n",
            " Test: 0.0011805512476712465 \n",
            " Test loss: 0.002754426095634699 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6941])), ('bias', tensor([0.3025]))])\n",
            "Epoch: 16750 \n",
            " Test: 0.0011461705435067415 \n",
            " Test loss: 0.0026751935947686434 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6943])), ('bias', tensor([0.3024]))])\n",
            "Epoch: 16760 \n",
            " Test: 0.001111863530240953 \n",
            " Test loss: 0.0025925158988684416 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6945])), ('bias', tensor([0.3023]))])\n",
            "Epoch: 16770 \n",
            " Test: 0.0010774530237540603 \n",
            " Test loss: 0.0025098384357988834 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6947])), ('bias', tensor([0.3022]))])\n",
            "Epoch: 16780 \n",
            " Test: 0.0010430931579321623 \n",
            " Test loss: 0.0024340152740478516 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6948])), ('bias', tensor([0.3022]))])\n",
            "Epoch: 16790 \n",
            " Test: 0.0010087169939652085 \n",
            " Test loss: 0.0023479163646698 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6950])), ('bias', tensor([0.3021]))])\n",
            "Epoch: 16800 \n",
            " Test: 0.0009743824484758079 \n",
            " Test loss: 0.002268678043037653 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6952])), ('bias', tensor([0.3020]))])\n",
            "Epoch: 16810 \n",
            " Test: 0.0009400121634826064 \n",
            " Test loss: 0.002192878630012274 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6953])), ('bias', tensor([0.3020]))])\n",
            "Epoch: 16820 \n",
            " Test: 0.0009056992712430656 \n",
            " Test loss: 0.0021101892925798893 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6955])), ('bias', tensor([0.3019]))])\n",
            "Epoch: 16830 \n",
            " Test: 0.0008712954586371779 \n",
            " Test loss: 0.002027499722316861 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6957])), ('bias', tensor([0.3018]))])\n",
            "Epoch: 16840 \n",
            " Test: 0.000836962484754622 \n",
            " Test loss: 0.001948279095813632 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6959])), ('bias', tensor([0.3017]))])\n",
            "Epoch: 16850 \n",
            " Test: 0.0008025564020499587 \n",
            " Test loss: 0.0018690585857257247 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6960])), ('bias', tensor([0.3017]))])\n",
            "Epoch: 16860 \n",
            " Test: 0.0007682197028771043 \n",
            " Test loss: 0.0017863691318780184 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6962])), ('bias', tensor([0.3016]))])\n",
            "Epoch: 16870 \n",
            " Test: 0.0007338725263252854 \n",
            " Test loss: 0.0017105520237237215 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6964])), ('bias', tensor([0.3015]))])\n",
            "Epoch: 16880 \n",
            " Test: 0.0006995379808358848 \n",
            " Test loss: 0.0016278624534606934 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6965])), ('bias', tensor([0.3015]))])\n",
            "Epoch: 16890 \n",
            " Test: 0.000665183353703469 \n",
            " Test loss: 0.0015486538177356124 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6967])), ('bias', tensor([0.3014]))])\n",
            "Epoch: 16900 \n",
            " Test: 0.0006308018928393722 \n",
            " Test loss: 0.001465964363887906 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6969])), ('bias', tensor([0.3013]))])\n",
            "Epoch: 16910 \n",
            " Test: 0.0005964629235677421 \n",
            " Test loss: 0.0013867318630218506 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6970])), ('bias', tensor([0.3012]))])\n",
            "Epoch: 16920 \n",
            " Test: 0.0005620576557703316 \n",
            " Test loss: 0.0013040483463555574 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6972])), ('bias', tensor([0.3012]))])\n",
            "Epoch: 16930 \n",
            " Test: 0.0005277201416902244 \n",
            " Test loss: 0.0012282312382012606 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6974])), ('bias', tensor([0.3011]))])\n",
            "Epoch: 16940 \n",
            " Test: 0.0004933759337291121 \n",
            " Test loss: 0.0011455476051196456 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6976])), ('bias', tensor([0.3010]))])\n",
            "Epoch: 16950 \n",
            " Test: 0.0004590310272760689 \n",
            " Test loss: 0.001066315220668912 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6977])), ('bias', tensor([0.3010]))])\n",
            "Epoch: 16960 \n",
            " Test: 0.00042464956641197205 \n",
            " Test loss: 0.0009870945941656828 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6979])), ('bias', tensor([0.3009]))])\n",
            "Epoch: 16970 \n",
            " Test: 0.0003903009055647999 \n",
            " Test loss: 0.0009044170146808028 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6981])), ('bias', tensor([0.3008]))])\n",
            "Epoch: 16980 \n",
            " Test: 0.00035589709295891225 \n",
            " Test loss: 0.0008217275026254356 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6982])), ('bias', tensor([0.3007]))])\n",
            "Epoch: 16990 \n",
            " Test: 0.000321570027153939 \n",
            " Test loss: 0.0007459103944711387 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6984])), ('bias', tensor([0.3007]))])\n",
            "Epoch: 17000 \n",
            " Test: 0.00028718262910842896 \n",
            " Test loss: 0.0006666779518127441 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6986])), ('bias', tensor([0.3006]))])\n",
            "Epoch: 17010 \n",
            " Test: 0.00025282055139541626 \n",
            " Test loss: 0.0005805969121865928 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6988])), ('bias', tensor([0.3005]))])\n",
            "Epoch: 17020 \n",
            " Test: 0.00021849498443771154 \n",
            " Test loss: 0.0005047798040322959 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6989])), ('bias', tensor([0.3004]))])\n",
            "Epoch: 17030 \n",
            " Test: 0.0001841425837483257 \n",
            " Test loss: 0.0004220843256916851 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6991])), ('bias', tensor([0.3004]))])\n",
            "Epoch: 17040 \n",
            " Test: 0.0001497305929660797 \n",
            " Test loss: 0.00033940671710297465 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6993])), ('bias', tensor([0.3003]))])\n",
            "Epoch: 17050 \n",
            " Test: 0.00011539905972313136 \n",
            " Test loss: 0.0002601802407298237 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6994])), ('bias', tensor([0.3002]))])\n",
            "Epoch: 17060 \n",
            " Test: 8.103847358142957e-05 \n",
            " Test loss: 0.00018436313257552683 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6996])), ('bias', tensor([0.3002]))])\n",
            "Epoch: 17070 \n",
            " Test: 4.6656281483592466e-05 \n",
            " Test loss: 9.827614121604711e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.6998])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17080 \n",
            " Test: 1.7155707610072568e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17090 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17100 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17110 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17120 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17130 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17140 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17150 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17160 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17170 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17180 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17190 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17200 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17210 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17220 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17230 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17240 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17250 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17260 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17270 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17280 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17290 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17300 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17310 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17320 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17330 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17340 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17350 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17360 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17370 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17380 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17390 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17400 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17410 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17420 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17430 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17440 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17450 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17460 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17470 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17480 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17490 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17500 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17510 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17520 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17530 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17540 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17550 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17560 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17570 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17580 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17590 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17600 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17610 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17620 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17630 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17640 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17650 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17660 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17670 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17680 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17690 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17700 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17710 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17720 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17730 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17740 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17750 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17760 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17770 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17780 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17790 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17800 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17810 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17820 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17830 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17840 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17850 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17860 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17870 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17880 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17890 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17900 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17910 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17920 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17930 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17940 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17950 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17960 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17970 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17980 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 17990 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18000 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18010 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18020 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18030 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18040 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18050 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18060 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18070 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18080 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18090 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18100 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18110 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18120 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18130 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18140 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18150 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18160 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18170 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18180 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18190 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18200 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18210 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18220 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18230 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18240 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18250 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18260 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18270 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18280 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18290 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18300 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18310 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18320 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18330 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18340 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18350 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18360 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18370 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18380 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18390 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18400 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18410 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18420 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18430 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18440 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18450 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18460 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18470 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18480 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18490 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18500 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18510 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18520 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18530 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18540 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18550 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18560 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18570 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18580 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18590 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18600 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18610 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18620 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18630 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18640 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18650 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18660 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18670 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18680 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18690 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18700 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18710 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18720 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18730 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18740 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18750 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18760 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18770 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18780 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18790 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18800 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18810 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18820 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18830 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18840 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18850 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18860 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18870 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18880 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18890 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18900 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18910 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18920 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18930 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18940 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18950 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18960 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18970 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18980 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 18990 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19000 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19010 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19020 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19030 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19040 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19050 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19060 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19070 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19080 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19090 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19100 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19110 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19120 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19130 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19140 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19150 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19160 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19170 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19180 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19190 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19200 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19210 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19220 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19230 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19240 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19250 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19260 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19270 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19280 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19290 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19300 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19310 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19320 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19330 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19340 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19350 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19360 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19370 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19380 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19390 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19400 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19410 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19420 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19430 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19440 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19450 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19460 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19470 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19480 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19490 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19500 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19510 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19520 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19530 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19540 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19550 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19560 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19570 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19580 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19590 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19600 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19610 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19620 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19630 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19640 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19650 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19660 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19670 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19680 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19690 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19700 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19710 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19720 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19730 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19740 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19750 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19760 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19770 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19780 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19790 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19800 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19810 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19820 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19830 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19840 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19850 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19860 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19870 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19880 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19890 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19900 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19910 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19920 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19930 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19940 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19950 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19960 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19970 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19980 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n",
            "Epoch: 19990 \n",
            " Test: 6.57841592328623e-05 \n",
            " Test loss: 3.293156623840332e-05 \n",
            "\n",
            "OrderedDict([('weights', tensor([0.7000])), ('bias', tensor([0.3001]))])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epoch_count, loss_values, test_loss_values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OeI-mJ2zXdTq",
        "outputId": "bed5c31f-c939-4366-ad18-d81cf033a68e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([0,\n",
              "  10,\n",
              "  20,\n",
              "  30,\n",
              "  40,\n",
              "  50,\n",
              "  60,\n",
              "  70,\n",
              "  80,\n",
              "  90,\n",
              "  100,\n",
              "  110,\n",
              "  120,\n",
              "  130,\n",
              "  140,\n",
              "  150,\n",
              "  160,\n",
              "  170,\n",
              "  180,\n",
              "  190,\n",
              "  200,\n",
              "  210,\n",
              "  220,\n",
              "  230,\n",
              "  240,\n",
              "  250,\n",
              "  260,\n",
              "  270,\n",
              "  280,\n",
              "  290,\n",
              "  300,\n",
              "  310,\n",
              "  320,\n",
              "  330,\n",
              "  340,\n",
              "  350,\n",
              "  360,\n",
              "  370,\n",
              "  380,\n",
              "  390,\n",
              "  400,\n",
              "  410,\n",
              "  420,\n",
              "  430,\n",
              "  440,\n",
              "  450,\n",
              "  460,\n",
              "  470,\n",
              "  480,\n",
              "  490,\n",
              "  500,\n",
              "  510,\n",
              "  520,\n",
              "  530,\n",
              "  540,\n",
              "  550,\n",
              "  560,\n",
              "  570,\n",
              "  580,\n",
              "  590,\n",
              "  600,\n",
              "  610,\n",
              "  620,\n",
              "  630,\n",
              "  640,\n",
              "  650,\n",
              "  660,\n",
              "  670,\n",
              "  680,\n",
              "  690,\n",
              "  700,\n",
              "  710,\n",
              "  720,\n",
              "  730,\n",
              "  740,\n",
              "  750,\n",
              "  760,\n",
              "  770,\n",
              "  780,\n",
              "  790,\n",
              "  800,\n",
              "  810,\n",
              "  820,\n",
              "  830,\n",
              "  840,\n",
              "  850,\n",
              "  860,\n",
              "  870,\n",
              "  880,\n",
              "  890,\n",
              "  900,\n",
              "  910,\n",
              "  920,\n",
              "  930,\n",
              "  940,\n",
              "  950,\n",
              "  960,\n",
              "  970,\n",
              "  980,\n",
              "  990,\n",
              "  1000,\n",
              "  1010,\n",
              "  1020,\n",
              "  1030,\n",
              "  1040,\n",
              "  1050,\n",
              "  1060,\n",
              "  1070,\n",
              "  1080,\n",
              "  1090,\n",
              "  1100,\n",
              "  1110,\n",
              "  1120,\n",
              "  1130,\n",
              "  1140,\n",
              "  1150,\n",
              "  1160,\n",
              "  1170,\n",
              "  1180,\n",
              "  1190,\n",
              "  1200,\n",
              "  1210,\n",
              "  1220,\n",
              "  1230,\n",
              "  1240,\n",
              "  1250,\n",
              "  1260,\n",
              "  1270,\n",
              "  1280,\n",
              "  1290,\n",
              "  1300,\n",
              "  1310,\n",
              "  1320,\n",
              "  1330,\n",
              "  1340,\n",
              "  1350,\n",
              "  1360,\n",
              "  1370,\n",
              "  1380,\n",
              "  1390,\n",
              "  1400,\n",
              "  1410,\n",
              "  1420,\n",
              "  1430,\n",
              "  1440,\n",
              "  1450,\n",
              "  1460,\n",
              "  1470,\n",
              "  1480,\n",
              "  1490,\n",
              "  1500,\n",
              "  1510,\n",
              "  1520,\n",
              "  1530,\n",
              "  1540,\n",
              "  1550,\n",
              "  1560,\n",
              "  1570,\n",
              "  1580,\n",
              "  1590,\n",
              "  1600,\n",
              "  1610,\n",
              "  1620,\n",
              "  1630,\n",
              "  1640,\n",
              "  1650,\n",
              "  1660,\n",
              "  1670,\n",
              "  1680,\n",
              "  1690,\n",
              "  1700,\n",
              "  1710,\n",
              "  1720,\n",
              "  1730,\n",
              "  1740,\n",
              "  1750,\n",
              "  1760,\n",
              "  1770,\n",
              "  1780,\n",
              "  1790,\n",
              "  1800,\n",
              "  1810,\n",
              "  1820,\n",
              "  1830,\n",
              "  1840,\n",
              "  1850,\n",
              "  1860,\n",
              "  1870,\n",
              "  1880,\n",
              "  1890,\n",
              "  1900,\n",
              "  1910,\n",
              "  1920,\n",
              "  1930,\n",
              "  1940,\n",
              "  1950,\n",
              "  1960,\n",
              "  1970,\n",
              "  1980,\n",
              "  1990,\n",
              "  2000,\n",
              "  2010,\n",
              "  2020,\n",
              "  2030,\n",
              "  2040,\n",
              "  2050,\n",
              "  2060,\n",
              "  2070,\n",
              "  2080,\n",
              "  2090,\n",
              "  2100,\n",
              "  2110,\n",
              "  2120,\n",
              "  2130,\n",
              "  2140,\n",
              "  2150,\n",
              "  2160,\n",
              "  2170,\n",
              "  2180,\n",
              "  2190,\n",
              "  2200,\n",
              "  2210,\n",
              "  2220,\n",
              "  2230,\n",
              "  2240,\n",
              "  2250,\n",
              "  2260,\n",
              "  2270,\n",
              "  2280,\n",
              "  2290,\n",
              "  2300,\n",
              "  2310,\n",
              "  2320,\n",
              "  2330,\n",
              "  2340,\n",
              "  2350,\n",
              "  2360,\n",
              "  2370,\n",
              "  2380,\n",
              "  2390,\n",
              "  2400,\n",
              "  2410,\n",
              "  2420,\n",
              "  2430,\n",
              "  2440,\n",
              "  2450,\n",
              "  2460,\n",
              "  2470,\n",
              "  2480,\n",
              "  2490,\n",
              "  2500,\n",
              "  2510,\n",
              "  2520,\n",
              "  2530,\n",
              "  2540,\n",
              "  2550,\n",
              "  2560,\n",
              "  2570,\n",
              "  2580,\n",
              "  2590,\n",
              "  2600,\n",
              "  2610,\n",
              "  2620,\n",
              "  2630,\n",
              "  2640,\n",
              "  2650,\n",
              "  2660,\n",
              "  2670,\n",
              "  2680,\n",
              "  2690,\n",
              "  2700,\n",
              "  2710,\n",
              "  2720,\n",
              "  2730,\n",
              "  2740,\n",
              "  2750,\n",
              "  2760,\n",
              "  2770,\n",
              "  2780,\n",
              "  2790,\n",
              "  2800,\n",
              "  2810,\n",
              "  2820,\n",
              "  2830,\n",
              "  2840,\n",
              "  2850,\n",
              "  2860,\n",
              "  2870,\n",
              "  2880,\n",
              "  2890,\n",
              "  2900,\n",
              "  2910,\n",
              "  2920,\n",
              "  2930,\n",
              "  2940,\n",
              "  2950,\n",
              "  2960,\n",
              "  2970,\n",
              "  2980,\n",
              "  2990,\n",
              "  3000,\n",
              "  3010,\n",
              "  3020,\n",
              "  3030,\n",
              "  3040,\n",
              "  3050,\n",
              "  3060,\n",
              "  3070,\n",
              "  3080,\n",
              "  3090,\n",
              "  3100,\n",
              "  3110,\n",
              "  3120,\n",
              "  3130,\n",
              "  3140,\n",
              "  3150,\n",
              "  3160,\n",
              "  3170,\n",
              "  3180,\n",
              "  3190,\n",
              "  3200,\n",
              "  3210,\n",
              "  3220,\n",
              "  3230,\n",
              "  3240,\n",
              "  3250,\n",
              "  3260,\n",
              "  3270,\n",
              "  3280,\n",
              "  3290,\n",
              "  3300,\n",
              "  3310,\n",
              "  3320,\n",
              "  3330,\n",
              "  3340,\n",
              "  3350,\n",
              "  3360,\n",
              "  3370,\n",
              "  3380,\n",
              "  3390,\n",
              "  3400,\n",
              "  3410,\n",
              "  3420,\n",
              "  3430,\n",
              "  3440,\n",
              "  3450,\n",
              "  3460,\n",
              "  3470,\n",
              "  3480,\n",
              "  3490,\n",
              "  3500,\n",
              "  3510,\n",
              "  3520,\n",
              "  3530,\n",
              "  3540,\n",
              "  3550,\n",
              "  3560,\n",
              "  3570,\n",
              "  3580,\n",
              "  3590,\n",
              "  3600,\n",
              "  3610,\n",
              "  3620,\n",
              "  3630,\n",
              "  3640,\n",
              "  3650,\n",
              "  3660,\n",
              "  3670,\n",
              "  3680,\n",
              "  3690,\n",
              "  3700,\n",
              "  3710,\n",
              "  3720,\n",
              "  3730,\n",
              "  3740,\n",
              "  3750,\n",
              "  3760,\n",
              "  3770,\n",
              "  3780,\n",
              "  3790,\n",
              "  3800,\n",
              "  3810,\n",
              "  3820,\n",
              "  3830,\n",
              "  3840,\n",
              "  3850,\n",
              "  3860,\n",
              "  3870,\n",
              "  3880,\n",
              "  3890,\n",
              "  3900,\n",
              "  3910,\n",
              "  3920,\n",
              "  3930,\n",
              "  3940,\n",
              "  3950,\n",
              "  3960,\n",
              "  3970,\n",
              "  3980,\n",
              "  3990,\n",
              "  4000,\n",
              "  4010,\n",
              "  4020,\n",
              "  4030,\n",
              "  4040,\n",
              "  4050,\n",
              "  4060,\n",
              "  4070,\n",
              "  4080,\n",
              "  4090,\n",
              "  4100,\n",
              "  4110,\n",
              "  4120,\n",
              "  4130,\n",
              "  4140,\n",
              "  4150,\n",
              "  4160,\n",
              "  4170,\n",
              "  4180,\n",
              "  4190,\n",
              "  4200,\n",
              "  4210,\n",
              "  4220,\n",
              "  4230,\n",
              "  4240,\n",
              "  4250,\n",
              "  4260,\n",
              "  4270,\n",
              "  4280,\n",
              "  4290,\n",
              "  4300,\n",
              "  4310,\n",
              "  4320,\n",
              "  4330,\n",
              "  4340,\n",
              "  4350,\n",
              "  4360,\n",
              "  4370,\n",
              "  4380,\n",
              "  4390,\n",
              "  4400,\n",
              "  4410,\n",
              "  4420,\n",
              "  4430,\n",
              "  4440,\n",
              "  4450,\n",
              "  4460,\n",
              "  4470,\n",
              "  4480,\n",
              "  4490,\n",
              "  4500,\n",
              "  4510,\n",
              "  4520,\n",
              "  4530,\n",
              "  4540,\n",
              "  4550,\n",
              "  4560,\n",
              "  4570,\n",
              "  4580,\n",
              "  4590,\n",
              "  4600,\n",
              "  4610,\n",
              "  4620,\n",
              "  4630,\n",
              "  4640,\n",
              "  4650,\n",
              "  4660,\n",
              "  4670,\n",
              "  4680,\n",
              "  4690,\n",
              "  4700,\n",
              "  4710,\n",
              "  4720,\n",
              "  4730,\n",
              "  4740,\n",
              "  4750,\n",
              "  4760,\n",
              "  4770,\n",
              "  4780,\n",
              "  4790,\n",
              "  4800,\n",
              "  4810,\n",
              "  4820,\n",
              "  4830,\n",
              "  4840,\n",
              "  4850,\n",
              "  4860,\n",
              "  4870,\n",
              "  4880,\n",
              "  4890,\n",
              "  4900,\n",
              "  4910,\n",
              "  4920,\n",
              "  4930,\n",
              "  4940,\n",
              "  4950,\n",
              "  4960,\n",
              "  4970,\n",
              "  4980,\n",
              "  4990,\n",
              "  5000,\n",
              "  5010,\n",
              "  5020,\n",
              "  5030,\n",
              "  5040,\n",
              "  5050,\n",
              "  5060,\n",
              "  5070,\n",
              "  5080,\n",
              "  5090,\n",
              "  5100,\n",
              "  5110,\n",
              "  5120,\n",
              "  5130,\n",
              "  5140,\n",
              "  5150,\n",
              "  5160,\n",
              "  5170,\n",
              "  5180,\n",
              "  5190,\n",
              "  5200,\n",
              "  5210,\n",
              "  5220,\n",
              "  5230,\n",
              "  5240,\n",
              "  5250,\n",
              "  5260,\n",
              "  5270,\n",
              "  5280,\n",
              "  5290,\n",
              "  5300,\n",
              "  5310,\n",
              "  5320,\n",
              "  5330,\n",
              "  5340,\n",
              "  5350,\n",
              "  5360,\n",
              "  5370,\n",
              "  5380,\n",
              "  5390,\n",
              "  5400,\n",
              "  5410,\n",
              "  5420,\n",
              "  5430,\n",
              "  5440,\n",
              "  5450,\n",
              "  5460,\n",
              "  5470,\n",
              "  5480,\n",
              "  5490,\n",
              "  5500,\n",
              "  5510,\n",
              "  5520,\n",
              "  5530,\n",
              "  5540,\n",
              "  5550,\n",
              "  5560,\n",
              "  5570,\n",
              "  5580,\n",
              "  5590,\n",
              "  5600,\n",
              "  5610,\n",
              "  5620,\n",
              "  5630,\n",
              "  5640,\n",
              "  5650,\n",
              "  5660,\n",
              "  5670,\n",
              "  5680,\n",
              "  5690,\n",
              "  5700,\n",
              "  5710,\n",
              "  5720,\n",
              "  5730,\n",
              "  5740,\n",
              "  5750,\n",
              "  5760,\n",
              "  5770,\n",
              "  5780,\n",
              "  5790,\n",
              "  5800,\n",
              "  5810,\n",
              "  5820,\n",
              "  5830,\n",
              "  5840,\n",
              "  5850,\n",
              "  5860,\n",
              "  5870,\n",
              "  5880,\n",
              "  5890,\n",
              "  5900,\n",
              "  5910,\n",
              "  5920,\n",
              "  5930,\n",
              "  5940,\n",
              "  5950,\n",
              "  5960,\n",
              "  5970,\n",
              "  5980,\n",
              "  5990,\n",
              "  6000,\n",
              "  6010,\n",
              "  6020,\n",
              "  6030,\n",
              "  6040,\n",
              "  6050,\n",
              "  6060,\n",
              "  6070,\n",
              "  6080,\n",
              "  6090,\n",
              "  6100,\n",
              "  6110,\n",
              "  6120,\n",
              "  6130,\n",
              "  6140,\n",
              "  6150,\n",
              "  6160,\n",
              "  6170,\n",
              "  6180,\n",
              "  6190,\n",
              "  6200,\n",
              "  6210,\n",
              "  6220,\n",
              "  6230,\n",
              "  6240,\n",
              "  6250,\n",
              "  6260,\n",
              "  6270,\n",
              "  6280,\n",
              "  6290,\n",
              "  6300,\n",
              "  6310,\n",
              "  6320,\n",
              "  6330,\n",
              "  6340,\n",
              "  6350,\n",
              "  6360,\n",
              "  6370,\n",
              "  6380,\n",
              "  6390,\n",
              "  6400,\n",
              "  6410,\n",
              "  6420,\n",
              "  6430,\n",
              "  6440,\n",
              "  6450,\n",
              "  6460,\n",
              "  6470,\n",
              "  6480,\n",
              "  6490,\n",
              "  6500,\n",
              "  6510,\n",
              "  6520,\n",
              "  6530,\n",
              "  6540,\n",
              "  6550,\n",
              "  6560,\n",
              "  6570,\n",
              "  6580,\n",
              "  6590,\n",
              "  6600,\n",
              "  6610,\n",
              "  6620,\n",
              "  6630,\n",
              "  6640,\n",
              "  6650,\n",
              "  6660,\n",
              "  6670,\n",
              "  6680,\n",
              "  6690,\n",
              "  6700,\n",
              "  6710,\n",
              "  6720,\n",
              "  6730,\n",
              "  6740,\n",
              "  6750,\n",
              "  6760,\n",
              "  6770,\n",
              "  6780,\n",
              "  6790,\n",
              "  6800,\n",
              "  6810,\n",
              "  6820,\n",
              "  6830,\n",
              "  6840,\n",
              "  6850,\n",
              "  6860,\n",
              "  6870,\n",
              "  6880,\n",
              "  6890,\n",
              "  6900,\n",
              "  6910,\n",
              "  6920,\n",
              "  6930,\n",
              "  6940,\n",
              "  6950,\n",
              "  6960,\n",
              "  6970,\n",
              "  6980,\n",
              "  6990,\n",
              "  7000,\n",
              "  7010,\n",
              "  7020,\n",
              "  7030,\n",
              "  7040,\n",
              "  7050,\n",
              "  7060,\n",
              "  7070,\n",
              "  7080,\n",
              "  7090,\n",
              "  7100,\n",
              "  7110,\n",
              "  7120,\n",
              "  7130,\n",
              "  7140,\n",
              "  7150,\n",
              "  7160,\n",
              "  7170,\n",
              "  7180,\n",
              "  7190,\n",
              "  7200,\n",
              "  7210,\n",
              "  7220,\n",
              "  7230,\n",
              "  7240,\n",
              "  7250,\n",
              "  7260,\n",
              "  7270,\n",
              "  7280,\n",
              "  7290,\n",
              "  7300,\n",
              "  7310,\n",
              "  7320,\n",
              "  7330,\n",
              "  7340,\n",
              "  7350,\n",
              "  7360,\n",
              "  7370,\n",
              "  7380,\n",
              "  7390,\n",
              "  7400,\n",
              "  7410,\n",
              "  7420,\n",
              "  7430,\n",
              "  7440,\n",
              "  7450,\n",
              "  7460,\n",
              "  7470,\n",
              "  7480,\n",
              "  7490,\n",
              "  7500,\n",
              "  7510,\n",
              "  7520,\n",
              "  7530,\n",
              "  7540,\n",
              "  7550,\n",
              "  7560,\n",
              "  7570,\n",
              "  7580,\n",
              "  7590,\n",
              "  7600,\n",
              "  7610,\n",
              "  7620,\n",
              "  7630,\n",
              "  7640,\n",
              "  7650,\n",
              "  7660,\n",
              "  7670,\n",
              "  7680,\n",
              "  7690,\n",
              "  7700,\n",
              "  7710,\n",
              "  7720,\n",
              "  7730,\n",
              "  7740,\n",
              "  7750,\n",
              "  7760,\n",
              "  7770,\n",
              "  7780,\n",
              "  7790,\n",
              "  7800,\n",
              "  7810,\n",
              "  7820,\n",
              "  7830,\n",
              "  7840,\n",
              "  7850,\n",
              "  7860,\n",
              "  7870,\n",
              "  7880,\n",
              "  7890,\n",
              "  7900,\n",
              "  7910,\n",
              "  7920,\n",
              "  7930,\n",
              "  7940,\n",
              "  7950,\n",
              "  7960,\n",
              "  7970,\n",
              "  7980,\n",
              "  7990,\n",
              "  8000,\n",
              "  8010,\n",
              "  8020,\n",
              "  8030,\n",
              "  8040,\n",
              "  8050,\n",
              "  8060,\n",
              "  8070,\n",
              "  8080,\n",
              "  8090,\n",
              "  8100,\n",
              "  8110,\n",
              "  8120,\n",
              "  8130,\n",
              "  8140,\n",
              "  8150,\n",
              "  8160,\n",
              "  8170,\n",
              "  8180,\n",
              "  8190,\n",
              "  8200,\n",
              "  8210,\n",
              "  8220,\n",
              "  8230,\n",
              "  8240,\n",
              "  8250,\n",
              "  8260,\n",
              "  8270,\n",
              "  8280,\n",
              "  8290,\n",
              "  8300,\n",
              "  8310,\n",
              "  8320,\n",
              "  8330,\n",
              "  8340,\n",
              "  8350,\n",
              "  8360,\n",
              "  8370,\n",
              "  8380,\n",
              "  8390,\n",
              "  8400,\n",
              "  8410,\n",
              "  8420,\n",
              "  8430,\n",
              "  8440,\n",
              "  8450,\n",
              "  8460,\n",
              "  8470,\n",
              "  8480,\n",
              "  8490,\n",
              "  8500,\n",
              "  8510,\n",
              "  8520,\n",
              "  8530,\n",
              "  8540,\n",
              "  8550,\n",
              "  8560,\n",
              "  8570,\n",
              "  8580,\n",
              "  8590,\n",
              "  8600,\n",
              "  8610,\n",
              "  8620,\n",
              "  8630,\n",
              "  8640,\n",
              "  8650,\n",
              "  8660,\n",
              "  8670,\n",
              "  8680,\n",
              "  8690,\n",
              "  8700,\n",
              "  8710,\n",
              "  8720,\n",
              "  8730,\n",
              "  8740,\n",
              "  8750,\n",
              "  8760,\n",
              "  8770,\n",
              "  8780,\n",
              "  8790,\n",
              "  8800,\n",
              "  8810,\n",
              "  8820,\n",
              "  8830,\n",
              "  8840,\n",
              "  8850,\n",
              "  8860,\n",
              "  8870,\n",
              "  8880,\n",
              "  8890,\n",
              "  8900,\n",
              "  8910,\n",
              "  8920,\n",
              "  8930,\n",
              "  8940,\n",
              "  8950,\n",
              "  8960,\n",
              "  8970,\n",
              "  8980,\n",
              "  8990,\n",
              "  9000,\n",
              "  9010,\n",
              "  9020,\n",
              "  9030,\n",
              "  9040,\n",
              "  9050,\n",
              "  9060,\n",
              "  9070,\n",
              "  9080,\n",
              "  9090,\n",
              "  9100,\n",
              "  9110,\n",
              "  9120,\n",
              "  9130,\n",
              "  9140,\n",
              "  9150,\n",
              "  9160,\n",
              "  9170,\n",
              "  9180,\n",
              "  9190,\n",
              "  9200,\n",
              "  9210,\n",
              "  9220,\n",
              "  9230,\n",
              "  9240,\n",
              "  9250,\n",
              "  9260,\n",
              "  9270,\n",
              "  9280,\n",
              "  9290,\n",
              "  9300,\n",
              "  9310,\n",
              "  9320,\n",
              "  9330,\n",
              "  9340,\n",
              "  9350,\n",
              "  9360,\n",
              "  9370,\n",
              "  9380,\n",
              "  9390,\n",
              "  9400,\n",
              "  9410,\n",
              "  9420,\n",
              "  9430,\n",
              "  9440,\n",
              "  9450,\n",
              "  9460,\n",
              "  9470,\n",
              "  9480,\n",
              "  9490,\n",
              "  9500,\n",
              "  9510,\n",
              "  9520,\n",
              "  9530,\n",
              "  9540,\n",
              "  9550,\n",
              "  9560,\n",
              "  9570,\n",
              "  9580,\n",
              "  9590,\n",
              "  9600,\n",
              "  9610,\n",
              "  9620,\n",
              "  9630,\n",
              "  9640,\n",
              "  9650,\n",
              "  9660,\n",
              "  9670,\n",
              "  9680,\n",
              "  9690,\n",
              "  9700,\n",
              "  9710,\n",
              "  9720,\n",
              "  9730,\n",
              "  9740,\n",
              "  9750,\n",
              "  9760,\n",
              "  9770,\n",
              "  9780,\n",
              "  9790,\n",
              "  9800,\n",
              "  9810,\n",
              "  9820,\n",
              "  9830,\n",
              "  9840,\n",
              "  9850,\n",
              "  9860,\n",
              "  9870,\n",
              "  9880,\n",
              "  9890,\n",
              "  9900,\n",
              "  9910,\n",
              "  9920,\n",
              "  9930,\n",
              "  9940,\n",
              "  9950,\n",
              "  9960,\n",
              "  9970,\n",
              "  9980,\n",
              "  9990,\n",
              "  ...],\n",
              " [tensor(0.3129, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.3117, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.3106, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.3094, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.3083, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.3071, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.3060, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.3048, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.3037, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.3025, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.3014, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.3002, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2991, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2979, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2968, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2956, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2944, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2933, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2921, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2910, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2898, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2887, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2875, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2864, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2852, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2841, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2829, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2818, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2806, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2795, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2783, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2772, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2760, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2749, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2737, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2726, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2714, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2703, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2691, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2679, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2668, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2656, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2645, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2633, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2622, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2610, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2599, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2587, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2576, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2564, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2553, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2541, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2530, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2518, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2507, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2495, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2484, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2472, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2461, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2449, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2438, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2426, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2414, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2403, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2391, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2380, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2368, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2357, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2345, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2334, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2322, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2311, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2299, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2288, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2276, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2265, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2253, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2242, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2230, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2219, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2207, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2196, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2184, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2173, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2161, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2149, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2138, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2126, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2115, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2103, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2092, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2080, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2069, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2057, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2046, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2034, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2023, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2011, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.2000, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1988, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1977, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1965, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1954, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1942, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1931, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1919, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1908, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1896, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1884, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1873, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1861, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1850, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1838, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1827, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1815, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1804, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1792, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1781, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1769, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1758, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1746, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1735, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1723, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1712, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1700, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1689, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1677, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1666, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1654, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1643, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1631, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1619, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1608, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1596, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1585, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1573, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1562, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1550, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1539, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1527, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1516, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1504, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1493, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1481, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1470, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1458, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1447, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1435, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1424, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1412, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1401, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1389, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1378, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1366, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1355, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1343, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1331, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1320, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1308, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1297, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1285, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1274, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1262, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1251, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1239, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1228, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1216, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1205, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1193, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1182, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1170, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1159, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1148, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1137, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1127, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1116, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1106, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1095, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1085, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1076, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1066, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1056, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1047, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1037, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1028, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1019, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1010, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.1002, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0993, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0984, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0975, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0967, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0959, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0951, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0943, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0936, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0928, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0920, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0912, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0905, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0898, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0891, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0884, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0877, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0870, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0863, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0857, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0850, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0844, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0838, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0832, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0825, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0819, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0813, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0808, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0802, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0796, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0791, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0785, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0780, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0774, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0769, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0764, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0759, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0754, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0749, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0744, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0739, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0734, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0730, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0725, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0721, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0717, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0713, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0708, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0704, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0700, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0695, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0691, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0688, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0684, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0680, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0677, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0673, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0669, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0666, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0662, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0658, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0655, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0652, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0649, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0645, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0642, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0639, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0636, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0633, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0630, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0627, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0624, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0621, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0618, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0616, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0613, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0610, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0608, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0605, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0602, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0600, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0597, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0595, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0592, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0590, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0588, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0586, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0584, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0582, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0579, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0577, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0575, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0573, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0571, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0568, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0567, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0565, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0563, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0561, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0559, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0558, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0556, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0554, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0552, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0550, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0549, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0547, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0545, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0544, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0542, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0541, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0539, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0538, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0536, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0535, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0534, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0532, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0531, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0529, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0528, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0526, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0525, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0524, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0522, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0521, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0520, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0519, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0518, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0517, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0516, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0514, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0513, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0512, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0511, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0510, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0509, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0508, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0507, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0505, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0504, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0503, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0502, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0501, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0501, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0500, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0499, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0498, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0497, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0496, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0495, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0495, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0494, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0493, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0492, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0491, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0490, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0489, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0488, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0488, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0487, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0486, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0485, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0484, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0484, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0483, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0482, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0482, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0481, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0480, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0480, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0479, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0478, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0478, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0477, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0477, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0476, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0475, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0475, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0474, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0473, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0473, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0472, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0471, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0471, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0470, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0469, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0469, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0468, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0468, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0467, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0467, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0466, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0466, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0465, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0465, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0464, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0464, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0463, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0462, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0462, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0461, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0461, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0460, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0460, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0459, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0459, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0458, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0458, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0457, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0457, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0456, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0456, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0455, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0455, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0454, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0454, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0453, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0453, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0452, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0452, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0451, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0451, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0450, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0450, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0450, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0449, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0449, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0448, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0448, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0448, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0447, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0447, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0446, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0446, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0446, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0445, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0445, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0444, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0444, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0444, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0443, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0443, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0442, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0442, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0442, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0441, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0441, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0440, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0440, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0440, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0439, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0439, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0438, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0438, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0438, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0437, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0437, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0436, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0436, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0436, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0435, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0435, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0434, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0434, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0434, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0433, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0433, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0432, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0432, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0432, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0431, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0431, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0430, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0430, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0430, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0429, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0429, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0428, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0428, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0428, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0427, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0427, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0427, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0426, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0426, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0426, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0425, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0425, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0425, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0424, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0424, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0424, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0423, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0423, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0423, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0422, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0422, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0421, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0421, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0421, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0420, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0420, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0420, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0419, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0419, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0419, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0418, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0418, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0418, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0417, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0417, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0417, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0416, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0416, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0416, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0415, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0415, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0415, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0414, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0414, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0413, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0413, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0413, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0412, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0412, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0412, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0411, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0411, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0411, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0410, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0410, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0410, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0409, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0409, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0409, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0408, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0408, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0408, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0407, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0407, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0406, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0406, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0406, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0405, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0405, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0405, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0404, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0404, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0404, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0403, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0403, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0403, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0402, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0402, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0402, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0401, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0401, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0401, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0400, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0400, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0399, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0399, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0399, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0398, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0398, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0398, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0397, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0397, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0397, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0396, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0396, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0396, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0395, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0395, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0395, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0394, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0394, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0394, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0393, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0393, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0393, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0392, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0392, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0391, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0391, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0391, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0390, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0390, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0390, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0389, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0389, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0389, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0388, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0388, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0388, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0387, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0387, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0387, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0386, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0386, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0386, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0385, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0385, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0384, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0384, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0384, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0383, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0383, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0383, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0382, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0382, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0382, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0381, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0381, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0381, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0380, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0380, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0380, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0379, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0379, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0379, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0378, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0378, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0378, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0377, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0377, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0376, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0376, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0376, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0375, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0375, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0375, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0374, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0374, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0374, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0373, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0373, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0373, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0372, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0372, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0372, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0371, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0371, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0371, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0370, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0370, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0369, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0369, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0369, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0368, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0368, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0368, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0367, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0367, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0367, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0366, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0366, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0366, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0365, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0365, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0365, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0364, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0364, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0364, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0363, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0363, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0363, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0362, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0362, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0362, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0361, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0361, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0361, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0360, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0360, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0360, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0359, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0359, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0358, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0358, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0358, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0357, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0357, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0357, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0356, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0356, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0356, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0355, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0355, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0355, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0354, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0354, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0354, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0353, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0353, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0353, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0352, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0352, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0352, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0351, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0351, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0351, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0350, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0350, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0350, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0349, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0349, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0349, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0348, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0348, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0347, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0347, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0347, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0346, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0346, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0346, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0345, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0345, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0345, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0344, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0344, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0344, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0343, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0343, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0343, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0342, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0342, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0342, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0341, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0341, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0341, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0340, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0340, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0340, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0339, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0339, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0339, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0338, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0338, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0338, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0337, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0337, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0337, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0336, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0336, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0335, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0335, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0335, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0334, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0334, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0334, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0333, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0333, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0333, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0332, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0332, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0332, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0331, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0331, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0331, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0330, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0330, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0330, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0329, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0329, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0329, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0328, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0328, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0328, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0327, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0327, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0327, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0326, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0326, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0326, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0325, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0325, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0324, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0324, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0324, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0323, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0323, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0323, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0322, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0322, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0322, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0321, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0321, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0321, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0320, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0320, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0320, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0319, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0319, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0319, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0318, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0318, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0318, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0317, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0317, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0317, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0316, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0316, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0316, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0315, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0315, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0315, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0314, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0314, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0313, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0313, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0313, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0312, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0312, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0312, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0311, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0311, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0311, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0310, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0310, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0310, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0309, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0309, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0309, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0308, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0308, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0308, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0307, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0307, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0307, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0306, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0306, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0306, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0305, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0305, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0305, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0304, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0304, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0304, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0303, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0303, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0302, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0302, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0302, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0301, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0301, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0301, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0300, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0300, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0300, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0299, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0299, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0299, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0298, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0298, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0298, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0297, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0297, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0297, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0296, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0296, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0296, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0295, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0295, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0295, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0294, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0294, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0294, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0293, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0293, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0293, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0292, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0292, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0291, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0291, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0291, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0290, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0290, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0290, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0289, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0289, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0289, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0288, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0288, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0288, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0287, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0287, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0287, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0286, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0286, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0286, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0285, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0285, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0285, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0284, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0284, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0284, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0283, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0283, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0283, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0282, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0282, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0282, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0281, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0281, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0280, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0280, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0280, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0279, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0279, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0279, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0278, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0278, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0278, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0277, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0277, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0277, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0276, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0276, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0276, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0275, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0275, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0275, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0274, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0274, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0274, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0273, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0273, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0273, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0272, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0272, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0272, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0271, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0271, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0271, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0270, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0270, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0269, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0269, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0269, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0268, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0268, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0268, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0267, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0267, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0267, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0266, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0266, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0266, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0265, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0265, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0265, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0264, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0264, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0264, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0263, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0263, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0263, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0262, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0262, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0262, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0261, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0261, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0261, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0260, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0260, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0260, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0259, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0259, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0259, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0258, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0258, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0257, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0257, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0257, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0256, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0256, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0256, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0255, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0255, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0255, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0254, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0254, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0254, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0253, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0253, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0253, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0252, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0252, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0252, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0251, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0251, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0251, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0250, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0250, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0250, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0249, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0249, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0249, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0248, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0248, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0248, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0247, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0247, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0246, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0246, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0246, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0245, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0245, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0245, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0244, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0244, grad_fn=<MeanBackward0>),\n",
              "  tensor(0.0244, grad_fn=<MeanBackward0>),\n",
              "  ...],\n",
              " [tensor(0.4944),\n",
              "  tensor(0.4931),\n",
              "  tensor(0.4917),\n",
              "  tensor(0.4904),\n",
              "  tensor(0.4890),\n",
              "  tensor(0.4877),\n",
              "  tensor(0.4863),\n",
              "  tensor(0.4850),\n",
              "  tensor(0.4836),\n",
              "  tensor(0.4823),\n",
              "  tensor(0.4809),\n",
              "  tensor(0.4796),\n",
              "  tensor(0.4782),\n",
              "  tensor(0.4769),\n",
              "  tensor(0.4755),\n",
              "  tensor(0.4742),\n",
              "  tensor(0.4728),\n",
              "  tensor(0.4715),\n",
              "  tensor(0.4702),\n",
              "  tensor(0.4688),\n",
              "  tensor(0.4675),\n",
              "  tensor(0.4661),\n",
              "  tensor(0.4648),\n",
              "  tensor(0.4634),\n",
              "  tensor(0.4621),\n",
              "  tensor(0.4607),\n",
              "  tensor(0.4594),\n",
              "  tensor(0.4580),\n",
              "  tensor(0.4567),\n",
              "  tensor(0.4553),\n",
              "  tensor(0.4540),\n",
              "  tensor(0.4526),\n",
              "  tensor(0.4513),\n",
              "  tensor(0.4499),\n",
              "  tensor(0.4486),\n",
              "  tensor(0.4472),\n",
              "  tensor(0.4459),\n",
              "  tensor(0.4446),\n",
              "  tensor(0.4432),\n",
              "  tensor(0.4419),\n",
              "  tensor(0.4405),\n",
              "  tensor(0.4392),\n",
              "  tensor(0.4378),\n",
              "  tensor(0.4365),\n",
              "  tensor(0.4351),\n",
              "  tensor(0.4338),\n",
              "  tensor(0.4324),\n",
              "  tensor(0.4311),\n",
              "  tensor(0.4297),\n",
              "  tensor(0.4284),\n",
              "  tensor(0.4270),\n",
              "  tensor(0.4257),\n",
              "  tensor(0.4243),\n",
              "  tensor(0.4230),\n",
              "  tensor(0.4217),\n",
              "  tensor(0.4203),\n",
              "  tensor(0.4190),\n",
              "  tensor(0.4176),\n",
              "  tensor(0.4163),\n",
              "  tensor(0.4149),\n",
              "  tensor(0.4136),\n",
              "  tensor(0.4122),\n",
              "  tensor(0.4109),\n",
              "  tensor(0.4095),\n",
              "  tensor(0.4082),\n",
              "  tensor(0.4068),\n",
              "  tensor(0.4055),\n",
              "  tensor(0.4041),\n",
              "  tensor(0.4028),\n",
              "  tensor(0.4014),\n",
              "  tensor(0.4001),\n",
              "  tensor(0.3987),\n",
              "  tensor(0.3974),\n",
              "  tensor(0.3961),\n",
              "  tensor(0.3947),\n",
              "  tensor(0.3934),\n",
              "  tensor(0.3920),\n",
              "  tensor(0.3907),\n",
              "  tensor(0.3893),\n",
              "  tensor(0.3880),\n",
              "  tensor(0.3866),\n",
              "  tensor(0.3853),\n",
              "  tensor(0.3839),\n",
              "  tensor(0.3826),\n",
              "  tensor(0.3812),\n",
              "  tensor(0.3799),\n",
              "  tensor(0.3785),\n",
              "  tensor(0.3772),\n",
              "  tensor(0.3758),\n",
              "  tensor(0.3745),\n",
              "  tensor(0.3732),\n",
              "  tensor(0.3718),\n",
              "  tensor(0.3705),\n",
              "  tensor(0.3691),\n",
              "  tensor(0.3678),\n",
              "  tensor(0.3664),\n",
              "  tensor(0.3651),\n",
              "  tensor(0.3637),\n",
              "  tensor(0.3624),\n",
              "  tensor(0.3610),\n",
              "  tensor(0.3597),\n",
              "  tensor(0.3583),\n",
              "  tensor(0.3570),\n",
              "  tensor(0.3556),\n",
              "  tensor(0.3543),\n",
              "  tensor(0.3529),\n",
              "  tensor(0.3516),\n",
              "  tensor(0.3502),\n",
              "  tensor(0.3489),\n",
              "  tensor(0.3476),\n",
              "  tensor(0.3462),\n",
              "  tensor(0.3449),\n",
              "  tensor(0.3435),\n",
              "  tensor(0.3422),\n",
              "  tensor(0.3408),\n",
              "  tensor(0.3395),\n",
              "  tensor(0.3381),\n",
              "  tensor(0.3368),\n",
              "  tensor(0.3354),\n",
              "  tensor(0.3341),\n",
              "  tensor(0.3327),\n",
              "  tensor(0.3314),\n",
              "  tensor(0.3300),\n",
              "  tensor(0.3287),\n",
              "  tensor(0.3273),\n",
              "  tensor(0.3260),\n",
              "  tensor(0.3247),\n",
              "  tensor(0.3233),\n",
              "  tensor(0.3220),\n",
              "  tensor(0.3206),\n",
              "  tensor(0.3193),\n",
              "  tensor(0.3179),\n",
              "  tensor(0.3166),\n",
              "  tensor(0.3152),\n",
              "  tensor(0.3139),\n",
              "  tensor(0.3125),\n",
              "  tensor(0.3112),\n",
              "  tensor(0.3098),\n",
              "  tensor(0.3085),\n",
              "  tensor(0.3071),\n",
              "  tensor(0.3058),\n",
              "  tensor(0.3044),\n",
              "  tensor(0.3031),\n",
              "  tensor(0.3018),\n",
              "  tensor(0.3004),\n",
              "  tensor(0.2991),\n",
              "  tensor(0.2977),\n",
              "  tensor(0.2964),\n",
              "  tensor(0.2950),\n",
              "  tensor(0.2937),\n",
              "  tensor(0.2923),\n",
              "  tensor(0.2910),\n",
              "  tensor(0.2896),\n",
              "  tensor(0.2883),\n",
              "  tensor(0.2869),\n",
              "  tensor(0.2856),\n",
              "  tensor(0.2842),\n",
              "  tensor(0.2829),\n",
              "  tensor(0.2815),\n",
              "  tensor(0.2802),\n",
              "  tensor(0.2789),\n",
              "  tensor(0.2775),\n",
              "  tensor(0.2762),\n",
              "  tensor(0.2748),\n",
              "  tensor(0.2735),\n",
              "  tensor(0.2721),\n",
              "  tensor(0.2708),\n",
              "  tensor(0.2694),\n",
              "  tensor(0.2681),\n",
              "  tensor(0.2667),\n",
              "  tensor(0.2654),\n",
              "  tensor(0.2640),\n",
              "  tensor(0.2627),\n",
              "  tensor(0.2614),\n",
              "  tensor(0.2601),\n",
              "  tensor(0.2588),\n",
              "  tensor(0.2575),\n",
              "  tensor(0.2562),\n",
              "  tensor(0.2550),\n",
              "  tensor(0.2537),\n",
              "  tensor(0.2525),\n",
              "  tensor(0.2512),\n",
              "  tensor(0.2500),\n",
              "  tensor(0.2488),\n",
              "  tensor(0.2475),\n",
              "  tensor(0.2463),\n",
              "  tensor(0.2451),\n",
              "  tensor(0.2439),\n",
              "  tensor(0.2427),\n",
              "  tensor(0.2415),\n",
              "  tensor(0.2404),\n",
              "  tensor(0.2392),\n",
              "  tensor(0.2380),\n",
              "  tensor(0.2369),\n",
              "  tensor(0.2358),\n",
              "  tensor(0.2346),\n",
              "  tensor(0.2335),\n",
              "  tensor(0.2323),\n",
              "  tensor(0.2312),\n",
              "  tensor(0.2301),\n",
              "  tensor(0.2291),\n",
              "  tensor(0.2280),\n",
              "  tensor(0.2269),\n",
              "  tensor(0.2258),\n",
              "  tensor(0.2247),\n",
              "  tensor(0.2236),\n",
              "  tensor(0.2226),\n",
              "  tensor(0.2216),\n",
              "  tensor(0.2205),\n",
              "  tensor(0.2195),\n",
              "  tensor(0.2185),\n",
              "  tensor(0.2174),\n",
              "  tensor(0.2164),\n",
              "  tensor(0.2154),\n",
              "  tensor(0.2144),\n",
              "  tensor(0.2134),\n",
              "  tensor(0.2125),\n",
              "  tensor(0.2115),\n",
              "  tensor(0.2105),\n",
              "  tensor(0.2095),\n",
              "  tensor(0.2085),\n",
              "  tensor(0.2076),\n",
              "  tensor(0.2067),\n",
              "  tensor(0.2058),\n",
              "  tensor(0.2048),\n",
              "  tensor(0.2039),\n",
              "  tensor(0.2030),\n",
              "  tensor(0.2021),\n",
              "  tensor(0.2012),\n",
              "  tensor(0.2002),\n",
              "  tensor(0.1994),\n",
              "  tensor(0.1985),\n",
              "  tensor(0.1977),\n",
              "  tensor(0.1968),\n",
              "  tensor(0.1959),\n",
              "  tensor(0.1951),\n",
              "  tensor(0.1942),\n",
              "  tensor(0.1933),\n",
              "  tensor(0.1925),\n",
              "  tensor(0.1917),\n",
              "  tensor(0.1909),\n",
              "  tensor(0.1901),\n",
              "  tensor(0.1893),\n",
              "  tensor(0.1885),\n",
              "  tensor(0.1876),\n",
              "  tensor(0.1868),\n",
              "  tensor(0.1860),\n",
              "  tensor(0.1852),\n",
              "  tensor(0.1845),\n",
              "  tensor(0.1837),\n",
              "  tensor(0.1830),\n",
              "  tensor(0.1822),\n",
              "  tensor(0.1815),\n",
              "  tensor(0.1807),\n",
              "  tensor(0.1800),\n",
              "  tensor(0.1792),\n",
              "  tensor(0.1785),\n",
              "  tensor(0.1777),\n",
              "  tensor(0.1770),\n",
              "  tensor(0.1763),\n",
              "  tensor(0.1756),\n",
              "  tensor(0.1750),\n",
              "  tensor(0.1743),\n",
              "  tensor(0.1736),\n",
              "  tensor(0.1729),\n",
              "  tensor(0.1722),\n",
              "  tensor(0.1715),\n",
              "  tensor(0.1708),\n",
              "  tensor(0.1701),\n",
              "  tensor(0.1695),\n",
              "  tensor(0.1688),\n",
              "  tensor(0.1682),\n",
              "  tensor(0.1676),\n",
              "  tensor(0.1670),\n",
              "  tensor(0.1663),\n",
              "  tensor(0.1657),\n",
              "  tensor(0.1651),\n",
              "  tensor(0.1645),\n",
              "  tensor(0.1638),\n",
              "  tensor(0.1632),\n",
              "  tensor(0.1626),\n",
              "  tensor(0.1620),\n",
              "  tensor(0.1614),\n",
              "  tensor(0.1608),\n",
              "  tensor(0.1603),\n",
              "  tensor(0.1597),\n",
              "  tensor(0.1591),\n",
              "  tensor(0.1586),\n",
              "  tensor(0.1580),\n",
              "  tensor(0.1574),\n",
              "  tensor(0.1569),\n",
              "  tensor(0.1563),\n",
              "  tensor(0.1557),\n",
              "  tensor(0.1552),\n",
              "  tensor(0.1546),\n",
              "  tensor(0.1541),\n",
              "  tensor(0.1536),\n",
              "  tensor(0.1531),\n",
              "  tensor(0.1526),\n",
              "  tensor(0.1521),\n",
              "  tensor(0.1516),\n",
              "  tensor(0.1511),\n",
              "  tensor(0.1506),\n",
              "  tensor(0.1501),\n",
              "  tensor(0.1496),\n",
              "  tensor(0.1491),\n",
              "  tensor(0.1486),\n",
              "  tensor(0.1481),\n",
              "  tensor(0.1476),\n",
              "  tensor(0.1471),\n",
              "  tensor(0.1466),\n",
              "  tensor(0.1462),\n",
              "  tensor(0.1457),\n",
              "  tensor(0.1453),\n",
              "  tensor(0.1448),\n",
              "  tensor(0.1444),\n",
              "  tensor(0.1440),\n",
              "  tensor(0.1435),\n",
              "  tensor(0.1431),\n",
              "  tensor(0.1426),\n",
              "  tensor(0.1422),\n",
              "  tensor(0.1418),\n",
              "  tensor(0.1413),\n",
              "  tensor(0.1409),\n",
              "  tensor(0.1404),\n",
              "  tensor(0.1400),\n",
              "  tensor(0.1396),\n",
              "  tensor(0.1391),\n",
              "  tensor(0.1388),\n",
              "  tensor(0.1384),\n",
              "  tensor(0.1380),\n",
              "  tensor(0.1376),\n",
              "  tensor(0.1373),\n",
              "  tensor(0.1369),\n",
              "  tensor(0.1365),\n",
              "  tensor(0.1361),\n",
              "  tensor(0.1358),\n",
              "  tensor(0.1354),\n",
              "  tensor(0.1350),\n",
              "  tensor(0.1346),\n",
              "  tensor(0.1343),\n",
              "  tensor(0.1339),\n",
              "  tensor(0.1335),\n",
              "  tensor(0.1331),\n",
              "  tensor(0.1328),\n",
              "  tensor(0.1324),\n",
              "  tensor(0.1320),\n",
              "  tensor(0.1316),\n",
              "  tensor(0.1313),\n",
              "  tensor(0.1310),\n",
              "  tensor(0.1307),\n",
              "  tensor(0.1304),\n",
              "  tensor(0.1301),\n",
              "  tensor(0.1298),\n",
              "  tensor(0.1294),\n",
              "  tensor(0.1291),\n",
              "  tensor(0.1288),\n",
              "  tensor(0.1285),\n",
              "  tensor(0.1282),\n",
              "  tensor(0.1279),\n",
              "  tensor(0.1276),\n",
              "  tensor(0.1273),\n",
              "  tensor(0.1270),\n",
              "  tensor(0.1266),\n",
              "  tensor(0.1263),\n",
              "  tensor(0.1260),\n",
              "  tensor(0.1257),\n",
              "  tensor(0.1254),\n",
              "  tensor(0.1251),\n",
              "  tensor(0.1248),\n",
              "  tensor(0.1245),\n",
              "  tensor(0.1242),\n",
              "  tensor(0.1238),\n",
              "  tensor(0.1235),\n",
              "  tensor(0.1233),\n",
              "  tensor(0.1230),\n",
              "  tensor(0.1228),\n",
              "  tensor(0.1226),\n",
              "  tensor(0.1223),\n",
              "  tensor(0.1221),\n",
              "  tensor(0.1218),\n",
              "  tensor(0.1216),\n",
              "  tensor(0.1213),\n",
              "  tensor(0.1211),\n",
              "  tensor(0.1208),\n",
              "  tensor(0.1206),\n",
              "  tensor(0.1203),\n",
              "  tensor(0.1201),\n",
              "  tensor(0.1199),\n",
              "  tensor(0.1196),\n",
              "  tensor(0.1194),\n",
              "  tensor(0.1191),\n",
              "  tensor(0.1189),\n",
              "  tensor(0.1186),\n",
              "  tensor(0.1184),\n",
              "  tensor(0.1181),\n",
              "  tensor(0.1179),\n",
              "  tensor(0.1177),\n",
              "  tensor(0.1174),\n",
              "  tensor(0.1172),\n",
              "  tensor(0.1169),\n",
              "  tensor(0.1167),\n",
              "  tensor(0.1164),\n",
              "  tensor(0.1162),\n",
              "  tensor(0.1159),\n",
              "  tensor(0.1157),\n",
              "  tensor(0.1154),\n",
              "  tensor(0.1152),\n",
              "  tensor(0.1150),\n",
              "  tensor(0.1148),\n",
              "  tensor(0.1146),\n",
              "  tensor(0.1144),\n",
              "  tensor(0.1142),\n",
              "  tensor(0.1141),\n",
              "  tensor(0.1139),\n",
              "  tensor(0.1137),\n",
              "  tensor(0.1135),\n",
              "  tensor(0.1134),\n",
              "  tensor(0.1132),\n",
              "  tensor(0.1130),\n",
              "  tensor(0.1128),\n",
              "  tensor(0.1126),\n",
              "  tensor(0.1125),\n",
              "  tensor(0.1123),\n",
              "  tensor(0.1121),\n",
              "  tensor(0.1119),\n",
              "  tensor(0.1118),\n",
              "  tensor(0.1116),\n",
              "  tensor(0.1114),\n",
              "  tensor(0.1112),\n",
              "  tensor(0.1110),\n",
              "  tensor(0.1109),\n",
              "  tensor(0.1107),\n",
              "  tensor(0.1105),\n",
              "  tensor(0.1103),\n",
              "  tensor(0.1102),\n",
              "  tensor(0.1100),\n",
              "  tensor(0.1098),\n",
              "  tensor(0.1096),\n",
              "  tensor(0.1094),\n",
              "  tensor(0.1093),\n",
              "  tensor(0.1091),\n",
              "  tensor(0.1089),\n",
              "  tensor(0.1087),\n",
              "  tensor(0.1086),\n",
              "  tensor(0.1084),\n",
              "  tensor(0.1082),\n",
              "  tensor(0.1080),\n",
              "  tensor(0.1078),\n",
              "  tensor(0.1077),\n",
              "  tensor(0.1075),\n",
              "  tensor(0.1073),\n",
              "  tensor(0.1071),\n",
              "  tensor(0.1069),\n",
              "  tensor(0.1068),\n",
              "  tensor(0.1066),\n",
              "  tensor(0.1064),\n",
              "  tensor(0.1062),\n",
              "  tensor(0.1061),\n",
              "  tensor(0.1059),\n",
              "  tensor(0.1057),\n",
              "  tensor(0.1055),\n",
              "  tensor(0.1053),\n",
              "  tensor(0.1052),\n",
              "  tensor(0.1050),\n",
              "  tensor(0.1049),\n",
              "  tensor(0.1048),\n",
              "  tensor(0.1046),\n",
              "  tensor(0.1045),\n",
              "  tensor(0.1044),\n",
              "  tensor(0.1043),\n",
              "  tensor(0.1042),\n",
              "  tensor(0.1041),\n",
              "  tensor(0.1040),\n",
              "  tensor(0.1039),\n",
              "  tensor(0.1038),\n",
              "  tensor(0.1036),\n",
              "  tensor(0.1035),\n",
              "  tensor(0.1034),\n",
              "  tensor(0.1033),\n",
              "  tensor(0.1032),\n",
              "  tensor(0.1031),\n",
              "  tensor(0.1030),\n",
              "  tensor(0.1029),\n",
              "  tensor(0.1028),\n",
              "  tensor(0.1027),\n",
              "  tensor(0.1025),\n",
              "  tensor(0.1024),\n",
              "  tensor(0.1023),\n",
              "  tensor(0.1022),\n",
              "  tensor(0.1021),\n",
              "  tensor(0.1020),\n",
              "  tensor(0.1019),\n",
              "  tensor(0.1018),\n",
              "  tensor(0.1017),\n",
              "  tensor(0.1016),\n",
              "  tensor(0.1014),\n",
              "  tensor(0.1013),\n",
              "  tensor(0.1012),\n",
              "  tensor(0.1011),\n",
              "  tensor(0.1010),\n",
              "  tensor(0.1009),\n",
              "  tensor(0.1008),\n",
              "  tensor(0.1007),\n",
              "  tensor(0.1006),\n",
              "  tensor(0.1005),\n",
              "  tensor(0.1003),\n",
              "  tensor(0.1002),\n",
              "  tensor(0.1001),\n",
              "  tensor(0.1000),\n",
              "  tensor(0.0999),\n",
              "  tensor(0.0998),\n",
              "  tensor(0.0997),\n",
              "  tensor(0.0996),\n",
              "  tensor(0.0995),\n",
              "  tensor(0.0994),\n",
              "  tensor(0.0992),\n",
              "  tensor(0.0991),\n",
              "  tensor(0.0990),\n",
              "  tensor(0.0989),\n",
              "  tensor(0.0988),\n",
              "  tensor(0.0987),\n",
              "  tensor(0.0986),\n",
              "  tensor(0.0985),\n",
              "  tensor(0.0984),\n",
              "  tensor(0.0983),\n",
              "  tensor(0.0981),\n",
              "  tensor(0.0980),\n",
              "  tensor(0.0979),\n",
              "  tensor(0.0978),\n",
              "  tensor(0.0977),\n",
              "  tensor(0.0976),\n",
              "  tensor(0.0975),\n",
              "  tensor(0.0974),\n",
              "  tensor(0.0973),\n",
              "  tensor(0.0972),\n",
              "  tensor(0.0970),\n",
              "  tensor(0.0969),\n",
              "  tensor(0.0968),\n",
              "  tensor(0.0967),\n",
              "  tensor(0.0966),\n",
              "  tensor(0.0965),\n",
              "  tensor(0.0964),\n",
              "  tensor(0.0963),\n",
              "  tensor(0.0962),\n",
              "  tensor(0.0960),\n",
              "  tensor(0.0959),\n",
              "  tensor(0.0958),\n",
              "  tensor(0.0957),\n",
              "  tensor(0.0956),\n",
              "  tensor(0.0955),\n",
              "  tensor(0.0954),\n",
              "  tensor(0.0953),\n",
              "  tensor(0.0952),\n",
              "  tensor(0.0951),\n",
              "  tensor(0.0949),\n",
              "  tensor(0.0948),\n",
              "  tensor(0.0947),\n",
              "  tensor(0.0946),\n",
              "  tensor(0.0945),\n",
              "  tensor(0.0944),\n",
              "  tensor(0.0943),\n",
              "  tensor(0.0942),\n",
              "  tensor(0.0941),\n",
              "  tensor(0.0940),\n",
              "  tensor(0.0938),\n",
              "  tensor(0.0937),\n",
              "  tensor(0.0936),\n",
              "  tensor(0.0935),\n",
              "  tensor(0.0934),\n",
              "  tensor(0.0933),\n",
              "  tensor(0.0932),\n",
              "  tensor(0.0931),\n",
              "  tensor(0.0930),\n",
              "  tensor(0.0929),\n",
              "  tensor(0.0927),\n",
              "  tensor(0.0926),\n",
              "  tensor(0.0925),\n",
              "  tensor(0.0924),\n",
              "  tensor(0.0923),\n",
              "  tensor(0.0922),\n",
              "  tensor(0.0921),\n",
              "  tensor(0.0920),\n",
              "  tensor(0.0919),\n",
              "  tensor(0.0918),\n",
              "  tensor(0.0916),\n",
              "  tensor(0.0915),\n",
              "  tensor(0.0914),\n",
              "  tensor(0.0913),\n",
              "  tensor(0.0912),\n",
              "  tensor(0.0911),\n",
              "  tensor(0.0910),\n",
              "  tensor(0.0909),\n",
              "  tensor(0.0908),\n",
              "  tensor(0.0907),\n",
              "  tensor(0.0905),\n",
              "  tensor(0.0904),\n",
              "  tensor(0.0903),\n",
              "  tensor(0.0902),\n",
              "  tensor(0.0901),\n",
              "  tensor(0.0900),\n",
              "  tensor(0.0899),\n",
              "  tensor(0.0898),\n",
              "  tensor(0.0897),\n",
              "  tensor(0.0896),\n",
              "  tensor(0.0894),\n",
              "  tensor(0.0893),\n",
              "  tensor(0.0892),\n",
              "  tensor(0.0891),\n",
              "  tensor(0.0890),\n",
              "  tensor(0.0889),\n",
              "  tensor(0.0888),\n",
              "  tensor(0.0887),\n",
              "  tensor(0.0886),\n",
              "  tensor(0.0884),\n",
              "  tensor(0.0883),\n",
              "  tensor(0.0882),\n",
              "  tensor(0.0881),\n",
              "  tensor(0.0880),\n",
              "  tensor(0.0879),\n",
              "  tensor(0.0878),\n",
              "  tensor(0.0877),\n",
              "  tensor(0.0876),\n",
              "  tensor(0.0875),\n",
              "  tensor(0.0873),\n",
              "  tensor(0.0872),\n",
              "  tensor(0.0871),\n",
              "  tensor(0.0870),\n",
              "  tensor(0.0869),\n",
              "  tensor(0.0868),\n",
              "  tensor(0.0867),\n",
              "  tensor(0.0866),\n",
              "  tensor(0.0865),\n",
              "  tensor(0.0864),\n",
              "  tensor(0.0863),\n",
              "  tensor(0.0862),\n",
              "  tensor(0.0862),\n",
              "  tensor(0.0861),\n",
              "  tensor(0.0860),\n",
              "  tensor(0.0859),\n",
              "  tensor(0.0858),\n",
              "  tensor(0.0857),\n",
              "  tensor(0.0857),\n",
              "  tensor(0.0856),\n",
              "  tensor(0.0855),\n",
              "  tensor(0.0854),\n",
              "  tensor(0.0853),\n",
              "  tensor(0.0853),\n",
              "  tensor(0.0852),\n",
              "  tensor(0.0851),\n",
              "  tensor(0.0850),\n",
              "  tensor(0.0849),\n",
              "  tensor(0.0849),\n",
              "  tensor(0.0848),\n",
              "  tensor(0.0847),\n",
              "  tensor(0.0846),\n",
              "  tensor(0.0845),\n",
              "  tensor(0.0845),\n",
              "  tensor(0.0844),\n",
              "  tensor(0.0843),\n",
              "  tensor(0.0842),\n",
              "  tensor(0.0841),\n",
              "  tensor(0.0841),\n",
              "  tensor(0.0840),\n",
              "  tensor(0.0839),\n",
              "  tensor(0.0838),\n",
              "  tensor(0.0837),\n",
              "  tensor(0.0837),\n",
              "  tensor(0.0836),\n",
              "  tensor(0.0835),\n",
              "  tensor(0.0834),\n",
              "  tensor(0.0833),\n",
              "  tensor(0.0833),\n",
              "  tensor(0.0832),\n",
              "  tensor(0.0831),\n",
              "  tensor(0.0830),\n",
              "  tensor(0.0829),\n",
              "  tensor(0.0829),\n",
              "  tensor(0.0828),\n",
              "  tensor(0.0827),\n",
              "  tensor(0.0826),\n",
              "  tensor(0.0825),\n",
              "  tensor(0.0825),\n",
              "  tensor(0.0824),\n",
              "  tensor(0.0823),\n",
              "  tensor(0.0822),\n",
              "  tensor(0.0821),\n",
              "  tensor(0.0821),\n",
              "  tensor(0.0820),\n",
              "  tensor(0.0819),\n",
              "  tensor(0.0818),\n",
              "  tensor(0.0817),\n",
              "  tensor(0.0817),\n",
              "  tensor(0.0816),\n",
              "  tensor(0.0815),\n",
              "  tensor(0.0814),\n",
              "  tensor(0.0813),\n",
              "  tensor(0.0812),\n",
              "  tensor(0.0812),\n",
              "  tensor(0.0811),\n",
              "  tensor(0.0810),\n",
              "  tensor(0.0809),\n",
              "  tensor(0.0808),\n",
              "  tensor(0.0808),\n",
              "  tensor(0.0807),\n",
              "  tensor(0.0806),\n",
              "  tensor(0.0805),\n",
              "  tensor(0.0804),\n",
              "  tensor(0.0804),\n",
              "  tensor(0.0803),\n",
              "  tensor(0.0802),\n",
              "  tensor(0.0801),\n",
              "  tensor(0.0800),\n",
              "  tensor(0.0800),\n",
              "  tensor(0.0799),\n",
              "  tensor(0.0798),\n",
              "  tensor(0.0797),\n",
              "  tensor(0.0796),\n",
              "  tensor(0.0796),\n",
              "  tensor(0.0795),\n",
              "  tensor(0.0794),\n",
              "  tensor(0.0793),\n",
              "  tensor(0.0792),\n",
              "  tensor(0.0792),\n",
              "  tensor(0.0791),\n",
              "  tensor(0.0790),\n",
              "  tensor(0.0789),\n",
              "  tensor(0.0788),\n",
              "  tensor(0.0788),\n",
              "  tensor(0.0787),\n",
              "  tensor(0.0786),\n",
              "  tensor(0.0785),\n",
              "  tensor(0.0784),\n",
              "  tensor(0.0784),\n",
              "  tensor(0.0783),\n",
              "  tensor(0.0782),\n",
              "  tensor(0.0781),\n",
              "  tensor(0.0780),\n",
              "  tensor(0.0780),\n",
              "  tensor(0.0779),\n",
              "  tensor(0.0778),\n",
              "  tensor(0.0777),\n",
              "  tensor(0.0776),\n",
              "  tensor(0.0776),\n",
              "  tensor(0.0775),\n",
              "  tensor(0.0774),\n",
              "  tensor(0.0773),\n",
              "  tensor(0.0772),\n",
              "  tensor(0.0772),\n",
              "  tensor(0.0771),\n",
              "  tensor(0.0770),\n",
              "  tensor(0.0769),\n",
              "  tensor(0.0768),\n",
              "  tensor(0.0767),\n",
              "  tensor(0.0767),\n",
              "  tensor(0.0766),\n",
              "  tensor(0.0765),\n",
              "  tensor(0.0764),\n",
              "  tensor(0.0763),\n",
              "  tensor(0.0763),\n",
              "  tensor(0.0762),\n",
              "  tensor(0.0761),\n",
              "  tensor(0.0760),\n",
              "  tensor(0.0759),\n",
              "  tensor(0.0759),\n",
              "  tensor(0.0758),\n",
              "  tensor(0.0757),\n",
              "  tensor(0.0756),\n",
              "  tensor(0.0755),\n",
              "  tensor(0.0755),\n",
              "  tensor(0.0754),\n",
              "  tensor(0.0753),\n",
              "  tensor(0.0752),\n",
              "  tensor(0.0751),\n",
              "  tensor(0.0751),\n",
              "  tensor(0.0750),\n",
              "  tensor(0.0749),\n",
              "  tensor(0.0748),\n",
              "  tensor(0.0747),\n",
              "  tensor(0.0747),\n",
              "  tensor(0.0746),\n",
              "  tensor(0.0745),\n",
              "  tensor(0.0744),\n",
              "  tensor(0.0743),\n",
              "  tensor(0.0743),\n",
              "  tensor(0.0742),\n",
              "  tensor(0.0741),\n",
              "  tensor(0.0740),\n",
              "  tensor(0.0739),\n",
              "  tensor(0.0739),\n",
              "  tensor(0.0738),\n",
              "  tensor(0.0737),\n",
              "  tensor(0.0736),\n",
              "  tensor(0.0735),\n",
              "  tensor(0.0735),\n",
              "  tensor(0.0734),\n",
              "  tensor(0.0733),\n",
              "  tensor(0.0732),\n",
              "  tensor(0.0731),\n",
              "  tensor(0.0731),\n",
              "  tensor(0.0730),\n",
              "  tensor(0.0729),\n",
              "  tensor(0.0728),\n",
              "  tensor(0.0727),\n",
              "  tensor(0.0726),\n",
              "  tensor(0.0726),\n",
              "  tensor(0.0725),\n",
              "  tensor(0.0724),\n",
              "  tensor(0.0723),\n",
              "  tensor(0.0722),\n",
              "  tensor(0.0722),\n",
              "  tensor(0.0721),\n",
              "  tensor(0.0720),\n",
              "  tensor(0.0719),\n",
              "  tensor(0.0718),\n",
              "  tensor(0.0718),\n",
              "  tensor(0.0717),\n",
              "  tensor(0.0716),\n",
              "  tensor(0.0715),\n",
              "  tensor(0.0714),\n",
              "  tensor(0.0714),\n",
              "  tensor(0.0713),\n",
              "  tensor(0.0712),\n",
              "  tensor(0.0711),\n",
              "  tensor(0.0710),\n",
              "  tensor(0.0710),\n",
              "  tensor(0.0709),\n",
              "  tensor(0.0708),\n",
              "  tensor(0.0707),\n",
              "  tensor(0.0706),\n",
              "  tensor(0.0706),\n",
              "  tensor(0.0705),\n",
              "  tensor(0.0704),\n",
              "  tensor(0.0703),\n",
              "  tensor(0.0702),\n",
              "  tensor(0.0702),\n",
              "  tensor(0.0701),\n",
              "  tensor(0.0700),\n",
              "  tensor(0.0699),\n",
              "  tensor(0.0698),\n",
              "  tensor(0.0698),\n",
              "  tensor(0.0697),\n",
              "  tensor(0.0696),\n",
              "  tensor(0.0695),\n",
              "  tensor(0.0694),\n",
              "  tensor(0.0694),\n",
              "  tensor(0.0693),\n",
              "  tensor(0.0692),\n",
              "  tensor(0.0691),\n",
              "  tensor(0.0690),\n",
              "  tensor(0.0690),\n",
              "  tensor(0.0689),\n",
              "  tensor(0.0688),\n",
              "  tensor(0.0687),\n",
              "  tensor(0.0686),\n",
              "  tensor(0.0686),\n",
              "  tensor(0.0685),\n",
              "  tensor(0.0684),\n",
              "  tensor(0.0683),\n",
              "  tensor(0.0682),\n",
              "  tensor(0.0682),\n",
              "  tensor(0.0681),\n",
              "  tensor(0.0680),\n",
              "  tensor(0.0679),\n",
              "  tensor(0.0678),\n",
              "  tensor(0.0678),\n",
              "  tensor(0.0677),\n",
              "  tensor(0.0676),\n",
              "  tensor(0.0675),\n",
              "  tensor(0.0674),\n",
              "  tensor(0.0673),\n",
              "  tensor(0.0673),\n",
              "  tensor(0.0672),\n",
              "  tensor(0.0671),\n",
              "  tensor(0.0670),\n",
              "  tensor(0.0669),\n",
              "  tensor(0.0669),\n",
              "  tensor(0.0668),\n",
              "  tensor(0.0667),\n",
              "  tensor(0.0666),\n",
              "  tensor(0.0665),\n",
              "  tensor(0.0665),\n",
              "  tensor(0.0664),\n",
              "  tensor(0.0663),\n",
              "  tensor(0.0662),\n",
              "  tensor(0.0661),\n",
              "  tensor(0.0661),\n",
              "  tensor(0.0660),\n",
              "  tensor(0.0659),\n",
              "  tensor(0.0658),\n",
              "  tensor(0.0657),\n",
              "  tensor(0.0657),\n",
              "  tensor(0.0656),\n",
              "  tensor(0.0655),\n",
              "  tensor(0.0654),\n",
              "  tensor(0.0653),\n",
              "  tensor(0.0653),\n",
              "  tensor(0.0652),\n",
              "  tensor(0.0651),\n",
              "  tensor(0.0650),\n",
              "  tensor(0.0649),\n",
              "  tensor(0.0649),\n",
              "  tensor(0.0648),\n",
              "  tensor(0.0647),\n",
              "  tensor(0.0646),\n",
              "  tensor(0.0645),\n",
              "  tensor(0.0645),\n",
              "  tensor(0.0644),\n",
              "  tensor(0.0643),\n",
              "  tensor(0.0642),\n",
              "  tensor(0.0641),\n",
              "  tensor(0.0641),\n",
              "  tensor(0.0640),\n",
              "  tensor(0.0639),\n",
              "  tensor(0.0638),\n",
              "  tensor(0.0637),\n",
              "  tensor(0.0637),\n",
              "  tensor(0.0636),\n",
              "  tensor(0.0635),\n",
              "  tensor(0.0634),\n",
              "  tensor(0.0633),\n",
              "  tensor(0.0633),\n",
              "  tensor(0.0632),\n",
              "  tensor(0.0631),\n",
              "  tensor(0.0630),\n",
              "  tensor(0.0629),\n",
              "  tensor(0.0629),\n",
              "  tensor(0.0628),\n",
              "  tensor(0.0627),\n",
              "  tensor(0.0626),\n",
              "  tensor(0.0625),\n",
              "  tensor(0.0624),\n",
              "  tensor(0.0624),\n",
              "  tensor(0.0623),\n",
              "  tensor(0.0622),\n",
              "  tensor(0.0621),\n",
              "  tensor(0.0620),\n",
              "  tensor(0.0620),\n",
              "  tensor(0.0619),\n",
              "  tensor(0.0618),\n",
              "  tensor(0.0617),\n",
              "  tensor(0.0616),\n",
              "  tensor(0.0616),\n",
              "  tensor(0.0615),\n",
              "  tensor(0.0614),\n",
              "  tensor(0.0613),\n",
              "  tensor(0.0612),\n",
              "  tensor(0.0612),\n",
              "  tensor(0.0611),\n",
              "  tensor(0.0610),\n",
              "  tensor(0.0609),\n",
              "  tensor(0.0608),\n",
              "  tensor(0.0608),\n",
              "  tensor(0.0607),\n",
              "  tensor(0.0606),\n",
              "  tensor(0.0605),\n",
              "  tensor(0.0604),\n",
              "  tensor(0.0604),\n",
              "  tensor(0.0603),\n",
              "  tensor(0.0602),\n",
              "  tensor(0.0601),\n",
              "  tensor(0.0600),\n",
              "  tensor(0.0600),\n",
              "  tensor(0.0599),\n",
              "  tensor(0.0598),\n",
              "  tensor(0.0597),\n",
              "  tensor(0.0596),\n",
              "  tensor(0.0596),\n",
              "  tensor(0.0595),\n",
              "  tensor(0.0594),\n",
              "  tensor(0.0593),\n",
              "  tensor(0.0592),\n",
              "  tensor(0.0592),\n",
              "  tensor(0.0591),\n",
              "  tensor(0.0590),\n",
              "  tensor(0.0589),\n",
              "  tensor(0.0588),\n",
              "  tensor(0.0588),\n",
              "  tensor(0.0587),\n",
              "  tensor(0.0586),\n",
              "  tensor(0.0585),\n",
              "  tensor(0.0584),\n",
              "  tensor(0.0583),\n",
              "  tensor(0.0583),\n",
              "  tensor(0.0582),\n",
              "  tensor(0.0581),\n",
              "  tensor(0.0580),\n",
              "  tensor(0.0579),\n",
              "  tensor(0.0579),\n",
              "  tensor(0.0578),\n",
              "  tensor(0.0577),\n",
              "  tensor(0.0576),\n",
              "  tensor(0.0575),\n",
              "  tensor(0.0575),\n",
              "  tensor(0.0574),\n",
              "  tensor(0.0573),\n",
              "  tensor(0.0572),\n",
              "  tensor(0.0571),\n",
              "  tensor(0.0571),\n",
              "  tensor(0.0570),\n",
              "  ...])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the loss curves ( remember to transform tensors to numpy, as mathplotlib dose not work with thensors)\n",
        "plt.plot(\n",
        "    epoch_count,\n",
        "    np.array(torch.tensor(loss_values).cpu().numpy()),\n",
        "    label=\"Train loss\"\n",
        ")\n",
        "plt.plot(epoch_count, test_loss_values, label=\"Test loss\")\n",
        "plt.title(\"Training and test loss curves\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.legend()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "wneCFHLCYKnV",
        "outputId": "f6cbe6a6-b0f6-47a9-ed9b-be5a0af41a93"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fe336191600>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHHCAYAAABDUnkqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABqjUlEQVR4nO3dd3hT9f4H8HeSNkn3ntABBdqyChRaykaqBbkIiIrIlXEVFcHxw8lVWVdFQZErKCheHDgYXgS9siuIDNllU4Z0AJ2U7p18f3+kSRtaSiltTpK+X8+Tp+nJSc7nNKV5811HJoQQICIiIrIScqkLICIiImpKDDdERERkVRhuiIiIyKow3BAREZFVYbghIiIiq8JwQ0RERFaF4YaIiIisCsMNERERWRWGGyIiIrIqDDdEzWzSpEkIDg5u1HPnzJkDmUzWtAWZmaSkJMhkMnz11VdSl9IoMpkMc+bMkboMIqqB4YZaLJlM1qDbrl27pC6VAJw5cwZz5sxBUlJSsx7n008/tdigRUQ6NlIXQCSVVatWGX3/zTffYPv27bW2h4eH39VxVqxYAa1W26jnvvnmm3j99dfv6vjW4syZM5g7dy4GDRrU6Jawhvj000/h6emJSZMmNdsxiKh5MdxQi/X3v//d6Ps///wT27dvr7X9ZsXFxbC3t2/wcWxtbRtVHwDY2NjAxob/TMm8aLValJeXQ61WS10KUZ3YLUVUj0GDBqFz5844cuQIBgwYAHt7e/zzn/8EAGzcuBHDhw+Hv78/VCoVQkJC8K9//QsajcboNW4ec6MfY/LBBx/g888/R0hICFQqFXr16oVDhw4ZPbeuMTcymQzTp0/Hhg0b0LlzZ6hUKnTq1AlbtmypVf+uXbvQs2dPqNVqhISE4LPPPmvwOJ4//vgDDz/8MAIDA6FSqRAQEID/+7//Q0lJSa3zc3R0xNWrVzFq1Cg4OjrCy8sLL7/8cq2fRW5uLiZNmgQXFxe4urpi4sSJyM3NvW0tX331FR5++GEAwODBg+vsMty8eTP69+8PBwcHODk5Yfjw4Th9+rTR66Snp2Py5Mlo3bo1VCoV/Pz8MHLkSENXV3BwME6fPo3ff//dcIxBgwbdtr6bHTt2DMOGDYOzszMcHR0xZMgQ/Pnnn0b7VFRUYO7cuWjfvj3UajU8PDzQr18/bN++vcH11ufcuXN45JFH4OXlBTs7O4SGhuKNN94wPH6rsWD1/c5999136NSpE1QqFX755Re4u7tj8uTJtV4jPz8farUaL7/8smFbWVkZZs+ejXbt2hl+n1599VWUlZUZPXf79u3o168fXF1d4ejoiNDQUMO/OaKG4n8JiW7j+vXrGDZsGB599FH8/e9/h4+PDwDdB66joyNmzJgBR0dH/Pbbb5g1axby8/OxcOHC277u999/j4KCAjz99NOQyWRYsGABHnzwQfz111+3be3Zs2cP1q9fj2effRZOTk74+OOPMWbMGKSkpMDDwwOA7gN26NCh8PPzw9y5c6HRaDBv3jx4eXk16LzXrVuH4uJiTJ06FR4eHjh48CCWLFmCK1euYN26dUb7ajQaxMXFITo6Gh988AF27NiBDz/8ECEhIZg6dSoAQAiBkSNHYs+ePXjmmWcQHh6On376CRMnTrxtLQMGDMDzzz+Pjz/+GP/85z8NXYX6r6tWrcLEiRMRFxeH999/H8XFxVi2bBn69euHY8eOGT7Ex4wZg9OnT+O5555DcHAwMjMzsX37dqSkpCA4OBiLFy/Gc889B0dHR0MQ0L/fDXX69Gn0798fzs7OePXVV2Fra4vPPvsMgwYNwu+//47o6GgAuhAxf/58PPnkk4iKikJ+fj4OHz6Mo0eP4t57721Qvbdy4sQJ9O/fH7a2tnjqqacQHByMS5cu4ZdffsE777xzR+ej99tvv2Ht2rWYPn06PD090b59e4wePRrr16/HZ599BqVSadh3w4YNKCsrw6OPPgpA19LzwAMPYM+ePXjqqacQHh6OkydP4qOPPsL58+exYcMGw8/ub3/7G7p27Yp58+ZBpVLh4sWL2Lt3b6NqphZMEJEQQohp06aJm/9JDBw4UAAQy5cvr7V/cXFxrW1PP/20sLe3F6WlpYZtEydOFEFBQYbvL1++LAAIDw8PkZOTY9i+ceNGAUD88ssvhm2zZ8+uVRMAoVQqxcWLFw3bjh8/LgCIJUuWGLaNGDFC2Nvbi6tXrxq2XbhwQdjY2NR6zbrUdX7z588XMplMJCcnG50fADFv3jyjfbt37y4iIyMN32/YsEEAEAsWLDBsq6ysFP379xcAxJdffllvPevWrRMAxM6dO422FxQUCFdXVzFlyhSj7enp6cLFxcWw/caNGwKAWLhwYb3H6dSpkxg4cGC9+9QEQMyePdvw/ahRo4RSqRSXLl0ybLt27ZpwcnISAwYMMGyLiIgQw4cPv+XrNrTeugwYMEA4OTkZvU9CCKHVag33b/691LvV75xcLhenT5822r5169Zav7NCCHH//feLtm3bGr5ftWqVkMvl4o8//jDab/ny5QKA2Lt3rxBCiI8++kgAEFlZWQ0/WaI6sFuK6DZUKlWdTe92dnaG+wUFBcjOzkb//v1RXFyMc+fO3fZ1x44dCzc3N8P3/fv3BwD89ddft31ubGwsQkJCDN937doVzs7OhudqNBrs2LEDo0aNgr+/v2G/du3aYdiwYbd9fcD4/IqKipCdnY0+ffpACIFjx47V2v+ZZ54x+r5///5G57Jp0ybY2NgYWnIAQKFQ4LnnnmtQPbeyfft25ObmYty4ccjOzjbcFAoFoqOjsXPnTsP5KJVK7Nq1Czdu3LirY96KRqPBtm3bMGrUKLRt29aw3c/PD4899hj27NmD/Px8AICrqytOnz6NCxcu1Plaja03KysLu3fvxj/+8Q8EBgYaPXY3ywoMHDgQHTt2NNp2zz33wNPTE2vWrDFsu3HjBrZv346xY8catq1btw7h4eEICwszeo/uueceADC8R66urgB0Xb6NHYRPBHDMDdFttWrVyqjJXe/06dMYPXo0XFxc4OzsDC8vL8Ng5Ly8vNu+7s0fPPqg05APspufq3++/rmZmZkoKSlBu3btau1X17a6pKSkYNKkSXB3dzeMoxk4cCCA2uenVqtrdXfVrAcAkpOT4efnB0dHR6P9QkNDG1TPrejDwT333AMvLy+j27Zt25CZmQlAF1Lff/99bN68GT4+PhgwYAAWLFiA9PT0uzp+TVlZWSguLq7znMLDw6HVapGamgoAmDdvHnJzc9GhQwd06dIFr7zyCk6cOGHYv7H16gNl586dm+y8AKBNmza1ttnY2GDMmDHYuHGjYezM+vXrUVFRYRRuLly4gNOnT9d6fzp06AAAhvdo7Nix6Nu3L5588kn4+Pjg0Ucfxdq1axl06I5xzA3RbdRswdDLzc3FwIED4ezsjHnz5iEkJARqtRpHjx7Fa6+91qA/xgqFos7tQohmfW5DaDQa3HvvvcjJycFrr72GsLAwODg44OrVq5g0aVKt87tVPaagr2XVqlXw9fWt9XjN2WYvvvgiRowYgQ0bNmDr1q146623MH/+fPz222/o3r27yWoGdOOILl26hI0bN2Lbtm344osv8NFHH2H58uV48sknm73eW7Xi3DwIXK+ufwcA8Oijj+Kzzz7D5s2bMWrUKKxduxZhYWGIiIgw7KPVatGlSxcsWrSoztcICAgwHGP37t3YuXMnfv31V2zZsgVr1qzBPffcg23btkn6e0aWheGGqBF27dqF69evY/369RgwYIBh++XLlyWsqpq3tzfUajUuXrxY67G6tt3s5MmTOH/+PL7++mtMmDDBsL3mTJ47FRQUhPj4eBQWFhq13iQmJjbo+bf6MNZ3z3l7eyM2Nva2rxMSEoKXXnoJL730Ei5cuIBu3brhww8/xLffflvvcRrCy8sL9vb2dZ7TuXPnIJfLDR/kAAyzjSZPnozCwkIMGDAAc+bMMYSbhtR7M3132KlTp+qt1c3Nrc6ZasnJyQ05VYMBAwbAz88Pa9asQb9+/fDbb78ZzcrSn8Px48cxZMiQ2/585XI5hgwZgiFDhmDRokV499138cYbb2Dnzp0Nen+JAHZLETWK/n+QNVtKysvL8emnn0pVkhGFQoHY2Fhs2LAB165dM2y/ePEiNm/e3KDnA8bnJ4TAv//970bXdP/996OyshLLli0zbNNoNFiyZEmDnu/g4AAAtT6Q4+Li4OzsjHfffRcVFRW1npeVlQVAtz5RaWmp0WMhISFwcnIymo7s4ODQoOnpdVEoFLjvvvuwceNGo+naGRkZ+P7779GvXz84OzsD0M3Cq8nR0RHt2rUz1NLQem/m5eWFAQMGYOXKlUhJSTF6rOb7GRISgry8PKOusLS0NPz00093dM5yuRwPPfQQfvnlF6xatQqVlZVGXVIA8Mgjj+Dq1atYsWJFreeXlJSgqKgIAJCTk1Pr8W7dugFAvedMdDO23BA1Qp8+feDm5oaJEyfi+eefh0wmw6pVq5qsW6gpzJkzB9u2bUPfvn0xdepUaDQaLF26FJ07d0ZCQkK9zw0LC0NISAhefvllXL16Fc7Ozvjvf/97VwNxR4wYgb59++L1119HUlISOnbsiPXr1zdofBKg+5BTKBR4//33kZeXB5VKhXvuuQfe3t5YtmwZHn/8cfTo0QOPPvoovLy8kJKSgl9//RV9+/bF0qVLcf78eQwZMgSPPPIIOnbsCBsbG/z000/IyMgwTFkGgMjISCxbtgxvv/022rVrB29vb8PA14Z4++23DWu1PPvss7CxscFnn32GsrIyLFiwwLBfx44dMWjQIERGRsLd3R2HDx/Gjz/+iOnTpwNAg+uty8cff4x+/fqhR48eeOqpp9CmTRskJSXh119/Nbz3jz76KF577TWMHj0azz//vGH6fIcOHXD06NEGny+gGyuzZMkSzJ49G126dKm1qvfjjz+OtWvX4plnnsHOnTvRt29faDQanDt3DmvXrsXWrVvRs2dPzJs3D7t378bw4cMRFBSEzMxMfPrpp2jdujX69et3RzVRCyfdRC0i83KrqeCdOnWqc/+9e/eK3r17Czs7O+Hv7y9effVVw9TYmtOVbzUVvK4pvrhpWvGtpuVOmzat1nODgoLExIkTjbbFx8eL7t27C6VSKUJCQsQXX3whXnrpJaFWq2/xU6h25swZERsbKxwdHYWnp6eYMmWKYcp5zWnbEydOFA4ODrWeX1ft169fF48//rhwdnYWLi4u4vHHHxfHjh1r0FRwIYRYsWKFaNu2rVAoFLV+zjt37hRxcXHCxcVFqNVqERISIiZNmiQOHz4shBAiOztbTJs2TYSFhQkHBwfh4uIioqOjxdq1a42OkZ6eLoYPHy6cnJwEgNtOC7/5PRNCiKNHj4q4uDjh6Ogo7O3txeDBg8W+ffuM9nn77bdFVFSUcHV1FXZ2diIsLEy88847ory8/I7qvZVTp06J0aNHC1dXV6FWq0VoaKh46623jPbZtm2b6Ny5s1AqlSI0NFR8++23d/Q7p6fVakVAQIAAIN5+++069ykvLxfvv/++6NSpk1CpVMLNzU1ERkaKuXPniry8PCGE7vd15MiRwt/fXyiVSuHv7y/GjRsnzp8/36BzJtKTCWFG/9UkomY3atSoeqcgExFZOo65IbJiN18q4cKFC9i0aVOjLilARGQp2HJDZMX8/PwwadIktG3bFsnJyVi2bBnKyspw7NgxtG/fXuryiIiaBQcUE1mxoUOH4ocffkB6ejpUKhViYmLw7rvvMtgQkVVjyw0RERFZFY65ISIiIqvCcENERERWpcWNudFqtbh27RqcnJzuapl1IiIiMh0hBAoKCuDv7w+5vP62mRYXbq5du2Z0bRciIiKyHKmpqWjdunW9+7S4cOPk5ARA98PRX+OFiIiIzFt+fj4CAgIMn+P1aXHhRt8V5ezszHBDRERkYRoypIQDiomIiMiqMNwQERGRVWG4ISIiIqvS4sbcEBGRddNoNKioqJC6DGoEpVJ522neDcFwQ0REVkEIgfT0dOTm5kpdCjWSXC5HmzZtoFQq7+p1GG6IiMgq6IONt7c37O3tuVCrhdEvspuWlobAwMC7ev/MItx88sknWLhwIdLT0xEREYElS5YgKiqqzn2/+uorTJ482WibSqVCaWmpKUolIiIzpNFoDMHGw8ND6nKokby8vHDt2jVUVlbC1ta20a8j+YDiNWvWYMaMGZg9ezaOHj2KiIgIxMXFITMz85bPcXZ2RlpamuGWnJxswoqJiMjc6MfY2NvbS1wJ3Q19d5RGo7mr15E83CxatAhTpkzB5MmT0bFjRyxfvhz29vZYuXLlLZ8jk8ng6+truPn4+JiwYiIiMlfsirJsTfX+SRpuysvLceTIEcTGxhq2yeVyxMbGYv/+/bd8XmFhIYKCghAQEICRI0fi9OnTt9y3rKwM+fn5RjciIiKyXpKGm+zsbGg0mlotLz4+PkhPT6/zOaGhoVi5ciU2btyIb7/9FlqtFn369MGVK1fq3H/+/PlwcXEx3HjRTCIismbBwcFYvHix5K8hJcm7pe5UTEwMJkyYgG7dumHgwIFYv349vLy88Nlnn9W5/8yZM5GXl2e4paammrhiIiKi2mQyWb23OXPmNOp1Dx06hKeeeqppi7Uwks6W8vT0hEKhQEZGhtH2jIwM+Pr6Nug1bG1t0b17d1y8eLHOx1UqFVQq1V3X2iBF2UBhJuDT0TTHIyIii5WWlma4v2bNGsyaNQuJiYmGbY6Ojob7QghoNBrY2Nz+Y9vLy6tpC7VAkrbcKJVKREZGIj4+3rBNq9UiPj4eMTExDXoNjUaDkydPws/Pr7nKbJhzm4CFIcDGZ6Wtg4iILELNiTEuLi5Gk2XOnTsHJycnbN68GZGRkVCpVNizZw8uXbqEkSNHwsfHB46OjujVqxd27Nhh9Lo3dynJZDJ88cUXGD16NOzt7dG+fXv8/PPPd1RrSkoKRo4cCUdHRzg7O+ORRx4xapg4fvw4Bg8eDCcnJzg7OyMyMhKHDx8GACQnJ2PEiBFwc3ODg4MDOnXqhE2bNjX+B9cAkq9zM2PGDEycOBE9e/ZEVFQUFi9ejKKiIsNaNhMmTECrVq0wf/58AMC8efPQu3dvtGvXDrm5uVi4cCGSk5Px5JNPSnkagF+E7mvacaA0H1A7S1sPEVELJoRAScXdTSduLDtbRZPN+nn99dfxwQcfoG3btnBzc0Nqairuv/9+vPPOO1CpVPjmm28wYsQIJCYmIjAw8JavM3fuXCxYsAALFy7EkiVLMH78eCQnJ8Pd3f22NWi1WkOw+f3331FZWYlp06Zh7Nix2LVrFwBg/Pjx6N69O5YtWwaFQoGEhATDOjXTpk1DeXk5du/eDQcHB5w5c8aoVao5SB5uxo4di6ysLMyaNQvp6eno1q0btmzZYhhknJKSYnSdiRs3bmDKlClIT0+Hm5sbIiMjsW/fPnTsKHFXkEsrwC0YuJEEpB4A2t8rbT1ERC1YSYUGHWdtleTYZ+bFwV7ZNB+v8+bNw733Vn+euLu7IyIiwvD9v/71L/z000/4+eefMX369Fu+zqRJkzBu3DgAwLvvvouPP/4YBw8exNChQ29bQ3x8PE6ePInLly8bJuV888036NSpEw4dOoRevXohJSUFr7zyCsLCwgAA7du3Nzw/JSUFY8aMQZcuXQAAbdu2vYOfQONIHm4AYPr06bd8U/SpUO+jjz7CRx99ZIKqGiGony7cJP3BcENERHetZ8+eRt8XFhZizpw5+PXXX5GWlobKykqUlJQgJSWl3tfp2rWr4b6DgwOcnZ3rXSy3prNnzyIgIMBotnHHjh3h6uqKs2fPolevXpgxYwaefPJJrFq1CrGxsXj44YcREhICAHj++ecxdepUbNu2DbGxsRgzZoxRPc3BLMKN1QjuCyR8CyTtlboSIqIWzc5WgTPz4iQ7dlNxcHAw+v7ll1/G9u3b8cEHH6Bdu3aws7PDQw89hPLy8npf5+ZLGchkMmi12iarc86cOXjsscfw66+/YvPmzZg9ezZWr16N0aNH48knn0RcXBx+/fVXbNu2DfPnz8eHH36I5557rsmOfzOGm6YU1Ff39doxoKwQUDVvnyIREdVNJpM1WdeQOdm7dy8mTZqE0aNHA9C15CQlJTXrMcPDw5GamorU1FRD682ZM2eQm5trNCSkQ4cO6NChA/7v//4P48aNw5dffmmoMyAgAM888wyeeeYZzJw5EytWrGjWcGNx69yYNbcgwCUQEBrduBsiIqIm1L59e6xfvx4JCQk4fvw4HnvssSZtgalLbGwsunTpgvHjx+Po0aM4ePAgJkyYgIEDB6Jnz54oKSnB9OnTsWvXLiQnJ2Pv3r04dOgQwsPDAQAvvvgitm7disuXL+Po0aPYuXOn4bHmwnDT1IKrWm+S9khbBxERWZ1FixbBzc0Nffr0wYgRIxAXF4cePXo06zFlMhk2btwINzc3DBgwALGxsWjbti3WrFkDAFAoFLh+/TomTJiADh064JFHHsGwYcMwd+5cALolW6ZNm4bw8HAMHToUHTp0wKefftq8NQshRLMewczk5+fDxcUFeXl5cHZuhunaR1cBP08HAqKBJ7Y1/esTEVEtpaWluHz5Mtq0aQO1Wi11OdRI9b2Pd/L5zZabphbcT/f16lGgvFjaWoiIiFoghpum5hYMOLcCtBXAlYNSV0NERNTiMNw0NZmsetYUp4QTERGZHMNNc+CgYiIiIskw3DSHIP24m8NARYm0tRAREbUwDDfNwSMEcPQFNOXAlcNSV0NERNSiMNw0B5msumsqmeNuiIiITInhprkEcdwNERGRFBhumot+vZsrh4DKMmlrISIiakEYbpqLZwfAwQuoLAWuHpG6GiIiIiNJSUmQyWRISEiQupQmx3DTXLjeDRER1UMmk9V7mzNnzl299oYNG5qsVktjfdeDNyfB/YAzG4DkPQBekboaIiIyI2lpaYb7a9aswaxZs5CYmGjY5ujoKEVZVoEtN81J33KTcgCoLJe2FiIiMiu+vr6Gm4uLC2QymdG21atXIzw8HGq1GmFhYUZX0i4vL8f06dPh5+cHtVqNoKAgzJ8/HwAQHBwMABg9ejRkMpnh+4b4/fffERUVBZVKBT8/P7z++uuorKw0PP7jjz+iS5cusLOzg4eHB2JjY1FUVAQA2LVrF6KiouDg4ABXV1f07dsXycnJd/+DagS23DQnrzDAzh0oyQGuHQMCo6WuiIioZRACqJDo4sW29rqhCXfhu+++w6xZs7B06VJ0794dx44dw5QpU+Dg4ICJEyfi448/xs8//4y1a9ciMDAQqampSE1NBQAcOnQI3t7e+PLLLzF06FAoFIoGHfPq1au4//77MWnSJHzzzTc4d+4cpkyZArVajTlz5iAtLQ3jxo3DggULMHr0aBQUFOCPP/6AEAKVlZUYNWoUpkyZgh9++AHl5eU4ePAgZHf5c2gshpvmJJfr1rs5+4uua4rhhojINCqKgXf9pTn2P68BSoe7eonZs2fjww8/xIMPPggAaNOmDc6cOYPPPvsMEydOREpKCtq3b49+/fpBJpMhKCjI8FwvLy8AgKurK3x9fRt8zE8//RQBAQFYunQpZDIZwsLCcO3aNbz22muYNWsW0tLSUFlZiQcffNBwvC5dugAAcnJykJeXh7/97W8ICQkBAISHh9/Vz+BusFuquekvxcBBxURE1ABFRUW4dOkSnnjiCTg6Ohpub7/9Ni5dugQAmDRpEhISEhAaGornn38e27Ztu+vjnj17FjExMUatLX379kVhYSGuXLmCiIgIDBkyBF26dMHDDz+MFStW4MaNGwAAd3d3TJo0CXFxcRgxYgT+/e9/G40pMjW23DQ3/UrFqQcATQWgsJW2HiKilsDWXteCItWx70JhYSEAYMWKFYiONm7x13cx9ejRA5cvX8bmzZuxY8cOPPLII4iNjcWPP/54V8euj0KhwPbt27Fv3z5s27YNS5YswRtvvIEDBw6gTZs2+PLLL/H8889jy5YtWLNmDd58801s374dvXv3braaboUtN83NuxOgdgXKC4G041JXQ0TUMshkuq4hKW53Oc7Ex8cH/v7++Ouvv9CuXTujW5s2bQz7OTs7Y+zYsVixYgXWrFmD//73v8jJyQEA2NraQqPR3NFxw8PDsX//fgghDNv27t0LJycntG7duurHKkPfvn0xd+5cHDt2DEqlEj/99JNh/+7du2PmzJnYt28fOnfujO+///5ufhSNxpab5iaX62ZNJf6quxRD655SV0RERGZu7ty5eP755+Hi4oKhQ4eirKwMhw8fxo0bNzBjxgwsWrQIfn5+6N69O+RyOdatWwdfX1+4uroC0M2Yio+PR9++faFSqeDm5nbbYz777LNYvHgxnnvuOUyfPh2JiYmYPXs2ZsyYAblcjgMHDiA+Ph733XcfvL29ceDAAWRlZSE8PByXL1/G559/jgceeAD+/v5ITEzEhQsXMGHChGb+SdWN4cYUgqvCTfJeoN+LUldDRERm7sknn4S9vT0WLlyIV155BQ4ODujSpQtefPFFAICTkxMWLFiACxcuQKFQoFevXti0aRPkcl2HzIcffogZM2ZgxYoVaNWqFZKSkm57zFatWmHTpk145ZVXEBERAXd3dzzxxBN48803Aehainbv3o3FixcjPz8fQUFB+PDDDzFs2DBkZGTg3Llz+Prrr3H9+nX4+flh2rRpePrpp5vrR1QvmajZ/tQC5Ofnw8XFBXl5eXB2djbNQa8lAJ8PBFTOwKuXAQUzJRFRUyotLcXly5fRpk0bqNVqqcuhRqrvfbyTz2+OuTEF3y6AygUoywfST0hdDRERkVVjuDEFuQIIrBotnswp4URERM2J4cZUgrneDRERkSkw3JiKfr2blH2A9s6m5xEREVHDMdyYim8EoHQCSvOAjNNSV0NEZJVa2BwZq9NU7x/DjakobKqvLZW0R9paiIisjK2tbvX34mKJLpZJTaK8vBwAGnyxz1vhnGRTCu4HXNyhG1Qc86zU1RARWQ2FQgFXV1dkZmYCAOzt7SW7IjU1jlarRVZWFuzt7WFjc3fxhOHGlPQX0UzeC2i1utWLiYioSeivgK0POGR55HI5AgMD7zqYMtyYkn83wNYBKLkBZJ0FfDpJXRERkdWQyWTw8/ODt7c3KioqpC6HGkGpVBpWWb4bDDempLAFAqKAv3bqxt0w3BARNTmFQnHXYzbIsrFfxNT0U8I5qJiIiKhZMNyYWnB/3dfkfQCnLBIRETU5hhtT8+8B2NgBxdlAVqLU1RAREVkdhhtTs1ECAb1095PZNUVERNTUGG6koJ8SznE3RERETY7hRgo1L6LJcTdERERNiuFGCq0iAYUKKMoErl+UuhoiIiKrwnAjBVs10Lpq3A27poiIiJoUw41UuN4NERFRs2C4kUpQVbhJ5rgbIiKipsRwI5XWvQCFEihIA3L+kroaIiIiq8FwIxWlvW5gMaBrvSEiIqImwXAjJX3XVBLDDRERUVNhuJFSzUHFHHdDRETUJBhupBQQDchtgPwrQG6y1NUQERFZBYYbKSkddBfSBNg1RURE1EQYbqQWXGNKOBEREd01hhupGS6i+Ye0dRAREVkJhhupBUYDMgWQmwLkpkpdDRERkcVjuJGaygnw76a7z64pIiKiu8ZwYw6CeJ0pIiKipmIW4eaTTz5BcHAw1Go1oqOjcfDgwQY9b/Xq1ZDJZBg1alTzFtjcgqvG3bDlhoiI6K5JHm7WrFmDGTNmYPbs2Th69CgiIiIQFxeHzMzMep+XlJSEl19+Gf379zdRpc0osDcgk+uuMZV/TepqiIiILJrk4WbRokWYMmUKJk+ejI4dO2L58uWwt7fHypUrb/kcjUaD8ePHY+7cuWjbtq0Jq20mahfAt4vuPte7ISIiuiuShpvy8nIcOXIEsbGxhm1yuRyxsbHYv3//LZ83b948eHt744knnrjtMcrKypCfn290M0vBVS1QyRx3Q0REdDckDTfZ2dnQaDTw8fEx2u7j44P09PQ6n7Nnzx785z//wYoVKxp0jPnz58PFxcVwCwgIuOu6mwUvoklERNQkJO+WuhMFBQV4/PHHsWLFCnh6ejboOTNnzkReXp7hlppqpmvJBMUAkAHXLwAFdQc7IiIiuj0bKQ/u6ekJhUKBjIwMo+0ZGRnw9fWttf+lS5eQlJSEESNGGLZptVoAgI2NDRITExESEmL0HJVKBZVK1QzVNzE7N8CnM5BxUjdrqvMYqSsiIiKySJK23CiVSkRGRiI+Pt6wTavVIj4+HjExMbX2DwsLw8mTJ5GQkGC4PfDAAxg8eDASEhLMt8upofRTwtk1RURE1GiSttwAwIwZMzBx4kT07NkTUVFRWLx4MYqKijB58mQAwIQJE9CqVSvMnz8farUanTt3Nnq+q6srANTabpGC+wIHlnG9GyIiorsgebgZO3YssrKyMGvWLKSnp6Nbt27YsmWLYZBxSkoK5HKLGhrUeIF9dF+zzgGFWYCjl7T1EBERWSCZEEJIXYQp5efnw8XFBXl5eXB2dpa6nNo+jQEyzwAPfw10GiV1NURERGbhTj6/W0iTiAXRTwln1xQREVGjMNyYGw4qJiIiuisMN+ZG33KTeRoozpG2FiIiIgvEcGNuHL0Az1Dd/eR90tZCRERkgRhuzFGw/lIMvM4UERHRnWK4MUf6cTe8iCYREdEdY7gxR0FV4Sb9FFByQ9paiIiILAzDjTly8gE82gEQQMqfUldDRERkURhuzFUQx90QERE1BsONuTKsd8NwQ0REdCcYbsyVvuUm/QRQmidtLURERBaE4cZcubQC3NoAQgukHJC6GiIiIovBcGPO9OvdcEo4ERFRgzHcmLMgjrshIiK6Uww35kzfcnMtASgrkLQUIiIiS8FwY85cA3U3oQFSOe6GiIioIRhuzJ2ha2qvtHUQERFZCIYbc8eLaBIREd0Rhhtzp1/v5tpRoLxI2lqIiIgsAMONuXMLBpxbA9pKIPWg1NUQERGZPYYbcyeT1VjvhuNuiIiIbofhxhIYLqLJcENERHQ7DDeWQH8RzauHgYoSaWshIiIycww3lsC9LeDkB2jKgSuHpK6GiIjIrDHcWAKZjF1TREREDcRwYyk4qJiIiKhBGG4shX6l4tSDQEWptLUQERGZMYYbS+HZHnDwBjRlwNUjUldDRERkthhuLAXXuyEiImoQhhtLEsTrTBEREd0Ow40lCa4x7qayXNpaiIiIzBTDjSXxCgPsPYDKEt2FNImIiKgWhhtLYrTeDbumiIiI6sJwY2n0XVMcVExERFQnhhtLo2+5STkAaCqkrYWIiMgMMdxYGu+OgJ0bUFEEXEuQuhoiIiKzw3BjaeRyILCP7n4yx90QERHdjOHGEunH3fAimkRERLUw3Fgi/UrFKX8CmkppayEiIjIzDDeWyKczoHIByguA9BNSV0NERGRWGG4skVwBBMXo7nO9GyIiIiMMN5YqiBfRJCIiqgvDjaUyLOa3H9BqpK2FiIjIjDDcWCrfroDSCSjLAzJOSV0NERGR2WC4sVQKGyCwt+4+x90QEREZMNxYMv2UcK53Q0REZMBwY8mC++u+puwDtFppayEiIjITDDeWzC8CsHUASm4AmWekroaIiMgsMNxYMoUtEBitu88p4URERAAYbiyffr2bpD+krYOIiMhMMNxYOsN6N/sAIaSthYiIyAww3DSRE1dy8Y+vDuHF1cdMe2D/HoCNHVB8Hcg6Z9pjExERmSGGmyYigwy/ncvEjrOZKK804cwlGyUQEKW7z/VuiIiIGG6aSid/Z3g6qlBYVonDyTmmPbiha4qDiomIiBhumohcLsOgUC8AwK7ELNMe3DCoeA/H3RARUYvHcNOEBod6AwB2nss07YFbRQI2aqAoC8i+YNpjExERmRmGmybUr70nFHIZLmQWIjWn2HQHtlUDrXvp7idz3A0REbVsZhFuPvnkEwQHB0OtViM6OhoHDx685b7r169Hz5494erqCgcHB3Tr1g2rVq0yYbW35mJni8hANwDArvNSdU1x3A0REbVskoebNWvWYMaMGZg9ezaOHj2KiIgIxMXFITOz7q4dd3d3vPHGG9i/fz9OnDiByZMnY/Lkydi6dauJK6/boLCqcTem7poK5rgbIiIiwAzCzaJFizBlyhRMnjwZHTt2xPLly2Fvb4+VK1fWuf+gQYMwevRohIeHIyQkBC+88AK6du2KPXvMoztGP+5m76VslFZoTHfg1r0AhRIoTAdy/jLdcYmIiMyMpOGmvLwcR44cQWxsrGGbXC5HbGws9u/ff9vnCyEQHx+PxMREDBgwoM59ysrKkJ+fb3RrTmG+TvB1VqO0QosDl004JdzWDmjVU3ef690QEVELJmm4yc7OhkajgY+Pj9F2Hx8fpKen3/J5eXl5cHR0hFKpxPDhw7FkyRLce++9de47f/58uLi4GG4BAQFNeg43k8lkGFzVNWXyWVP6rimud0NERC2Y5N1SjeHk5ISEhAQcOnQI77zzDmbMmIFdu3bVue/MmTORl5dnuKWmpjZ7fYOquqZ2JZo43NQcVMxxN0RE1ELZSHlwT09PKBQKZGRkGG3PyMiAr6/vLZ8nl8vRrl07AEC3bt1w9uxZzJ8/H4MGDaq1r0qlgkqlatK6b6dvO0/YKmRIul6My9lFaOPpYJoDB0QBchsg/wpwIwlwb2Oa4xIREZkRSVtulEolIiMjER8fb9im1WoRHx+PmJiYBr+OVqtFWVlZc5TYKI4qG0S1cQdg4q4ppYNuQT+AXVNERNRiSd4tNWPGDKxYsQJff/01zp49i6lTp6KoqAiTJ08GAEyYMAEzZ8407D9//nxs374df/31F86ePYsPP/wQq1atwt///nepTqFOhtWKpeyaIiIiaoEk7ZYCgLFjxyIrKwuzZs1Ceno6unXrhi1bthgGGaekpEAur85gRUVFePbZZ3HlyhXY2dkhLCwM3377LcaOHSvVKdRpUKg33v71LA78lYPi8krYK030ow7uC+xZxJWKiYioxZIJ0bJGnubn58PFxQV5eXlwdnZutuMIITBg4U6k5pTgiwk9EdvR5/ZPagplBcB7QYDQAC+eBFwDTXNcIiKiZnQnn9+Sd0tZK5lMJk3XlMoJ8O+mu8+uKSIiaoEYbprRYMOU8CyYtIEsuJ/uK7umiIioBWK4aUa923pAZSPH1dwSXMgsNN2Bg6rCDVtuiIioBWK4aUZ2SgViQjwAmHhKeGBvQCYHblwG8q6a7rhERERmgOGmmUky7kbtDPh21d3nejdERNTCMNw0M324OZx0AwWlFaY7sH7cDS+iSURELQzDTTML9LBHWy8HVGoF9l7MNt2BDYOK2XJDREQtC8ONCRi6ps5lme6ggTEAZMD1i0DBra+wTkREZG0Ybkyg5rgbk00Jt3MFfDvr7rNrioiIWhCGGxPo1cYN9koFMgvKcCYt33QHDmLXFBERtTwMNyagslGgT4gnAN2CfiYTzPVuiIio5WG4MZHBYV4ATLzeTVAf3dfsRKDQhKGKiIhIQgw3JjKoatzN0ZQbyC0uN81B7d0B7066++yaIiKiFoLhxkRaudoh1McJWgHsvmDKKeF9dV85qJiIiFoIhhsTGlTVNbXLlF1TXO+GiIhaGIYbEzJcJfx8FrRaE00JD6pquck8AxRdN80xiYiIJMRwY0KRQW5wUtkgp6gcJ67mmeagDp6AV5jufso+0xyTiIhIQo0KN6mpqbhy5Yrh+4MHD+LFF1/E559/3mSFWSNbhRz9O+imhJt21hTH3RARUcvRqHDz2GOPYefOnQCA9PR03HvvvTh48CDeeOMNzJs3r0kLtDb6WVO7THmVcMOgYo67ISIi69eocHPq1ClERUUBANauXYvOnTtj3759+O677/DVV181ZX1WZ1AH3aDi41fykFVQZpqD6lcqzjgFlNwwzTGJiIgk0qhwU1FRAZVKBQDYsWMHHnjgAQBAWFgY0tLSmq46K+TtrEbnVs4AgN3nTbSwnpMP4NEegACS95vmmERERBJpVLjp1KkTli9fjj/++APbt2/H0KFDAQDXrl2Dh4dHkxZojWpeSNNk9F1TnBJORERWrlHh5v3338dnn32GQYMGYdy4cYiIiAAA/Pzzz4buKro1/bib3eezUKnRmuag+q6ppD9MczwiIiKJ2DTmSYMGDUJ2djby8/Ph5uZm2P7UU0/B3t6+yYqzVt0CXOFqb4vc4gocS81Fr2D35j+ovuUm/SRQmgeoXZr/mERERBJoVMtNSUkJysrKDMEmOTkZixcvRmJiIry9vZu0QGukkMswsIOJL6Tp7A+4twWEFkj50zTHJCIikkCjws3IkSPxzTffAAByc3MRHR2NDz/8EKNGjcKyZcuatEBrVT3uxoRX69ZfiuGv3013TCIiIhNrVLg5evQo+vfvDwD48ccf4ePjg+TkZHzzzTf4+OOPm7RAazWggxdkMuBsWj7S80pNc9C2g3VfL8Wb5nhEREQSaFS4KS4uhpOTEwBg27ZtePDBByGXy9G7d28kJyc3aYHWyt1BiW4BrgCA38+bqGuq7SBAJgeyzgF5V01zTCIiIhNrVLhp164dNmzYgNTUVGzduhX33XcfACAzMxPOzs5NWqA1M3RNnTNR15S9O+DfQ3f/0m+mOSYREZGJNSrczJo1Cy+//DKCg4MRFRWFmJgYALpWnO7duzdpgdZMH272XMxGeaWJpoSH3KP7yq4pIiKyUo0KNw899BBSUlJw+PBhbN261bB9yJAh+Oijj5qsOGvXyd8Zno5KFJZV4nByjmkO2m6I7uulnYBWY5pjEhERmVCjwg0A+Pr6onv37rh27ZrhCuFRUVEICwtrsuKsnVwuw8AO+gtpmqhrqlVPQOUClOYC146Z5phEREQm1Khwo9VqMW/ePLi4uCAoKAhBQUFwdXXFv/71L2i1JupesRKDw0y83o3CBmg7QHf/IrumiIjI+jQq3LzxxhtYunQp3nvvPRw7dgzHjh3Du+++iyVLluCtt95q6hqtWv92XlDIZbiQWYjUnGLTHDRE3zXFcENERNanUZdf+Prrr/HFF18YrgYOAF27dkWrVq3w7LPP4p133mmyAq2di70tIgPdcDApB7vOZ+Hx3kHNf1D9oOIrh4GSXMDOtfmPSUREZCKNarnJycmpc2xNWFgYcnJMNDDWigyq6praZaquKbcgwKMdIDTAZa5WTERE1qVR4SYiIgJLly6ttX3p0qXo2rXrXRfV0uinhO+9lI3SChPNYGp3r+7r+a3170dERGRhGtUttWDBAgwfPhw7duwwrHGzf/9+pKamYtOmTU1aYEsQ5usEX2c10vNLceByjuGims0qdChwYJku3Gg1gFzR/MckIiIygUa13AwcOBDnz5/H6NGjkZubi9zcXDz44IM4ffo0Vq1a1dQ1Wj2ZTGb6WVNBfXVTwouzgatHTHNMIiIiE5AJIURTvdjx48fRo0cPaDTmuzhcfn4+XFxckJeXZ1aXith6Oh1PrzqCYA977HplsGkOum4ycHo90G8GEDvbNMckIiJqhDv5/G70In7UtPq284StQoak68W4nF1kmoOGDtN9Pb/FNMcjIiIyAYYbM+GoskFUG3cAJuyaahcLyBRA5hngRpJpjklERNTMGG7MiOEq4YkmCjf27kCgbkA4Etl6Q0RE1uGOZks9+OCD9T6em5t7N7W0eINCvfH2r2dx4K8cFJdXwl7ZqMlsdyZ0KJC8Bzi/Gej9TPMfj4iIqJndUcuNi4tLvbegoCBMmDChuWq1eiFeDghwt0O5Rot9F6+b5qAdqsbdJO0FSvNMc0wiIqJmdEdNA19++WVz1UGomhIe6o1v9idjZ2ImYjv6NP9BPdvpViu+fhG4sB3o8lDzH5OIiKgZccyNmdGPu9mVmIUmnKVfv/ARuq9nNprmeERERM2I4cbM9G7rAZWNHFdzS3Axs9A0B+04Svf1wnag3ETT0ImIiJoJw42ZsVMqEBPiAcCEs6b8IgC3YKCyhNeaIiIii8dwY4YMU8LPZZnmgDJZdevNmQ2mOSYREVEzYbgxQ/pwcygpBwWlFaY5aKdRuq/nt7FrioiILBrDjRkK9LBHW08HVGoF9l7MNs1B/boBrkG6rqkL20xzTCIiombAcGOmBknRNaVvvTm9wTTHJCIiagYMN2ZqcJgXAN2gYpNNCTfMmtoGlBeb5phERERNjOHGTEW1cYedrQKZBWU4k5ZvmoP6dwdcA4GKYnZNERGRxWK4MVMqGwX6tvMEoFvQzyRqzpo6vd40xyQiImpiDDdmzNA1dc5E690AQOcxuq+JW4CSG6Y7LhERURMxi3DzySefIDg4GGq1GtHR0Th48OAt912xYgX69+8PNzc3uLm5ITY2tt79LZl+UPHRlBvILS43zUH9IgDvjoCmDDj9k2mOSURE1IQkDzdr1qzBjBkzMHv2bBw9ehQRERGIi4tDZmbdrRW7du3CuHHjsHPnTuzfvx8BAQG47777cPXqVRNX3vxaudoh1McJWgHsvmCiKeEyGdDtMd39hO9Nc0wiIqImJHm4WbRoEaZMmYLJkyejY8eOWL58Oezt7bFy5co69//uu+/w7LPPolu3bggLC8MXX3wBrVaL+Ph4E1duGoOquqZ2mbJrqssjgEwBXDkEZF8w3XGJiIiagKThpry8HEeOHEFsbKxhm1wuR2xsLPbv39+g1yguLkZFRQXc3d3rfLysrAz5+flGN0tiuEr4+SxotSaaEu7kA7Qbort//AfTHJOIiKiJSBpusrOzodFo4OPjY7Tdx8cH6enpDXqN1157Df7+/kYBqab58+fDxcXFcAsICLjruk0pMsgNTiob5BSV48TVPNMdOGKc7uvx1YBWY7rjEhER3SXJu6XuxnvvvYfVq1fjp59+glqtrnOfmTNnIi8vz3BLTU01cZV3x1YhR/8OuinhJp01FXo/oHYB8q8Cl3eb7rhERER3SdJw4+npCYVCgYyMDKPtGRkZ8PX1rfe5H3zwAd577z1s27YNXbt2veV+KpUKzs7ORjdLo581tSvRhOHGVl09LZxdU0REZEEkDTdKpRKRkZFGg4H1g4NjYmJu+bwFCxbgX//6F7Zs2YKePXuaolRJDeqgG1R8/EoesgrKTHfgiKpZU2d/AcoKTHdcIiKiuyB5t9SMGTOwYsUKfP311zh79iymTp2KoqIiTJ48GQAwYcIEzJw507D/+++/j7feegsrV65EcHAw0tPTkZ6ejsLCQqlOodl5O6vRuZWuxWn3eROtVgwArXsCHu11l2M49V/THZeIiOguSB5uxo4diw8++ACzZs1Ct27dkJCQgC1bthgGGaekpCAtLc2w/7Jly1BeXo6HHnoIfn5+htsHH3wg1SmYhH7W1E5Tdk3JZECPCbr7h/4DmOoCnkRERHdBJkx2yWnzkJ+fDxcXF+Tl5VnU+JsjyTcwZtk+OKttcPSte2GjMFEuLc4BPgzTrVj8ZLyuNYeIiMjE7uTzW/KWG2qYbgGucLW3RX5pJRJSc013YHt3oPODuvuHvjDdcYmIiBqJ4cZCKOQyDKwaWGzSrikA6PmE7uup9bqWHCIiIjPGcGNBDONuzplwUDGg64ry7arrmkr4zrTHJiIiukMMNxZkQAcvyGTAmbR8pOeVmu7AMhnQq6r15uAKQFNpumMTERHdIYYbC+LuoEREa1cAwO/nTdw11eURwM4dyE0Gzv5s2mMTERHdAYYbCyNZ15TSHoh6Snd/7785LZyIiMwWw42FGRymG1S852I2yiu1pj141BTAxg5ISwCS/jDtsYmIiBqI4cbCdPZ3gaejEoVllTicbOKZSw6eQPfxuvt7/23aYxMRETUQw42FkctlGNhBfyFNE3dNAUDMNEAmBy7uANJPmf74REREt8FwY4H0XVM7z5l4UDEAuLcFwh/Q3d+3xPTHJyIiug2GGwvUv50XFHIZLmQWIjWn2PQF9H1e9/XUj0DeFdMfn4iIqB4MNxbIxd4WkYFuAIBdprxKuF6rSCC4P6Ct5NgbIiIyOww3FmpQVdfULim6pgBgwCu6r0e+YusNERGZFYYbC6Vf72bvpWyUVmhMX0CbAUBQP0BTDvyxyPTHJyIiugWGGwsV5usEX2c1Siu0OHBZgotZymTA4Jm6+0e/AXJTTF8DERFRHRhuLJRMJpN21hQABPfTteBoK4Df35emBiIiopsw3FiwQaH69W4kCjcAcM9buq8J3wMZZ6Srg4iIqArDjQXr284TtgoZkq4X43J2kTRFBETp1r0RWmD7LGlqICIiqoHhxoI5qmwQ1cYdgIRdUwAQOweQ2wAXtwOXdkpXBxERERhuLJ7hKuFSdk15hAC9ntTd3/YWoJVg9hYREVEVhhsLpx93c+CvHBSXV0pXyIBXAZULkHESOPatdHUQEVGLx3Bj4UK8HBDgbodyjRb7L12XrhAHD2Dgq7r7O2YDRRLWQkRELRrDjYWTyWTm0TUFANFPA96dgJIbwA4OLiYiImkw3FgBQ7g5lwUhhHSFKGyBv1WtVnzsWyDlT+lqISKiFovhxgr0busBlY0cV3NLcDGzUNpiAnsD3R/X3f/f/wGaCmnrISKiFofhxgrYKRXo3dYDgBl0TQHAvfMAO3cg8wywZ7HU1RARUQvDcGMlBofqL8WQJXElAOzdgWFVl2P4/T0g7bi09RARUYvCcGMl9FPCDyXloKDUDLqCujwMhI8AtJXAT88AlWVSV0RERC0Ew42VCPZ0QFtPB1RqBfZezJa6HN1Vw/+2GLD31HVP7XxX6oqIiKiFYLixIoNqzJoyCw6ewIh/6+7v+xhI3i9tPURE1CIw3FiRwWFV424SM6WdEl5T+N+AiHG6C2v+OBkoNIMBz0REZNUYbqxIVBt32NkqkFlQhjNp+VKXU+3+DwDPUKAgDfjxH4BGwstEEBGR1WO4sSIqGwX6tvMEAOxKNJOuKQBQOQJjVwFKRyDpD+C3eVJXREREVozhxsoYuqbOmVn3j1coMHKp7v7efwNnf5G2HiIisloMN1ZGP6j4aMoN5BaXS1zNTTqNBnpP093/aSqQeVbaeoiIyCox3FiZVq52CPVxglYAuy+YwZTwm907FwjqC5QXAN+OAfKuSF0RERFZGYYbKzSoqmtql7l1TQG6i2uO/VY3wDj/KrDqQaA4R+qqiIjIijDcWCH9VcJ3nc+CVmsmU8JrsncHHl8POPkD2YnA92OB8mKpqyIiIivBcGOFIoPc4KSyQU5ROU5czZO6nLq5tNYFHLULcOUgsG4SryBORERNguHGCtkq5OjfQTcl3OxmTdXkHQ48thawUQMXtgIbpjLgEBHRXWO4sVL6WVO7Es043ABAYG/g4a8AmQI4uQ5Y83d2URER0V1huLFSgzroBhUfv5KH7EIzvyJ36DDg0e91LTjntwCrRgMlN6SuioiILBTDjZXydlajcytnAMDu82a0WvGthA4FHt+gG4OT+ifw5f1AfprUVRERkQViuLFi+llTO83pUgz1CYoBJm8GHH2BzDPAf+4Dsi9KXRUREVkYhhsrph93s/t8Fio1WomraSCfTsAT2wD3ECAvBVgZB1w5LHVVRERkQRhurFi3AFe42tsir6QCCam5UpfTcG5BwD+2An4RQHG2LuDsWQxoLSSgERGRpBhurJhCLsOA9lUX0jT3WVM3c/QCJv4P6DgK0FYCO2YDq0ZyHA4REd0Ww42Vq75KuIWMu6lJ7aybJv7AUsDWHri8G1jWBzi3SerKiIjIjDHcWLkB7b0gkwFn0vKRnlcqdTl3TiYDejwOPL1b101VkgOsHgf8+hJQUSJ1dUREZIYYbqych6MKEa1dAQC/n7ewrqmaPNsDT2wHYqbrvj/0BfD5YCDjtLR1ERGR2WG4aQEMU8ItsWuqJhsVEPcO8Pf1gIM3kHVWF3B2vguU5ktdHRERmQmGmxZAP+5mz8VslFdawYyjdkOAqfuA9vcBmjLg9/eBf0cA+5awq4qIiBhuWoLO/i7wdFSisKwSh5NzpC6naTh66S66+fDXgEd73VicbW8CH/cAjnzFC3ASEbVgDDctgFwuw8AO+gtpWnjXVE0yGdBpFPDsn7oZVc6tgYJrwC8vAJ9EAyd/5No4REQtEMNNC6Hvmtp2Oh1CCImraWIKG92MquePAkPfA+w9gZxLwH+fAD4fAJzfBljbORMR0S0x3LQQg0O9obaVI+l6MU5ezZO6nOZhowJ6TwVeSAAGvwmonIH0k8D3DwNfDgOS90tdIRERmQDDTQvhoLLBvR19AQAbE65JXE0zUzkBA18BXjgO9HkesFEDKfuBL4cC3z4EXD3KlhwiIismebj55JNPEBwcDLVajejoaBw8ePCW+54+fRpjxoxBcHAwZDIZFi9ebLpCrcDICH8AwC/Hr0GjbQEf7vbuwH3/Ap4/BkROBuQ2wMXtwIrBwCdRwG9v61p2GHSIiKyKpOFmzZo1mDFjBmbPno2jR48iIiICcXFxyMyse7G54uJitG3bFu+99x58fX1NXK3lG9DBCy52tsgsKMOBv65LXY7pOPsDIxYD0w4CXccCCiWQfR7YvRBY3g9YEgnsmAtcS2DQISKyAjIh4ejS6Oho9OrVC0uXLgUAaLVaBAQE4LnnnsPrr79e73ODg4Px4osv4sUXX7yjY+bn58PFxQV5eXlwdnZubOkWa+b6k/jhYAoe6dkaCx6KkLocaZTmA+e3Amc2ABd3AJU1LkvhFgx0HKm7+ffQzcgiIiLJ3cnnt2QtN+Xl5Thy5AhiY2Ori5HLERsbi/37m27gZ1lZGfLz841uLdnIbrquqU0n01FUVilxNRJROwNdHwYe/Q545SLw0EpdmLGxA24kAXv/Day4B1jcFdj6BpB6iFPKiYgsiGThJjs7GxqNBj4+PkbbfXx8kJ6e3mTHmT9/PlxcXAy3gICAJnttSxTdxh1tPB1QWFaJX45b+cDihlA5AZ3HAI98A7x6SbcoYKcHAVsHIC8F2L8U+E8ssLgzsPl13YwrBh0iIrMm+YDi5jZz5kzk5eUZbqmpqVKXJCmZTIZxUbqA9/3BFImrMTNKB92igA9/qQs6Y78FujwMKJ2A/KvAgWW6GVeLwoFNrwBJewCtRuqqiYjoJjZSHdjT0xMKhQIZGRlG2zMyMpp0sLBKpYJKpWqy17MGY3q0xgdbz+PElTycupqHzq1cpC7J/NjaAeEjdLeKUuCvncDpDUDiZqAwHTj4ue7m4K3bp+NIIKivbkFBIiKSlGQtN0qlEpGRkYiPjzds02q1iI+PR0xMjFRltQgejirEddYFSLbeNICtGggdBjz4GfDKBeCxdUC3vwNqV6AoEzj8H+CbB4APQ3WXfrj0G69tRUQkIUm7pWbMmIEVK1bg66+/xtmzZzF16lQUFRVh8uTJAIAJEyZg5syZhv3Ly8uRkJCAhIQElJeX4+rVq0hISMDFixelOgWLpe+a2nDsKnKLyyWuxoLYqIAO9wGjPgFevgD8/b9AjwmAnTtQnK27aOeq0cAH7YGN04ALO4BK/nyJiExJ0qngALB06VIsXLgQ6enp6NatGz7++GNER0cDAAYNGoTg4GB89dVXAICkpCS0adOm1msMHDgQu3btatDxWvpUcD0hBIb9+w+cSy/AK3GhmDa4ndQlWTZNJZD0B3BmI3D2F13Q0VO7AKHDdV1XIYN1AYmIiO7InXx+Sx5uTI3hptr6o1cwY+1xeDup8Mdrg6GyUUhdknXQaoDkvbqgc+ZnXdeVnspZ18XVcSQQMkTX5UVERLfFcFMPhptq5ZVa9F/wGzLyy7BgTFc80qtlT5NvFloNkHpANxj57M9AQVr1Y0pHoMNQXdBpFwso7SUrk4jI3DHc1IPhxtiK3X/hnU1nEeBuh99eGgRbhdWvDiAdrRa4cqiqRWcjkH+l+jFbe6D9fbpuq6C+gEc7ro5MRFQDw009GG6MFZdXYsCCncguLGfrjSlptcC1o7pLQJzeqFswsCYHLyCojy7oBPUBvDsBcgZPImq5GG7qwXBTm771prWbHeJfGsixN6YmBHDtmG4NneR9utYdTZnxPmoXIDCmOvD4RQAKW2nqJSKSAMNNPRhuaisp12Dgwp3ILCjDG/eHY8qAtlKX1LJVlgFXj+oGJSfv043ZKS803sfWHgiIqm7ZaRWpW3iQiMhKMdzUg+GmbmsPpeLV/56Ak9oGv78yGO4OSqlLIj1NJZB+AkjZrws7yXuBkhvG+yiUuoAT1Ed3C4jWXTeLiMhKMNzUg+GmbhqtwIgle3AmLR+P9w7Cv0Z1lrokuhWtFshOrG7ZSdqruyRETTK5rutK37ITGAPYu0tTLxFRE2C4qQfDza3tu5SNx1YcgEwGrJ/aB90D3aQuiRpCCODGZV3ISdmvCz03kmrv5xVe3bIT1Bdw9jN5qUREjcVwUw+Gm/rNWJOA9ceuIszXCT9P7welDWfoWKS8q9VBJ3kfkHWu9j5ubapbdoL6AG7BnH5ORGaL4aYeDDf1yykqR+yi35FTVI4XY9vjxdgOUpdETaEo23jMTvpJQGiN93HyN27Z8Qpl2CEis8FwUw+Gm9vbmHAVL6xOgEIuw5qneqNnMMdqWJ3SPCD1YFXLzn7g6hFAe9OVzO09qqafV7Xu+HYB5FwmgIikwXBTD4abhvm/NQn46dhVtHK1w6bn+8PFnmuqWLXyYl3A0bfspB4EKkuM91E6AYG9gaCqwOPfnRcBJSKTYbipB8NNwxSUVuBvS/Yg+XoxBnTwwsqJPWHDSzO0HJXlQNrx6jE7KX8CZXnG+9iogda9qruyWvcClA7S1EtEVo/hph4MNw136moeHl6+HyUVGkzuG4zZIzpJXRJJRasBMk5Xt+wk7wOKs433kdvoWnP0Y3YCogE7V0nKJSLrw3BTD4abO7P5ZBqmfncUADB7REdM7ttG4orILAgBZF+oDjrJe4H8qzftJAN8O9eYkdUXcPCUpFwisnwMN/VguLlzS3+7gA+2nQcAvD+mC8b2CpS4IjI7QgC5KcYtOzmXau/n2cH4gqAurU1fKxFZJIabejDc3DkhBN759Sy+2HMZMhkwf3QXPBrFgEO3UZBeFXaqAk/mmdr7uAYat+y4t+X0cyKqE8NNPRhuGkcIgbc2nsK3f6YAAF4Y0h4vxraHjB9E1FDFOcZr7aQdr73WjqOPccuOVzgg50B2ImK4qRfDTeMJIfDhtvNYuvMiAOChyNZ4e1RnqG259gk1QlmB7orn+tadq0cATbnxPmrXGgsL9gF8IwCFjSTlEpG0GG7qwXBz9747kIy3NpyCVgBhvk5Y+lh3tPPmFajpLlWU1l5rp6LIeB+lIxAQVd26498DsFVLUy8RmRTDTT0YbprG3ovZeGF1ArILy2Bnq8DLcaGYGBPEtXCo6WgqgLQTNdba2adbWbkmhQpo3bPGWjtRgMpRmnqJqFkx3NSD4abpZBaU4v/WJGDvxesAgM6tnPH2qC7oFuAqbWFknbRa3aDkmjOyijKN95EpAP9u1S07gb0BO17dnsgaMNzUg+GmaWm1AqsPpeK9zWeRX1oJAIjr5IMXhnRAR3/+fKkZCQFcv1RjrZ19QF7KTTvJAJ9O1S07gX0AJx9JyiWiu8NwUw+Gm+aRVVCG+ZvP4qdjV6H/jYrr5IOJfYIR09aDs6rINHJTdBcC1Qee6xdq7+PRznhGliuXNSCyBAw39WC4aV4XMgrw8W8X8b8T1wwhp42nAx7tFYDRPVrB24mDP8mECjONp5+nnwJw0588l4AaM7L66sIPwziR2WG4qQfDjWmczyjA1/uSsDHhGgrLdN1VMhkQGeiG+zr54N6OvmjjyYsskomV5Oqmnyft0YWea8cAbaXxPg5eQGBMdcuOTydAzuUOiKTGcFMPhhvTKiqrxC/Hr2H1oVQkpOYaPdbG0wG9gt0Q1cYDUcHuCHC3Y/cVmVZ5EXDlUPWYnSuHgMpS431ULrqByYbp590Aha0k5RK1ZAw39WC4kU5aXgm2n8nAttMZ+POv66jUGv/q+Tqr0S3AFeF+zgj3c0K4nzNauzHwkAlVlulacwzTzw8A5QXG+9jaA617VbfstO4J2NpJUy9RC8JwUw+GG/OQV1KBI8k5OHA5B4cu5+Dk1TxUaGr/KjqpbBDm54RQXycEezjobp72aO1mz5WRqflpKoGMkzWukbUPKMkx3kduC7SKrG7ZCYgC1PzbQtTUGG7qwXBjnkrKNUhIzcXpa3k4k5aPs2kFuJhZUGfgAXTjd/yc1QiqCjtBHg4IcreHn6sd/F3U8HRUQS5niw81Ma0WyD4PJO+pnpVVkGa8j0wO+HatbtkJjAEcPKSpl8iKMNzUg+HGcpRXavFXdiHOpuXjfEYhUq4XIzmnCEnZxYZByrdiq5DB10UNPxdd2NGHHn9XO902VzVc7GzZ5UV3RwjgRpLx1c9vXK69n1e48TWynP1NXiqRpWO4qQfDjeUTQiCnqBxJ14uRfL3I8DUlpxhpuaXILCiFtgG/1Xa2Cvi5qtHK1Q5+VUHI10UNX2c1fJzV8HFWwd1ByQBEdyb/mnE3VtbZ2vu4talu2QnqA7gFc/o50W0w3NSD4cb6VWq0yCgoQ1puCa7lleq+6u/nlSAttxTXi8pv/0IAlAo5vJ1V8HHWhR5vZxV8ndXwdVHD20ltCEN2So7/oVsoun7TWjsnAKE13sfJHwiKqR634xkKyHmdNqKaGG7qwXBDAFBaoUF6Xml16Kn6mpFfivQ8XetPdmHDAhAAOKltbgo9qqrWn+qWIE9HJS8sSkBpvu6K5/oZWVePANoK433s3I27sXy6AAobaeolMhMMN/VguKGGKq/UIrOgFBn5ZcjIrwo++aXIyKvelp5fiuJyTYNeTy4DvJyqQ49PVSuQIQS56L46q23YFdaSVJQAVw5Xt+ykHgQqS4z3UToBgdE11trpDtiopKmXSCIMN/VguKGmJIRAYVllVfgpQ3qeLvBkVgWf9PwyZOaXIrOgDJqGDASCbiyQj7PKKPR4O6mMxgN5O6ugsmFXmFWqLAfSjtdYa+dPoCzPeB8bddVaO1UtO617AUqu+E3WjeGmHgw3JAWNVuB6URky8sp0rT/51V1gGQVlyKgKRXklFbd/sSruDkqj0ONt6AJTGUKRu72SU+ItnVYDZJ6pbtlJ3gcUZRnvI7fRteYY1tqJBuxcJSmXqLkw3NSD4YbMWWmFproVyNAFVlojEOm2l1dqb/9i0E2J93ZSG7UE6YKP8ZggBxXHc1gMIYDrF3VBJ6kq7ORfuWknGeDbucZaO30ARy9JyiVqKgw39WC4IUsnhEBeSYWu2yuvFJlVgadmd1hGfhmyC8vQ0H/dTiob3UwwFzV8nNTwcandCuTpqIItB0Sbp9wU45ad6xdr7+PZobplJ6gP4NLa9HUS3QWGm3ow3FBLUaHRIqug7JZdYPqWoNstiKgnkwGejiqj0GOYDeZSPUCaiyOagYIMIKXGWjsZp2rv4xpYY62dvoB7W661Q2aN4aYeDDdExgwDovNKkVFQivQ849lhmVUzw26+0OmtqGzkda8LVLVNv53XBjOh4hwg9UB1y861BEDcNMvP0ce4ZccrnGvtkFlhuKkHww3RndNqBa4XlRtPic8vqxGIdNtvFDd8QLSrvW2NLrDa6wL5uKjg4aCCggOim15ZQY21dvYDVw8DmpvWdVK7Gq+14xvBtXZIUgw39WC4IWo+pRUaZBVUzwjTLYhYe4p8aUXDBkQr5DJ4Oap03V5VM8N8nKunxusDEdcGuksVpcC1ozWmnx8AKoqM97F1qF5rJ7CP7krotmpp6qUWieGmHgw3RNISQiC/pNKoxceoNajq+6yCsgZdIwwA1LZyo9lg+iB08/R4doU1kKZCd5mImtfIKs013kehBFr1rG7ZCYgCVE6SlEstA8NNPRhuiCxDpUaL60Xl1QGoajC00f2CUuTeQVeYi53tTdPiVYY1gvRdYrxMRh20Wt0FQGvOyCrMMN5HpgD8IqrH7QT2BuzdpamXrBLDTT0YboisS2mFRjfo+aaWoJsvm9HQrrBbzQq7eYaYq30LnhUmBJDzV3XQSd6rm45+M+9OxuN2nHxNXytZDYabejDcELU8QggUlFUargumHxOUeVN32J1cJkN/xfial8Soea0wfRhqMQsk5qbWuPr5PiA7sfY+7iHGM7JcAzn9nBqM4aYeDDdEdCs3zwrTB6GaISgzvxTXi+7givFVCyRWT4+vnh3mXTVN3stRBaWNlXWFFWbVCDt7gfSTAG76uHFuVaNlp69uoUGGHboFhpt6MNwQ0d0qq9TUWCCxrNaaQPr7DV0gEQA8HJTV44Bc1FWXzdBdKsPbyQquFVaSW2P6+T7d7CztTT8fe0/jlh2fToCcg8BJh+GmHgw3RGQq1VeMrz0OSH8V+cyCUlRoGvZn2EYug7eTfmp8VddX1X3fqlWivZ3VcFJZwNT48mLgyqHqlp0rh4DKUuN9VC66gcn6wOMXAdgopamXJMdwUw+GGyIyJ0II3CiuqLo8RlUXWJ5ugLR+RtidXivMXqmocz0gnxrdY15OZrZKdGU5cO1YjbV2/gTKC4z3sbUHWveqbtlp3ROwtZOmXjI5hpt6MNwQkSWq0GiRXVhWNSOsDJmG2WE175civ7ThXWFu9rbVY39qjgOqMU3ew1GiVaK1Gt04nZrTz0tyjPeR2wKtelS37AREA2r+XbdWDDf1YLghImtWXF5Za+xP9eyw6vtllQ2bGi+XAV5ONdcDUtUYGF3dItTsF0wVAshKrDH9fB9QcM14H5kc8O1S3bIT2Adw8Gi+msikGG7qwXBDRC2dfpXo9BrrAGXeNDtM3yLU0FWi9RdMvXk9oJozxXyc1bBTNlFXmBDAjSTjlp0bl2vv5xVmPEjZ2b9pjk8mx3BTD4YbIqKG0WgFrheW1bo0hi4QlVWFoDu7YKqz2sb4Uhk3rxjtooanowq2jVklOv9adatOyn4g80ztfdyCq4NOUB/ArQ2nn1sIhpt6MNwQETUt/QVT67pGWM2ZYSUVmga9nkwGeDio4Ouigo/TTdcIqzFTzN1BWX9XWHGO8Vo7accBcVN3nJNfVRdWjC70eIUBcitbc8hKMNzUg+GGiMj09KtEZ9YIO7rZYdX3M6quIl95B6tEezmpjNYG0k+J93FS64KQsxqO+lWiS/OBKwerW3euHgE0Ny3IaOemG6ujb9nx7QooWsgq02bO4sLNJ598goULFyI9PR0RERFYsmQJoqKibrn/unXr8NZbbyEpKQnt27fH+++/j/vvv79Bx2K4ISIyX1qtQE5xuWENoPQ8/aUxjGeHZRc2fJVoR/0q0VXhR3/f3xFoU3YWvjeOwinjEORXDgIVxcZPVjrqZmHpx+206gHYqJr4rKkhLCrcrFmzBhMmTMDy5csRHR2NxYsXY926dUhMTIS3t3et/fft24cBAwZg/vz5+Nvf/obvv/8e77//Po4ePYrOnTvf9ngMN0RElq+8UousqqnxmTePA6oKQpn5ZSi4g1Wive3l6ONwFTGKc+iiOY22xSeg1hQa7SMUKsha96oKOzFA6yhA5djUp0d1sKhwEx0djV69emHp0qUAAK1Wi4CAADz33HN4/fXXa+0/duxYFBUV4X//+59hW+/evdGtWzcsX778tsdjuCEiajmKDKtEG18l/ubp8uWa2lPj5dAiVJaKKPk5RMnPIkp+Dl6yfKN9NFAg2ykMud5RqGzdG3Z+4VDY2prq9MyWrVIN/4C2Tfqad/L5LWlHYnl5OY4cOYKZM2catsnlcsTGxmL//v11Pmf//v2YMWOG0ba4uDhs2LChzv3LyspQVlZm+D4/P7/O/YiIyPo4qGzQ1ssRbb1u3boihEBucYVharxxGPLF0fyu2JRfiuzCUgQj3RB2ouXn0FqWDZ+C0/ApOA1c+tKEZ2beztmEw//NPyU7vqThJjs7GxqNBj4+PkbbfXx8cO7cuTqfk56eXuf+6enpde4/f/58zJ07t2kKJiIiqyOTyeDmoISbgxLhfrduEajUaJFdWG4IQTvzS1GcdRlOGYfhl3sEbUpOwUubBdnNVz9vgTQKacclWf0Q8JkzZxq19OTn5yMgIEDCioiIyBLZKOTwddENSq4WDGCwRBWZr04SH1/ScOPp6QmFQoGMjAyj7RkZGfD19a3zOb6+vne0v0qlgkrFke1EREQthaQrFSmVSkRGRiI+Pt6wTavVIj4+HjExMXU+JyYmxmh/ANi+ffst9yciIqKWRfJuqRkzZmDixIno2bMnoqKisHjxYhQVFWHy5MkAgAkTJqBVq1aYP38+AOCFF17AwIED8eGHH2L48OFYvXo1Dh8+jM8//1zK0yAiIiIzIXm4GTt2LLKysjBr1iykp6ejW7du2LJli2HQcEpKCuQ1lsLu06cPvv/+e7z55pv45z//ifbt22PDhg0NWuOGiIiIrJ/k69yYGte5ISIisjx38vnNq4MRERGRVWG4ISIiIqvCcENERERWheGGiIiIrArDDREREVkVhhsiIiKyKgw3REREZFUYboiIiMiqMNwQERGRVZH88gumpl+QOT8/X+JKiIiIqKH0n9sNubBCiws3BQUFAICAgACJKyEiIqI7VVBQABcXl3r3aXHXltJqtbh27RqcnJwgk8ma9LXz8/MREBCA1NRUq7xulbWfH2D958jzs3zWfo7Wfn6A9Z9jc52fEAIFBQXw9/c3uqB2XVpcy41cLkfr1q2b9RjOzs5W+QurZ+3nB1j/OfL8LJ+1n6O1nx9g/efYHOd3uxYbPQ4oJiIiIqvCcENERERWheGmCalUKsyePRsqlUrqUpqFtZ8fYP3nyPOzfNZ+jtZ+foD1n6M5nF+LG1BMRERE1o0tN0RERGRVGG6IiIjIqjDcEBERkVVhuCEiIiKrwnDTRD755BMEBwdDrVYjOjoaBw8elLqkOs2fPx+9evWCk5MTvL29MWrUKCQmJhrtM2jQIMhkMqPbM888Y7RPSkoKhg8fDnt7e3h7e+OVV15BZWWl0T67du1Cjx49oFKp0K5dO3z11VfNfXqYM2dOrdrDwsIMj5eWlmLatGnw8PCAo6MjxowZg4yMDIs4N73g4OBa5yiTyTBt2jQAlvf+7d69GyNGjIC/vz9kMhk2bNhg9LgQArNmzYKfnx/s7OwQGxuLCxcuGO2Tk5OD8ePHw9nZGa6urnjiiSdQWFhotM+JEyfQv39/qNVqBAQEYMGCBbVqWbduHcLCwqBWq9GlSxds2rSpWc+voqICr732Grp06QIHBwf4+/tjwoQJuHbtmtFr1PWev/fee2Zxfrc7RwCYNGlSrfqHDh1qtI+lvocA6vz3KJPJsHDhQsM+5vweNuRzwZR/O5vk81TQXVu9erVQKpVi5cqV4vTp02LKlCnC1dVVZGRkSF1aLXFxceLLL78Up06dEgkJCeL+++8XgYGBorCw0LDPwIEDxZQpU0RaWprhlpeXZ3i8srJSdO7cWcTGxopjx46JTZs2CU9PTzFz5kzDPn/99Zewt7cXM2bMEGfOnBFLliwRCoVCbNmypVnPb/bs2aJTp05GtWdlZRkef+aZZ0RAQICIj48Xhw8fFr179xZ9+vSxiHPTy8zMNDq/7du3CwBi586dQgjLe/82bdok3njjDbF+/XoBQPz0009Gj7/33nvCxcVFbNiwQRw/flw88MADok2bNqKkpMSwz9ChQ0VERIT4888/xR9//CHatWsnxo0bZ3g8Ly9P+Pj4iPHjx4tTp06JH374QdjZ2YnPPvvMsM/evXuFQqEQCxYsEGfOnBFvvvmmsLW1FSdPnmy288vNzRWxsbFizZo14ty5c2L//v0iKipKREZGGr1GUFCQmDdvntF7WvPfrJTnd7tzFEKIiRMniqFDhxrVn5OTY7SPpb6HQgij80pLSxMrV64UMplMXLp0ybCPOb+HDflcMNXfzqb6PGW4aQJRUVFi2rRphu81Go3w9/cX8+fPl7CqhsnMzBQAxO+//27YNnDgQPHCCy/c8jmbNm0ScrlcpKenG7YtW7ZMODs7i7KyMiGEEK+++qro1KmT0fPGjh0r4uLimvYEbjJ79mwRERFR52O5ubnC1tZWrFu3zrDt7NmzAoDYv3+/EMK8z+1WXnjhBRESEiK0Wq0QwrLfv5s/OLRarfD19RULFy40bMvNzRUqlUr88MMPQgghzpw5IwCIQ4cOGfbZvHmzkMlk4urVq0IIIT799FPh5uZmOD8hhHjttddEaGio4ftHHnlEDB8+3Kie6Oho8fTTTzfb+dXl4MGDAoBITk42bAsKChIfffTRLZ9jLucnRN3nOHHiRDFy5MhbPsfa3sORI0eKe+65x2ibJb2HN38umPJvZ1N9nrJb6i6Vl5fjyJEjiI2NNWyTy+WIjY3F/v37JaysYfLy8gAA7u7uRtu/++47eHp6onPnzpg5cyaKi4sNj+3fvx9dunSBj4+PYVtcXBzy8/Nx+vRpwz41fyb6fUzxM7lw4QL8/f3Rtm1bjB8/HikpKQCAI0eOoKKiwqiusLAwBAYGGuoy93O7WXl5Ob799lv84x//MLoQrCW/fzVdvnwZ6enpRrW4uLggOjra6D1zdXVFz549DfvExsZCLpfjwIEDhn0GDBgApVJp2CcuLg6JiYm4ceOGYR9zOOe8vDzIZDK4uroabX/vvffg4eGB7t27Y+HChUbN/ZZwfrt27YK3tzdCQ0MxdepUXL9+3ah+a3kPMzIy8Ouvv+KJJ56o9ZilvIc3fy6Y6m9nU36etrgLZza17OxsaDQaozcUAHx8fHDu3DmJqmoYrVaLF198EX379kXnzp0N2x977DEEBQXB398fJ06cwGuvvYbExESsX78eAJCenl7n+eofq2+f/Px8lJSUwM7OrlnOKTo6Gl999RVCQ0ORlpaGuXPnon///jh16hTS09OhVCprfWj4+Pjctm5zOLe6bNiwAbm5uZg0aZJhmyW/fzfT11NXLTVr9fb2NnrcxsYG7u7uRvu0adOm1mvoH3Nzc7vlOetfwxRKS0vx2muvYdy4cUYXHHz++efRo0cPuLu7Y9++fZg5cybS0tKwaNEiwzmY8/kNHToUDz74INq0aYNLly7hn//8J4YNG4b9+/dDoVBY1Xv49ddfw8nJCQ8++KDRdkt5D+v6XDDV384bN2402ecpw00LNm3aNJw6dQp79uwx2v7UU08Z7nfp0gV+fn4YMmQILl26hJCQEFOXeUeGDRtmuN+1a1dER0cjKCgIa9euNWnoMJX//Oc/GDZsGPz9/Q3bLPn9a8kqKirwyCOPQAiBZcuWGT02Y8YMw/2uXbtCqVTi6aefxvz58y1iCf9HH33UcL9Lly7o2rUrQkJCsGvXLgwZMkTCypreypUrMX78eKjVaqPtlvIe3upzwdKwW+oueXp6QqFQ1Bo1npGRAV9fX4mqur3p06fjf//7H3bu3InWrVvXu290dDQA4OLFiwAAX1/fOs9X/1h9+zg7O5s0ZLi6uqJDhw64ePEifH19UV5ejtzc3Fp13a5u/WP17WPqc0tOTsaOHTvw5JNP1rufJb9/+nrq+/fl6+uLzMxMo8crKyuRk5PTJO+rKf4d64NNcnIytm/fbtRqU5fo6GhUVlYiKSkJgPmf383atm0LT09Po99JS38PAeCPP/5AYmLibf9NAub5Ht7qc8FUfzub8vOU4eYuKZVKREZGIj4+3rBNq9UiPj4eMTExElZWNyEEpk+fjp9++gm//fZbrWbQuiQkJAAA/Pz8AAAxMTE4efKk0R8j/R/kjh07Gvap+TPR72Pqn0lhYSEuXboEPz8/REZGwtbW1qiuxMREpKSkGOqypHP78ssv4e3tjeHDh9e7nyW/f23atIGvr69RLfn5+Thw4IDRe5abm4sjR44Y9vntt9+g1WoNwS4mJga7d+9GRUWFYZ/t27cjNDQUbm5uhn2kOGd9sLlw4QJ27NgBDw+P2z4nISEBcrnc0JVjzudXlytXruD69etGv5OW/B7q/ec//0FkZCQiIiJuu685vYe3+1ww1d/OJv08vaPhx1Sn1atXC5VKJb766itx5swZ8dRTTwlXV1ejUePmYurUqcLFxUXs2rXLaEpicXGxEEKIixcvinnz5onDhw+Ly5cvi40bN4q2bduKAQMGGF5DP+XvvvvuEwkJCWLLli3Cy8urzil/r7zyijh79qz45JNPTDJd+qWXXhK7du0Sly9fFnv37hWxsbHC09NTZGZmCiF00xkDAwPFb7/9Jg4fPixiYmJETEyMRZxbTRqNRgQGBorXXnvNaLslvn8FBQXi2LFj4tixYwKAWLRokTh27JhhttB7770nXF1dxcaNG8WJEyfEyJEj65wK3r17d3HgwAGxZ88e0b59e6NpxLm5ucLHx0c8/vjj4tSpU2L16tXC3t6+1jRbGxsb8cEHH4izZ8+K2bNnN8k02/rOr7y8XDzwwAOidevWIiEhwejfpH6Gyb59+8RHH30kEhISxKVLl8S3334rvLy8xIQJE8zi/G53jgUFBeLll18W+/fvF5cvXxY7duwQPXr0EO3btxelpaWG17DU91AvLy9P2Nvbi2XLltV6vrm/h7f7XBDCdH87m+rzlOGmiSxZskQEBgYKpVIpoqKixJ9//il1SXUCUOftyy+/FEIIkZKSIgYMGCDc3d2FSqUS7dq1E6+88orROilCCJGUlCSGDRsm7OzshKenp3jppZdERUWF0T47d+4U3bp1E0qlUrRt29ZwjOY0duxY4efnJ5RKpWjVqpUYO3asuHjxouHxkpIS8eyzzwo3Nzdhb28vRo8eLdLS0izi3GraunWrACASExONtlvi+7dz5846fycnTpwohNBNB3/rrbeEj4+PUKlUYsiQIbXO+/r162LcuHHC0dFRODs7i8mTJ4uCggKjfY4fPy769esnVCqVaNWqlXjvvfdq1bJ27VrRoUMHoVQqRadOncSvv/7arOd3+fLlW/6b1K9bdOTIEREdHS1cXFyEWq0W4eHh4t133zUKBlKe3+3Osbi4WNx3333Cy8tL2NraiqCgIDFlypRaH1aW+h7qffbZZ8LOzk7k5ubWer65v4e3+1wQwrR/O5vi81RWdWJEREREVoFjboiIiMiqMNwQERGRVWG4ISIiIqvCcENERERWheGGiIiIrArDDREREVkVhhsiIiKyKgw3RNQiyWQybNiwQeoyiKgZMNwQkclNmjQJMpms1m3o0KFSl0ZEVsBG6gKIqGUaOnQovvzyS6NtKpVKomqIyJqw5YaIJKFSqeDr62t001/9WCaTYdmyZRg2bBjs7OzQtm1b/Pjjj0bPP3nyJO655x7Y2dnBw8MDTz31FAoLC432WblyJTp16gSVSgU/Pz9Mnz7d6PHs7GyMHj0a9vb2aN++PX7++WfDYzdu3MD48ePh5eUFOzs7tG/fvlYYIyLzxHBDRGbprbfewpgxY3D8+HGMHz8ejz76KM6ePQsAKCoqQlxcHNzc3HDo0CGsW7cOO3bsMAovy5Ytw7Rp0/DUU0/h5MmT+Pnnn9GuXTujY8ydOxePPPIITpw4gfvvvx/jx49HTk6O4fhnzpzB5s2bcfbsWSxbtgyenp6m+wEQUePd8aU2iYju0sSJE4VCoRAODg5Gt3feeUcIobtK8TPPPGP0nOjoaDF16lQhhBCff/65cHNzE4WFhYbHf/31VyGXyw1Xm/b39xdvvPHGLWsAIN58803D94WFhQKA2Lx5sxBCiBEjRojJkyc3zQkTkUlxzA0RSWLw4MFYtmyZ0TZ3d3fD/ZiYGKPHYmJikJCQAAA4e/YsIiIi4ODgYHi8b9++0Gq1SExMhEwmw7Vr1zBkyJB6a+jatavhvoODA5ydnZGZmQkAmDp1KsaMGYOjR4/ivvvuw6hRo9CnT59GnSsRmRbDDRFJwsHBoVY3UVOxs7Nr0H62trZG38tkMmi1WgDAsGHDkJycjE2bNmH79u0YMmQIpk2bhg8++KDJ6yWipsUxN0Rklv78889a34eHhwMAwsPDcfz4cRQVFRke37t3L+RyOUJDQ+Hk5ITg4GDEx8ffVQ1eXl6YOHEivv32WyxevBiff/75Xb0eEZkGW26ISBJlZWVIT0832mZjY2MYtLtu3Tr07NkT/fr1w3fffYeDBw/iP//5DwBg/PjxmD17NiZOnIg5c+YgKysLzz33HB5//HH4+PgAAObMmYNnnnkG3t7eGDZsGAoKCrB3714899xzDapv1qxZiIyMRKdOnVBWVob//e9/hnBFROaN4YaIJLFlyxb4+fkZbQsNDcW5c+cA6GYyrV69Gs8++yz8/Pzwww8/oGPHjgAAe3t7bN26FS+88AJ69eoFe3t7jBkzBosWLTK81sSJE1FaWoqPPvoIL7/8Mjw9PfHQQw81uD6lUomZM2ciKSkJdnZ26N+/P1avXt0EZ05EzU0mhBBSF0FEVJNMJsNPP/2EUaNGSV0KEVkgjrkhIiIiq8JwQ0RERFaFY26IyOywt5yI7gZbboiIiMiqMNwQERGRVWG4ISIiIqvCcENERERWheGGiIiIrArDDREREVkVhhsiIiKyKgw3REREZFUYboiIiMiq/D/jWoXDHatehAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions with model\n",
        "with torch.inference_mode():\n",
        "  y_preds = model_0(X_test)\n",
        "\n",
        "y_preds\n",
        "plot_predictions(predictions=y_preds)"
      ],
      "metadata": {
        "id": "wSMONoNpM23T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "outputId": "82d5cbba-69f4-4f35-ebfd-208369ba3c8f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x700 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weight, bias"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOOtxF3qO9DF",
        "outputId": "a1477307-e5e9-4f54-9cc2-6d8c2895b191"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.7, 0.3)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_0.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGB_wnYTPxkn",
        "outputId": "11322b35-2441-410a-a9f9-74bfa40adfca"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('weights', tensor([0.6999])), ('bias', tensor([0.3000]))])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving a model in PyTorch\n",
        "\n",
        "There are three main methods you should know about for saving and loading models in PyTorch.\n",
        "\n",
        "1. `torch.save()` - allows you to save a PyTorch object in Python's pickle format\n",
        "2. `torch.load()` - allows you to load a saved PyTorch object\n",
        "3. `torch.nn.Module.load_state_dict()` - this allows me to load a model's saved state dictionary"
      ],
      "metadata": {
        "id": "hrO_wyAQbwtt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_0.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GURHWFKb1jC",
        "outputId": "b4a81767-ee1b-4124-85ef-49d1d1b8f9b9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('weights', tensor([0.6999])), ('bias', tensor([0.3000]))])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving our PyTorch model\n",
        "from pathlib import Path\n",
        "\n",
        "# 1. Create model directory\n",
        "MODEL_PATH = Path(\"models\")\n",
        "MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# 2. Create a moidel save path\n",
        "MODEL_NAME = \"01_pytorch_workflow_model_0.pth\"\n",
        "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
        "\n",
        "MODEL_SAVE_PATH\n",
        "\n",
        "# 3. Save the model state dict\n",
        "print(f\"Saving model to: {MODEL_SAVE_PATH}\")\n",
        "torch.save(obj=model_0.state_dict(),\n",
        "           f=MODEL_SAVE_PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfnOkbKmppo7",
        "outputId": "9e989d81-593e-4d86-ad6e-525186a0c09b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving model to: models/01_pytorch_workflow_model_0.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check models\n",
        "!ls -l models"
      ],
      "metadata": {
        "id": "tsWXp3j3y00F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9ff9879-3024-46a9-d4ec-dbf912eb1411"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 4\n",
            "-rw-r--r-- 1 root root 1207 May 25 12:49 01_pytorch_workflow_model_0.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading a PyTorch model\n",
        "\n",
        "Since we saved our model's `state_dict()` rather than the entire model, we'll create anew instance of our model class and load the saved `state_dict()` into that."
      ],
      "metadata": {
        "id": "PoERp37R-STu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_0.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQeSY59i-qb8",
        "outputId": "0d04f8ff-5b0b-40c0-8375-1dca0e15bbd7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('weights', tensor([0.6999])), ('bias', tensor([0.3000]))])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To load in a saved state_dict we have to instantiate a new instance of our model class\n",
        "loaded_model_0 = LinearRegressionModel()\n",
        "\n",
        "# Load the saved state_dict of model_0 (this will update the new instance with updated parameters)\n",
        "loaded_model_0.load_state_dict(torch.load(f=MODEL_SAVE_PATH))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-DvKgHd-w9A",
        "outputId": "a57a8819-a84d-4975-be4f-1c6e19f53ab2"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# View the state dict, and see if it loaded\n",
        "loaded_model_0.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9R2URLG9AAc6",
        "outputId": "780aff13-3b19-41fb-e227-a6cebdda06b6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('weights', tensor([0.6999])), ('bias', tensor([0.3000]))])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make some predictions with our loaded model\n",
        "loaded_model_0.eval()\n",
        "with torch.inference_mode():\n",
        "  loaded_model_preds = loaded_model_0(X_test)\n",
        "\n",
        "loaded_model_preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLvA7B0GAJcC",
        "outputId": "f8b62be5-fbeb-47bd-9e6f-7f0e8f0dd6ec"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.8599],\n",
              "        [0.8739],\n",
              "        [0.8879],\n",
              "        [0.9019],\n",
              "        [0.9159],\n",
              "        [0.9299],\n",
              "        [0.9439],\n",
              "        [0.9579],\n",
              "        [0.9719],\n",
              "        [0.9859]])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare loaded model predictions with original model predictions\n",
        "y_preds == loaded_model_preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QcUw6I1PBQTZ",
        "outputId": "e817d543-5ef3-408f-e8e9-cec24ae4c90f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[True],\n",
              "        [True],\n",
              "        [True],\n",
              "        [True],\n",
              "        [True],\n",
              "        [True],\n",
              "        [True],\n",
              "        [True],\n",
              "        [True],\n",
              "        [True]])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Putting it all together\n",
        "\n",
        "Let's go back through the steps above, and see it all in one place"
      ],
      "metadata": {
        "id": "X2KBvS33B7G_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import PyTorch and matplotlib\n",
        "import torch\n",
        "from torch import nn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Check PyTorch version\n",
        "torch.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LG2WCqSaBcji",
        "outputId": "ac207971-755b-438c-b678-cdfe57c8ab3a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.0.1+cu118'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create device-agnostic code.\n",
        "\n",
        "This means if we've got access to a GPU, our code will use it (for potentiall faster computing ).\n",
        "\n",
        "If no GPU is available, the code will default to using CPU."
      ],
      "metadata": {
        "id": "MWm1M2W3CtLI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up device agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcKD2VeMDgZT",
        "outputId": "13703c94-61f5-418c-b3d8-ae338cdbcc2d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# In case we want to check the GPU we are using ( if we are using )\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJfuZYhfE723",
        "outputId": "b88282a2-a55e-4601-82f9-c2e65a83cd41"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: nvidia-smi: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1 Data"
      ],
      "metadata": {
        "id": "_SV2tTwQCK5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create some data using the linear regression formula of y = weight * X + bias\n",
        "weight = 0.7\n",
        "bias = 0.3\n",
        "\n",
        "# Create range values\n",
        "start = 0\n",
        "end = 1\n",
        "step = 0.02\n",
        "\n",
        "# Create the X and y ( features and labels )\n",
        "X= torch.arange(start, end, step).unsqueeze(dim=1) # If we don't use unsqueeze, we will have to many dimentions for this test\n",
        "y = weight * X + bias\n",
        "X[:10], y[:10]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1S4xjyvFDjFt",
        "outputId": "aae32d43-9c76-41dc-f5f6-43cef9c0d3a2"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0.0000],\n",
              "         [0.0200],\n",
              "         [0.0400],\n",
              "         [0.0600],\n",
              "         [0.0800],\n",
              "         [0.1000],\n",
              "         [0.1200],\n",
              "         [0.1400],\n",
              "         [0.1600],\n",
              "         [0.1800]]),\n",
              " tensor([[0.3000],\n",
              "         [0.3140],\n",
              "         [0.3280],\n",
              "         [0.3420],\n",
              "         [0.3560],\n",
              "         [0.3700],\n",
              "         [0.3840],\n",
              "         [0.3980],\n",
              "         [0.4120],\n",
              "         [0.4260]]))"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data ( train and test )\n",
        "train_split = int(0.8 * len(X))\n",
        "X_train, y_train = X[:train_split], y[:train_split]\n",
        "X_test, y_test = X[train_split:], y[train_split:] \n",
        "len(X_train), len(y_train), len(X_test), len(y_test)"
      ],
      "metadata": {
        "id": "MYxzayf3GziS",
        "outputId": "a3bfa928-efb7-4d33-ce76-74adb44744a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40, 40, 10, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the data\n",
        "# Note: if you don't have the plot_predictions() function loaded, this will error\n",
        "plot_predictions(X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "id": "9zptViwKHnFm",
        "outputId": "e18b940d-3f47-4cbd-83fb-65c37dfa5aee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x700 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.2 Building a PyTorch linear model"
      ],
      "metadata": {
        "id": "7Me4PAQTIPRj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-c8CwGC-H_hH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}